{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p><code>doctable</code> is a Python package for manipulating SQL databases through an object-oriented interface without the overhead of object-relational mapping. It is built on top of the Sqlalchemy core interface, and takes advantage of dataclasses to define database schemas.</p> <p><code>doctable</code> also enables the high-performance storage of binary files (think ML models), parse trees, and compressed text files.</p> <p>Created by Devin J. Cornell.</p>"},{"location":"#news-update-doctable-has-a-new-interface","title":"News update: <code>doctable</code> has a new interface!","text":"<p>The package has been updated with an entirely new API to improve on previous limitations and better match the Sqlalchemy 2.0 interface. Inspired by the attrs project, I used different names for functions and classes to make it clear that the interface has changed and open the possibility for backwards compatibility with upgraded internals in the future. </p> <p>For now, install version 1.0 when using sqlalchemy &lt;= 1.4 and version 2.0 when using sqlalchemy &gt;= 2.0. See the installation page for more.</p>"},{"location":"installation/","title":"Installation and Change Log","text":"<p><code>doctable</code> is on PyPI, so you can install it with <code>pip</code>:</p> <p>For sqlalchemy &gt;= 2.0: <code>pip install doctable==2.0</code></p> <p>For sqlalchemy &lt;= 1.4: <code>pip install doctable==1.0</code></p>"},{"location":"installation/#changes-in-version-20","title":"Changes in Version 2.0","text":"<ul> <li> <p>Create database connections using <code>ConnectCore</code> objects instead of <code>ConnectEngine</code> or <code>DocTable</code> objects.</p> </li> <li> <p>Database tables represented by <code>DBTable</code> objects instead of <code>DocTable</code> objects. All <code>DBTable</code> instances originate from a <code>ConnectCore</code> object.</p> </li> <li> <p>Create schemas using the <code>doctable.table_schema</code> decorator instead of the <code>doctable.schema</code> decorator. This new decorator includes constraint and index parameters as well as those for the <code>dataclass</code> objects.</p> </li> <li> <p>The <code>Column</code> function replaces <code>Col</code> as generic default parameter values with more fine-grained control over column properties. This function provides a clearer separation between parameters that affect the behavior of the object as a dataclass (supplied as a <code>FieldArgs</code> object) and those that affect the database column schema (supplied via a <code>ColumnArgs</code> object).</p> </li> <li> <p>New command line interface: you may execute doctable functions through the command line. Just use <code>python -m doctable execute {args here}</code> to see how to use it.</p> </li> </ul>"},{"location":"documentation/a-ex_basics/","title":"Introduction","text":"<p>Here I will give an overview of the basic functionality of <code>doctable</code>.</p> <p>I will cover the following topics:</p> <ul> <li>Connecting to the database using <code>ConnectCore</code>.</li> <li>Defining a database schema using the <code>table_schema</code> decorator.</li> <li>Creating the table using the <code>begin_ddl()</code> context manager.</li> <li>Inserting values into the database using the <code>ConnectQuery</code> and <code>ConnectTable</code> interfaces.</li> </ul> <pre><code>import sys\nsys.path.append('../')\nimport doctable\nimport pprint\n</code></pre> <p>The <code>ConnectCore</code> objects acts as the primary starting point for any actions performed on the database. We create a new connection to the datbase using the <code>.open()</code> factory method constructor.</p> <pre><code>core = doctable.ConnectCore.open(\n    target=':memory:', \n    dialect='sqlite'\n)\ncore\n</code></pre> <pre><code>ConnectCore(target=':memory:', dialect='sqlite', engine=Engine(sqlite:///:memory:), metadata=MetaData())\n</code></pre> <p>Next we define a very basic schema using the <code>table_schema</code> decorator. This decorator is used to create a Container object, which contains information about the database schema and is also a dataclass that can be inserted or retrieved from the database. Read the schema definition examples for more information on creating container objects and database schemas.</p> <pre><code>@doctable.table_schema\nclass MyContainer0:\n    id: int\n    name: str\n    age: int\n\ndoctable.inspect_schema(MyContainer0).column_info_df()\n</code></pre> Col Name Col Type Attr Name Hint Order Primary Key Foreign Key Index Default 0 id Integer id int (inf, 0) False False None None 1 name String name str (inf, 1) False False None None 2 age Integer age int (inf, 2) False False None None <p>We actually connect to the database table using the context manager returned by <code>.begin_ddl()</code>. This design is necessary for multi-table schemas, but, because of the readability it provides, I will use it for single-table schemas as well. The method <code>create_table_if_not_exists</code> here returns a new instance of <code>DBTable</code>. Alternatively, we could reflect a database table, in which we would not be required to provide a schema container.</p> <pre><code>with core.begin_ddl() as emitter:\n    tab0 = emitter.create_table_if_not_exists(container_type=MyContainer0)\nfor ci in core.inspect_columns('MyContainer0'):\n    print(ci)\n</code></pre> <pre><code>{'name': 'id', 'type': INTEGER(), 'nullable': True, 'default': None, 'primary_key': 0}\n{'name': 'name', 'type': VARCHAR(), 'nullable': True, 'default': None, 'primary_key': 0}\n{'name': 'age', 'type': INTEGER(), 'nullable': True, 'default': None, 'primary_key': 0}\n</code></pre> <p>We can perform queries on the database using the <code>ConnectQuery</code> interface returned from the <code>ConnectCore.query()</code> method. In this case, we insert a new row into the database using the <code>insert_multi()</code> method. Not that we will use an alternative interface for inserting container instances into the database.</p> <pre><code>with core.query() as q:\n    q.insert_multi(tab0, [\n        {'name': 'Devin J. Cornell', 'age': 50},\n        {'name': 'Dorothy Andrews', 'age': 49},\n    ])\n    print(q.select(tab0.all_cols()).all())\n</code></pre> <pre><code>[(None, 'Devin J. Cornell', 50), (None, 'Dorothy Andrews', 49)]\n</code></pre> <p>To insert container object instances into the table, I instead use the <code>DBTable.query()</code> method to generate a <code>TableQuery</code> instance. This behaves much like <code>ConnectQuery</code> except that returned data will be placed into new container instances and we may insert data from container instances directly.</p> <pre><code>with tab0.query() as q:\n    q.insert_single(MyContainer0(id=0, name='John Doe', age=30))\n    print(q.select())\n</code></pre> <pre><code>[MyContainer0(id=None, name='Devin J. Cornell', age=50), MyContainer0(id=None, name='Dorothy Andrews', age=49), MyContainer0(id=0, name='John Doe', age=30)]\n</code></pre> <p>Here I define a more complicated schema. </p> <ul> <li>The standard <code>id</code> column is now included. Notice that <code>order=0</code> means the column will appear first in the table.</li> <li>The <code>updated</code> and <code>added</code> attributes have been created to automatically record the time of insertion and update.</li> <li>I added the <code>birthyear</code> method to the container type.</li> </ul> <pre><code>import datetime\n@doctable.table_schema(table_name='mytable1')\nclass MyContainer1:\n    name: str\n    age: int\n    id: int = doctable.Column(\n        column_args=doctable.ColumnArgs(order=0, primary_key=True, autoincrement=True),\n    )\n    updated: datetime.datetime = doctable.Column(\n        column_args=doctable.ColumnArgs(default=datetime.datetime.utcnow),\n    )\n    added: datetime.datetime = doctable.Column(\n        column_args=doctable.ColumnArgs(\n            default=datetime.datetime.utcnow, \n            onupdate=datetime.datetime.utcnow,\n        )\n    )\n\n    def birthyear(self):\n        '''Retrieve the birthyear of the person at the time this database entry was added.'''\n        try:\n            return self.added.year - self.age\n        except AttributeError as e:\n            raise AttributeError('Cannot calculate birthyear without the added date. '\n                'Did you mean to call this on a retrieved container instance?') from e\n\ndoctable.inspect_schema(MyContainer1).column_info_df()\n</code></pre> Col Name Col Type Attr Name Hint Order Primary Key Foreign Key Index Default 0 id Integer id int (0, 2) True False None None 1 name String name str (inf, 0) False False None None 2 age Integer age int (inf, 1) False False None None 3 updated DateTime updated datetime (inf, 3) False False None utcnow 4 added DateTime added datetime (inf, 4) False False None utcnow <p>We create this table just as we did the one before, and show the new schema using inspection.</p> <pre><code>with core.begin_ddl() as emitter:\n    tab1 = emitter.create_table_if_not_exists(container_type=MyContainer1)\n\nfor ci in core.inspect_columns('mytable1'):\n    print(ci)\n</code></pre> <pre><code>{'name': 'id', 'type': INTEGER(), 'nullable': True, 'default': None, 'primary_key': 1}\n{'name': 'name', 'type': VARCHAR(), 'nullable': True, 'default': None, 'primary_key': 0}\n{'name': 'age', 'type': INTEGER(), 'nullable': True, 'default': None, 'primary_key': 0}\n{'name': 'updated', 'type': DATETIME(), 'nullable': True, 'default': None, 'primary_key': 0}\n{'name': 'added', 'type': DATETIME(), 'nullable': True, 'default': None, 'primary_key': 0}\n</code></pre> <p>We can create a containser instance just as we did before. Note that <code>id</code>, <code>updated</code>, and <code>added</code> are optionally now because we expect the database to create them.</p> <pre><code>o = MyContainer1(name='John Doe', age=30)\no\n</code></pre> <pre><code>MyContainer1(name='John Doe', age=30, id=MISSING, updated=MISSING, added=MISSING)\n</code></pre> <p>As expected, calling <code>.birthyear()</code> raises an exception because the <code>added</code> entry has not been recorded - that will happen at insertion into the db.</p> <pre><code>try:\n    o.birthyear()\nexcept AttributeError as e:\n    print('error raised:', e)\n</code></pre> <pre><code>error raised: Cannot calculate birthyear without the added date. Did you mean to call this on a retrieved container instance?\n</code></pre> <p>After inserting the object into the database and retrieving it again, we can see that those previously missing fileds have been populated.</p> <pre><code>with tab1.query() as q:\n    q.insert_single(o)\n    results = q.select()\nresults[0]\n</code></pre> <pre><code>MyContainer1(name='John Doe', age=30, id=1, updated=datetime.datetime(2023, 11, 16, 17, 40, 7, 684832), added=datetime.datetime(2023, 11, 16, 17, 40, 7, 684836))\n</code></pre> <p>And now we can call the <code>birthyear()</code> method.</p> <pre><code>results[0].birthyear()\n</code></pre> <pre><code>1993\n</code></pre>"},{"location":"documentation/a-ex_basics/#conclusion","title":"Conclusion","text":"<p>For more detailed explanations of these topics, see the documentation and API reference provided on the website. Good luck!</p>"},{"location":"documentation/b-ex_schemas/","title":"Table Schemas","text":"<p>In this document, I give some examples for defining single and multi-table database schemas in Python. </p> <pre><code>import sys\nsys.path.append('../')\nimport doctable\nimport pprint\n</code></pre>"},{"location":"documentation/b-ex_schemas/#containers-and-the-table_schema-decorator","title":"Containers and the <code>table_schema</code> decorator","text":"<p>The first step in using doctable is to define a container object. Container objects are defined using the <code>table_schema</code> decorator, and are used both to define the schema of a database table and to wrap the data for insertion and selection. Container objects act very similar to normal dataclasses - in fact, they actually are dataclasses with additional information needed to create the database table attached. This informaiton is collected at the time when the decorator is used, and thus the decorator serves only to parse the database schema from the class definition, attach that information to the container class, and return the container type as a dataclass.</p> <pre><code>@doctable.table_schema # equivalent to @doctable.table_schema()\nclass Container1:\n    name: str\n\nins = doctable.inspect_schema(Container1)\nprint(ins.table_name())\nins.column_info_df()\n</code></pre> <pre><code>Container1\n</code></pre> Col Name Col Type Attr Name Hint Order Primary Key Foreign Key Index Default 0 name String name str (inf, 0) False False None None <pre><code>@doctable.table_schema(table_name='container2')\nclass Container2:\n    name: str\n    age: int\nins = doctable.inspect_schema(Container2)\nprint(ins.table_name())\nins.column_info_df()\n</code></pre> <pre><code>container2\n</code></pre> Col Name Col Type Attr Name Hint Order Primary Key Foreign Key Index Default 0 name String name str (inf, 0) False False None None 1 age Integer age int (inf, 1) False False None None"},{"location":"documentation/b-ex_schemas/#specifying-column-properties","title":"Specifying Column Properties","text":"<p>There are two sets of parameters you may adjust to change the behavior of a column: </p> <ul> <li> <p><code>ColumnArgs</code>: adjust the behavior of the generated column. This does not affect the container object, but does affect the database column.</p> </li> <li> <p><code>FieldArgs</code>: adjust the behavior of container attribute by passing arguments to <code>dataclasses.field()</code>. This does not affect the database column, but does affect the way the container object can be used.</p> </li> </ul> <p>Both are passed directly to the <code>Column</code> function, which, as you can see, simply returns a <code>dataclasses.field</code> object with column arguments passed to the <code>metadata</code> attribute. Note that by default, the <code>default</code> argument is set to <code>doctable.MISSING</code>, so the parameter is optional and will be populated with that value. Missing values will be ignored when inserting the object into the database.</p> <pre><code>doctable.Column(\n    column_args=doctable.ColumnArgs(),\n    field_args=doctable.FieldArgs(),\n)\n</code></pre> <pre><code>Field(name=None,type=None,default=MISSING,default_factory=&lt;dataclasses._MISSING_TYPE object at 0x7fc0f9afe590&gt;,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'_column_args': ColumnArgs(order=inf, column_name=None, type_kwargs={}, use_type=None, sqlalchemy_type=None, autoincrement=False, nullable=True, unique=None, primary_key=False, index=None, foreign_key=None, default=None, onupdate=None, server_default=None, server_onupdate=None, comment=None, other_kwargs={})}),kw_only=&lt;dataclasses._MISSING_TYPE object at 0x7fc0f9afe590&gt;,_field_type=None)\n</code></pre> <pre><code>import datetime\n\nclass PhoneNumber(str):\n    pass\n\nclass Address(str):\n    pass\n\n@doctable.table_schema(table_name='container3')\nclass Container3:\n    name: str\n    age: int = doctable.Column(field_args=doctable.FieldArgs(init_required=True))\n    address: Address = doctable.Column()\n    phone: PhoneNumber = doctable.Column()\n\n    # this column will appear first in the database, even though this attribute is later\n    _id: int = doctable.Column(\n        column_args=doctable.ColumnArgs(\n            column_name='id', # name of the column in the db (might not want to have an attr called 'id')\n            order = 0, # affects the ordering of the columns in the db\n            primary_key=True,\n            autoincrement=True,\n        ),\n    )\n\n    # doctable will define default and onupdate when inserting into database\n    added: datetime.datetime = doctable.Column(\n        column_args=doctable.ColumnArgs(\n            default=datetime.datetime.now, \n            onupdate=datetime.datetime.now\n        ),\n        field_args = doctable.FieldArgs(\n            repr=False, # don't show this field when printing\n        )\n    )    \n\ndoctable.inspect_schema(Container3).column_info_df()\n</code></pre> Col Name Col Type Attr Name Hint Order Primary Key Foreign Key Index Default 0 id Integer _id int (0, 4) True False None None 1 name String name str (inf, 0) False False None None 2 age Integer age int (inf, 1) False False None None 3 address String address Address (inf, 2) False False None None 4 phone String phone PhoneNumber (inf, 3) False False None None 5 added DateTime added datetime (inf, 5) False False None now <p>Notice that the string representation does not show the <code>added</code> attribute, as specified via <code>FieldAargs(repr=False)</code>.</p> <pre><code>Container3('Devin J. Cornell', 30)\n</code></pre> <pre><code>Container3(name='Devin J. Cornell', age=30, address=MISSING, phone=MISSING, _id=MISSING)\n</code></pre>"},{"location":"documentation/b-ex_schemas/#indices","title":"Indices","text":"<p>Indices may be added to a table by passing a dictionary of name, <code>Index</code> pairs to the <code>indices</code> parameter of the <code>table_schema</code> decorator. The arguments are the columns, and any additional keyword arguments may be passed after.</p> <pre><code>@doctable.table_schema(\n    table_name='container4',\n    indices = {\n        'ind_name': doctable.Index('name'),\n        'ind_name_age': doctable.Index('name', 'age', unique=True),\n    }\n)\nclass Container4:\n    name: str\n    age: int\n\nins = doctable.inspect_schema(Container4)\nins.index_info_df()\n</code></pre> name columns kwargs 0 ind_name name 1 ind_name_age name, age unique: True"},{"location":"documentation/b-ex_schemas/#constraints","title":"Constraints","text":"<p>You may pass constraints through the <code>constraint</code> parameter of the <code>table_schema</code> decorator.</p> <p>There are several types of constraints you may want to use in your schema. The following methods are thin wrappers over the SQLAlchemy objects of the same name.</p> docs Constraint Description link <code>ForeignKey(local_columns, foreign_columns, optional[onupdate], optional[ondelete])</code> A foreign key constraint. link <code>CheckConstraint(text, optional[Name])</code> A unique constraint. link <code>UniqueConstraint(*column_names, optional[name])</code> A unique constraint. link <code>PrimaryKeyConstraint(*column_names, optional[name])</code> A unique constraint. <pre><code>@doctable.table_schema(\n    table_name='container5',\n    constraints = [\n        #doctable.ForeignKey(..), # see multi-table schemas below\n        doctable.CheckConstraint('age &gt;= 0', name='check_age'),\n        doctable.UniqueConstraint('age', 'name', name='unique_age_name'),\n        doctable.PrimaryKeyConstraint('id'),\n    ]\n)\nclass Container5:\n    id: int # this is the primary key now\n    name: str\n    age: int\n</code></pre>"},{"location":"documentation/b-ex_schemas/#column-types","title":"Column Types","text":"<p>The column type resolution works according to the following steps:</p> <ol> <li>Check <code>ColumnArgs.sqlalchemy_type</code> and use this if it is not <code>None</code>.</li> <li>Check if column is foreign key - if it is, ask sqlalchemy to resolve the type</li> <li>Check <code>ColumnArgs.use_type</code> and use this if it is provided.</li> <li>Use the provided type hint to resolve the type.</li> </ol> <p>The valid type hints and their sqlalchemy equivalents are listed below.</p> Type Hint SQLAlchemy Type <code>int</code> <code>sqlalchemy.Integer</code> <code>float</code> <code>sqlalchemy.Float</code> <code>bool</code> <code>sqlalchemy.Boolean</code> <code>str</code> <code>sqlalchemy.String</code> <code>bytes</code> <code>sqlalchemy.LargeBinary</code> <code>datetime.datetime</code> <code>sqlalchemy.DateTime</code> <code>datetime.time</code> <code>sqlalchemy.Time</code> <code>datetime.date</code> <code>sqlalchemy.Date</code> <code>typing.Any</code> <code>sqlalchemy.PickleType</code> <code>'datetime.datetime'</code> <code>sqlalchemy.DateTime</code> <code>'datetime.time'</code> <code>sqlalchemy.Time</code> <code>'datetime.date'</code> <code>sqlalchemy.Date</code> <code>'Any'</code> <code>sqlalchemy.PickleType</code> <p>You can get the mappings programatically if needed as well:</p> <pre><code>doctable.type_mappings()\n</code></pre> <pre><code>{int: sqlalchemy.sql.sqltypes.Integer,\n float: sqlalchemy.sql.sqltypes.Float,\n bool: sqlalchemy.sql.sqltypes.Boolean,\n str: sqlalchemy.sql.sqltypes.String,\n bytes: sqlalchemy.sql.sqltypes.LargeBinary,\n datetime.datetime: sqlalchemy.sql.sqltypes.DateTime,\n datetime.time: sqlalchemy.sql.sqltypes.Time,\n datetime.date: sqlalchemy.sql.sqltypes.Date,\n doctable.schema.column.column_types.PickleType: sqlalchemy.sql.sqltypes.PickleType,\n 'datetime.datetime': sqlalchemy.sql.sqltypes.DateTime,\n 'datetime.time': sqlalchemy.sql.sqltypes.Time,\n 'datetime.date': sqlalchemy.sql.sqltypes.Date,\n doctable.schema.column.column_types.JSON: sqlalchemy.sql.sqltypes.JSON}\n</code></pre>"},{"location":"documentation/b-ex_schemas/#special-column-types","title":"Special Column Types","text":"<p>There are several special column types that can be used in your schemas.</p> Type Hint SQLAlchemy Type Description <code>doctable.JSON</code> <code>sqlalchemy.JSON</code> Calls <code>json.dumps</code> on write, <code>json.loads</code> on read. <code>doctable.PickleType</code> <code>sqlalchemy.PickleType</code> Calls <code>pickle.dumps</code> on write, <code>pickle.loads</code> on read. <pre><code>import dataclasses\nimport typing\n\n@dataclasses.dataclass\nclass Address:\n    street: str\n    city: str\n    state: str\n    zip: str\n\n@doctable.table_schema\nclass Container6:\n    name: str\n\n    # NOTE: will be serialized as a JSON string in the database\n    # notice how we can use a more accurate type hint and still specify\n    # the column type using use_type\n    other_info: typing.Dict[str, typing.Union[str,int,float]] = doctable.Column(\n        column_args=doctable.ColumnArgs(\n            use_type=doctable.JSON,\n        ),\n        field_args=doctable.FieldArgs(default_factory=dict),\n    )\n\n    # NOTE: will be pickled in the database\n    address: Address = doctable.Column(\n        column_args=doctable.ColumnArgs(\n            use_type=doctable.PickleType,\n        )\n    )\n\ndoctable.inspect_schema(Container6).column_info_df()\n</code></pre> Col Name Col Type Attr Name Hint Order Primary Key Foreign Key Index Default 0 name String name str (inf, 0) False False None None 1 other_info JSON other_info Dict (inf, 1) False False None None 2 address PickleType address Address (inf, 2) False False None None <p>Now create a new container object that contains an address for insertion.</p> <pre><code>new_obj = Container6(\n    name = 'Devin J. Cornell', \n    other_info = {'favorite_color': 'blue'},\n    address = Address('123 Main St.', 'San Francisco', 'CA', '94122'), \n)\nnew_obj\n</code></pre> <pre><code>Container6(name='Devin J. Cornell', other_info={'favorite_color': 'blue'}, address=Address(street='123 Main St.', city='San Francisco', state='CA', zip='94122'))\n</code></pre> <p>Now we open a new database, insert the row, and query it back - you can see that the dict data was converted to json and back again, and the address was converted to pickle data and back again.</p> <pre><code>core = doctable.ConnectCore.open(':memory:', 'sqlite')\nwith core.begin_ddl() as ddl:\n    tab = ddl.create_table(Container6)\n\nwith tab.query() as q:\n    q.insert_single(new_obj)\n\nwith core.query() as q:\n    result = q.select(tab.all_cols())\nresult.first()\n</code></pre> <pre><code>('Devin J. Cornell', {'favorite_color': 'blue'}, Address(street='123 Main St.', city='San Francisco', state='CA', zip='94122'))\n</code></pre>"},{"location":"documentation/b-ex_schemas/#multi-table-schemas","title":"Multi-table Schemas","text":"<p>The example below shows two linked tables: one for colors, and the other for people. Each person has a favorite color that is constrained by a foriegn key to the colors table. The colors table also has a unique constraint on the color name. I demonstrate use of the <code>Column</code> function to describe behavior of columns - specifically the use of <code>ColumnArgs</code> to specify additional column features that are not conveyed through type annotations or attribute names. I also show use of the <code>Index</code> object for creating indexes, the <code>UniqueConstraint</code> object for creating unique constraints, and the <code>ForeignKey</code> object for creating foreign key constraints. </p> <p>Note that the container object representing the database schema is also a usable <code>dataclass</code> that can used like any other container object. In fact, tables created according to this schema can insert these objects directly and will wrap return values issued via select queries.</p> <pre><code>import datetime\n\n@doctable.table_schema(\n    table_name='color',\n    constraints = [\n        doctable.UniqueConstraint('name'),\n    ]\n)\nclass Color:\n    name: str\n    id: int = doctable.Column(\n        column_args=doctable.ColumnArgs(\n            primary_key=True,\n            autoincrement=True,\n        )\n    )\n\n# lets say we use this instead of an int\nclass PersonID(int):\n    pass\n\n# add table-level parameters to this decorator\n@doctable.table_schema(\n    table_name='person',\n    indices = {\n        'ind_name_birthday': doctable.Index('name', 'birthday', unique=True),\n    },\n    constraints = [ # these constraints are set on the database\n        doctable.CheckConstraint('length(address) &gt; 0'), # cannot have a blank address\n        doctable.UniqueConstraint('birthday', 'fav_color'),\n        doctable.ForeignKey(['fav_color'], ['color.name'], onupdate='CASCADE', ondelete='CASCADE'),\n    ],\n    frozen = True, # parameter passed to dataclasses.dataclass\n)\nclass Person:\n    name: str\n\n    # default value will be \"not provided\" - good standardization\n    address: str = doctable.Column(\n        column_args=doctable.ColumnArgs(\n            server_default='not provided',\n        )\n    )\n\n    # provided as datetime, set to be indexed\n    birthday: datetime.datetime = doctable.Column(\n        column_args=doctable.ColumnArgs(\n            index = True,\n        )\n    )\n\n    # note that this has a foreign key constraint above\n    fav_color: str = doctable.Column(\n        column_args=doctable.ColumnArgs(\n            nullable=False,\n        )\n    )\n\n    id: PersonID = doctable.Column( # standard id column\n        column_args=doctable.ColumnArgs(\n            order=0, # will be the first column\n            primary_key=True,\n            autoincrement=True\n        ),\n    )\n\n    # doctable will define default and onupdate when inserting into database\n    added: datetime.datetime = doctable.Column(\n        column_args=doctable.ColumnArgs(\n            index=True,\n            default=datetime.datetime.utcnow, \n            onupdate=datetime.datetime.utcnow\n        )\n    )\n\n    # this property will not be stored in the database \n    #   - it acts like any other property\n    @property\n    def age(self):\n        return datetime.datetime.now() - self.birthday\n\n\ncore = doctable.ConnectCore.open(\n    target=':memory:', \n    dialect='sqlite'\n)\n# NOTE: weird error when trying to run this twice after defining containers\nwith core.begin_ddl() as emitter:\n    core.enable_foreign_keys() # NOTE: NEEDED TO ENABLE FOREIGN KEYS\n    color_tab = emitter.create_table_if_not_exists(container_type=Color)\n    person_tab = emitter.create_table_if_not_exists(container_type=Person)\nfor col_info in person_tab.inspect_columns():\n    print(f'{col_info[\"name\"]}: {col_info[\"type\"]}')\n</code></pre> <pre><code>id: INTEGER\nname: VARCHAR\naddress: VARCHAR\nbirthday: DATETIME\nfav_color: VARCHAR\nadded: DATETIME\n</code></pre> <p>Insertion into the color table is fairly straightforward.</p> <pre><code>color_names = ['red', 'green', 'blue']\ncolors = [Color(name=name) for name in color_names]\nwith color_tab.query() as q:\n    q.insert_multi(colors)\n    for c in q.select():\n        print(c)\n    #print(q.select())\n</code></pre> <pre><code>Color(name='red', id=1)\nColor(name='green', id=2)\nColor(name='blue', id=3)\n</code></pre> <p>Insertion into the person table is similar, and note that we see an exception if we try to insert a person with a favorite color that is not in the color table.</p> <pre><code>persons = [\n    Person(name='John', birthday=datetime.datetime(1990, 1, 1), fav_color='red'),\n    Person(name='Sue', birthday=datetime.datetime(1991, 1, 1), fav_color='green'),\n    Person(name='Ren', birthday=datetime.datetime(1995, 1, 1), fav_color='blue'),\n]\nother_person = Person(\n    name='Bob', \n    address='123 Main St', \n    birthday=datetime.datetime(1990, 1, 1), \n    fav_color='other', # NOTE: THIS WILL CAUSE AN ERROR (NOT IN COLOR TABLE)\n)\n\nimport sqlalchemy.exc\n\nsec_in_one_year = 24*60*60*365\nwith person_tab.query() as q:\n    q.insert_multi(persons, ifnotunique='replace')\n\n    try:\n        q.insert_single(other_person, ifnotunique='replace')\n        print(f'THIS SHOULD NOT APPEAR')\n    except sqlalchemy.exc.IntegrityError as e:\n        print(f'successfully threw exception: {e}')\n\n    for p in q.select():\n        print(f'{p.name} ({p.fav_color}): {p.age.total_seconds()//sec_in_one_year:0.0f} y/o')\n</code></pre> <pre><code>successfully threw exception: (sqlite3.IntegrityError) FOREIGN KEY constraint failed\n[SQL: INSERT OR REPLACE INTO person (name, address, birthday, fav_color, added) VALUES (?, ?, ?, ?, ?)]\n[parameters: ('Bob', '123 Main St', '1990-01-01 00:00:00.000000', 'other', '2023-11-14 22:17:40.402308')]\n(Background on this error at: https://sqlalche.me/e/20/gkpj)\nJohn (red): 33 y/o\nSue (green): 32 y/o\nRen (blue): 28 y/o\n</code></pre> <p>The foreign key works as expected because we set <code>onupdate</code>: changing that value in the parent table will update the value in the child table.</p> <pre><code>with color_tab.query() as q:\n    q.update_single(dict(name='reddish'), where=color_tab['name']=='red')\n    for c in q.select():\n        print(c)\n\nwith person_tab.query() as q:\n    for p in q.select():\n        print(f'{p.name} ({p.fav_color}): {p.age.total_seconds()//sec_in_one_year:0.0f} y/o')\n</code></pre> <pre><code>Color(name='reddish', id=1)\nColor(name='green', id=2)\nColor(name='blue', id=3)\nJohn (reddish): 33 y/o\nSue (green): 32 y/o\nRen (blue): 28 y/o\n</code></pre>"},{"location":"documentation/c-ex_select/","title":"Select Queries","text":"<p>In this document I will describe the interface for performing select queries with <code>doctable</code>. </p> <pre><code>\nimport pandas as pd\nimport numpy as np\nimport typing\n\nimport sys\nsys.path.append('..')\nimport doctable\n</code></pre>"},{"location":"documentation/c-ex_select/#define-a-demonstration-schema","title":"Define a demonstration schema","text":"<p>The very first step is to define a table schema that will be appropriate for our examples. This table includes the typical <code>id</code> column (the first column, specified by <code>order=0</code>), as well as string, integer, and boolean attributes. The object used to specify the schema is called a container, and I will use that terminology as we go.</p> <pre><code>@doctable.table_schema\nclass Record:\n    name: str = doctable.Column(column_args=doctable.ColumnArgs(nullable=False, unique=True))\n    age: int = doctable.Column()\n    is_old: bool = doctable.Column()\n\n    id: int = doctable.Column(\n        column_args=doctable.ColumnArgs(\n            order = 0, \n            primary_key=True, \n            autoincrement=True\n        ),\n    )\n\ncore = doctable.ConnectCore.open(target=':memory:', dialect='sqlite', echo=True)\n\nwith core.begin_ddl() as ddl:\n    rtab = ddl.create_table_if_not_exists(container_type=Record)\n</code></pre> <pre><code>2023-11-13 08:38:32,059 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:32,060 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"Record\")\n2023-11-13 08:38:32,060 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-11-13 08:38:32,061 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"Record\")\n2023-11-13 08:38:32,062 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-11-13 08:38:32,063 INFO sqlalchemy.engine.Engine \nCREATE TABLE \"Record\" (\n    id INTEGER, \n    age INTEGER, \n    is_old INTEGER, \n    name VARCHAR NOT NULL, \n    PRIMARY KEY (id), \n    UNIQUE (name)\n)\n\n\n2023-11-13 08:38:32,063 INFO sqlalchemy.engine.Engine [no key 0.00039s] ()\n2023-11-13 08:38:32,064 INFO sqlalchemy.engine.Engine COMMIT\n</code></pre>"},{"location":"documentation/c-ex_select/#insert-test-data","title":"Insert test data","text":"<p>We insert the test data using the <code>TableQuery</code> interface. Because this document is about select queries, feel free to look over this for now. I show the contents of the table as a dataframe below. The interface for doing this will be covered later in this document.</p> <pre><code>import random\nrandom.seed(0)\n\nnew_records: typing.List[Record] = list()\nfor i in range(10):\n    age = int(random.random()*100) # number in [0,1]\n    is_old = age &gt; 50\n    new_records.append(Record(name='user_'+str(i), age=age, is_old=is_old))\n\n# insert new records\nwith rtab.query() as q:\n    print(q.insert_multi(new_records))\n\n# dataframe select (for example purposes - .df() will be covered later)\nwith core.query() as q:\n    r = q.select(rtab.all_cols()).df()\nr\n</code></pre> <pre><code>2023-11-13 08:38:32,104 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:32,104 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (age, is_old, name) VALUES (?, ?, ?)\n2023-11-13 08:38:32,105 INFO sqlalchemy.engine.Engine [generated in 0.00140s] [(84, True, 'user_0'), (75, True, 'user_1'), (42, False, 'user_2'), (25, False, 'user_3'), (51, True, 'user_4'), (40, False, 'user_5'), (78, True, 'user_6'), (30, False, 'user_7'), (47, False, 'user_8'), (58, True, 'user_9')]\n&lt;sqlalchemy.engine.cursor.CursorResult object at 0x7feef3d133f0&gt;\n2023-11-13 08:38:32,106 INFO sqlalchemy.engine.Engine COMMIT\n2023-11-13 08:38:32,109 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:32,109 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n2023-11-13 08:38:32,109 INFO sqlalchemy.engine.Engine [generated in 0.00102s] ()\n2023-11-13 08:38:32,112 INFO sqlalchemy.engine.Engine COMMIT\n</code></pre> id age is_old name 0 1 84 1 user_0 1 2 75 1 user_1 2 3 42 0 user_2 3 4 25 0 user_3 4 5 51 1 user_4 5 6 40 0 user_5 6 7 78 1 user_6 7 8 30 0 user_7 8 9 47 0 user_8 9 10 58 1 user_9"},{"location":"documentation/c-ex_select/#two-interfaces-connectquery-and-tablequery","title":"Two Interfaces: <code>ConnectQuery</code> and <code>TableQuery</code>","text":"<p>There are two interfaces for performing queries: <code>ConnectQuery</code> and <code>TableQuery</code>. </p> <ul> <li> <p><code>ConnectQuery</code> table-agnostic interface for querying any table in any result format.</p> </li> <li> <p><code>TableQuery</code> table-specific interface for querying a specific table. Insert and select from container objects used to define the schema.</p> </li> </ul> <pre><code># ConnectQuery - table agnostic\nwith core.query() as q:\n    print(type(q))\n\n# TableQuery - queries are relative to specific \n# table, results appear as container objects\nwith rtab.query() as q:\n    print(type(q))\n</code></pre> <pre><code>&lt;class 'doctable.query.connectquery.ConnectQuery'&gt;\n&lt;class 'doctable.query.tablequery.TableQuery'&gt;\n</code></pre>"},{"location":"documentation/c-ex_select/#connectquery-basics","title":"<code>ConnectQuery</code> Basics","text":"<p>First I will discuss the <code>ConnectQuery</code> interface, which is created via the <code>ConnectCore.query()</code> method. This object maintains a database connection, and, when used as a context manager, will commit all changes upon exit. It is fine to use the <code>ConnectQuery</code> object without a context manager for queries that do not require commits.</p> <p>This example is the most basic select query we can execute. Note that <code>ConnectQuery</code> methods are table-agnostic, so we must specify columns to be selected - in this case, we provide <code>rtab.all_cols()</code> to specify that we want to query all columns from the <code>Record</code> table. It returns a <code>sqlalchemy.CursorResult</code> object that we will discuss later.</p> <pre><code>core.query().select(rtab.all_cols())\n</code></pre> <pre><code>2023-11-13 08:38:32,204 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:32,205 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n2023-11-13 08:38:32,205 INFO sqlalchemy.engine.Engine [cached since 0.09654s ago] ()\n\n\n\n\n\n&lt;sqlalchemy.engine.cursor.CursorResult at 0x7feef3d88130&gt;\n</code></pre>"},{"location":"documentation/c-ex_select/#selecting-specific-columns","title":"Selecting Specific Columns","text":"<p>In many cases, you would not want to select all columns from a given table - for this reason, there are several methods you can use to specify the desired columns. In addition to <code>.all_cols()</code> in the above snippet, you may use any of these methods.</p> Method Description <code>.all_cols()</code> specify that we want all columns <code>.cols('col1', 'col2')</code> specify set of columns <code>table['col1']</code> specify single column <code>table[['col1', 'col2']]</code> specify multiple columns <code>table['col1':'col3']</code> specify sequential range of columns <pre><code># select all columns\nrtab.all_cols()\n</code></pre> <pre><code>[Column('id', Integer(), table=&lt;Record&gt;, primary_key=True),\n Column('age', Integer(), table=&lt;Record&gt;),\n Column('is_old', Integer(), table=&lt;Record&gt;),\n Column('name', String(), table=&lt;Record&gt;, nullable=False)]\n</code></pre> <pre><code># .cols method\nrtab.cols('id', 'name')\n</code></pre> <pre><code>[Column('id', Integer(), table=&lt;Record&gt;, primary_key=True),\n Column('name', String(), table=&lt;Record&gt;, nullable=False)]\n</code></pre> <pre><code># single-column subscript\nrtab['age']\n</code></pre> <pre><code>Column('age', Integer(), table=&lt;Record&gt;)\n</code></pre> <pre><code># list of columns\nrtab[['id','is_old']]\n</code></pre> <pre><code>[Column('id', Integer(), table=&lt;Record&gt;, primary_key=True),\n Column('is_old', Integer(), table=&lt;Record&gt;)]\n</code></pre> <pre><code># slice select\nrtab['id':'is_old']\n</code></pre> <pre><code>[Column('id', Integer(), table=&lt;Record&gt;, primary_key=True),\n Column('age', Integer(), table=&lt;Record&gt;),\n Column('is_old', Integer(), table=&lt;Record&gt;)]\n</code></pre> <p>Note that the <code>.select()</code> method requires a list of columns, so we can combine these methods by combining the lists they return. Obviously, the order matters for the returned values.</p> <pre><code>rtab.cols('id','is_old') + [rtab['name']]\n</code></pre> <pre><code>[Column('id', Integer(), table=&lt;Record&gt;, primary_key=True),\n Column('is_old', Integer(), table=&lt;Record&gt;),\n Column('name', String(), table=&lt;Record&gt;, nullable=False)]\n</code></pre> <p>The <code>.select()</code> method always accepts a list of columns, so be sure to wrap single-column selections in a list.</p> <pre><code>core.query().select([rtab['name']])\n</code></pre> <pre><code>2023-11-13 08:38:32,540 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:32,541 INFO sqlalchemy.engine.Engine SELECT \"Record\".name \nFROM \"Record\"\n2023-11-13 08:38:32,542 INFO sqlalchemy.engine.Engine [generated in 0.00130s] ()\n\n\n\n\n\n&lt;sqlalchemy.engine.cursor.CursorResult at 0x7feef3d12f90&gt;\n</code></pre> <pre><code>core.query().select(rtab.cols('id','is_old') + [rtab['name']])\n</code></pre> <pre><code>2023-11-13 08:38:32,589 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:32,590 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n2023-11-13 08:38:32,590 INFO sqlalchemy.engine.Engine [generated in 0.00142s] ()\n\n\n\n\n\n&lt;sqlalchemy.engine.cursor.CursorResult at 0x7feef3d89010&gt;\n</code></pre>"},{"location":"documentation/c-ex_select/#working-with-query-results","title":"Working with Query Results","text":"<p>Now we turn to working with the results objects. So far I have demonstrated values for returning <code>sqlalchemy.CursorResult</code> objects, but additional methods are required to return the results in a usable format. The following methods are available for various purposes:</p> Method Description <code>result.all()</code> return all results in query <code>result.df()</code> return multiple results as a dataframe <code>result.first()</code> return first result in query <code>result.one()</code> return exactly one result in query. NOTE: raises exception if not exactly one result. <code>result.scalar_one()</code> return single result, end query. NOTE: raises exception if not exactly one result. <code>result.scalars().all()</code> return single column of results <pre><code>with core.query() as q:\n    r = q.select(rtab.all_cols(), limit=3)\nr.all()\n</code></pre> <pre><code>2023-11-13 08:38:32,637 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:32,638 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n LIMIT ? OFFSET ?\n2023-11-13 08:38:32,639 INFO sqlalchemy.engine.Engine [generated in 0.00185s] (3, 0)\n2023-11-13 08:38:32,640 INFO sqlalchemy.engine.Engine COMMIT\n\n\n\n\n\n[(1, 84, 1, 'user_0'), (2, 75, 1, 'user_1'), (3, 42, 0, 'user_2')]\n</code></pre> <pre><code>core.query().select(rtab.all_cols(), limit=3).df()\n</code></pre> <pre><code>2023-11-13 08:38:32,689 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:32,690 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n LIMIT ? OFFSET ?\n2023-11-13 08:38:32,690 INFO sqlalchemy.engine.Engine [cached since 0.05336s ago] (3, 0)\n</code></pre> id age is_old name 0 1 84 1 user_0 1 2 75 1 user_1 2 3 42 0 user_2 <pre><code># raises exception without limit=1\ncore.query().select(rtab.all_cols()).first()\n</code></pre> <pre><code>2023-11-13 08:38:32,740 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:32,741 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n2023-11-13 08:38:32,742 INFO sqlalchemy.engine.Engine [cached since 0.6331s ago] ()\n\n\n\n\n\n(1, 84, 1, 'user_0')\n</code></pre> <pre><code># raises exception if more than one result is returned \n# (here I forced this with limit=1)\ncore.query().select(rtab.all_cols(), limit=1).one()\n</code></pre> <pre><code>2023-11-13 08:38:32,788 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:32,789 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n LIMIT ? OFFSET ?\n2023-11-13 08:38:32,790 INFO sqlalchemy.engine.Engine [cached since 0.1527s ago] (1, 0)\n\n\n\n\n\n(1, 84, 1, 'user_0')\n</code></pre> <pre><code># this returns the first column from the first row, then closes the cursor\ncore.query().select(rtab.all_cols(), limit=1).scalar_one()\n</code></pre> <pre><code>2023-11-13 08:38:32,837 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:32,838 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n LIMIT ? OFFSET ?\n2023-11-13 08:38:32,838 INFO sqlalchemy.engine.Engine [cached since 0.2012s ago] (1, 0)\n\n\n\n\n\n1\n</code></pre> <pre><code># it makes more sense to query a single column\ncore.query().select(rtab.cols('is_old'), limit=1).scalar_one()\n</code></pre> <pre><code>2023-11-13 08:38:32,889 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:32,890 INFO sqlalchemy.engine.Engine SELECT \"Record\".is_old \nFROM \"Record\"\n LIMIT ? OFFSET ?\n2023-11-13 08:38:32,890 INFO sqlalchemy.engine.Engine [generated in 0.00136s] (1, 0)\n\n\n\n\n\n1\n</code></pre> <pre><code># and when when you need a single column, use .scalars() instead of .all()\ncore.query().select(rtab.cols('is_old')).scalars().all()\n</code></pre> <pre><code>2023-11-13 08:38:32,940 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:32,941 INFO sqlalchemy.engine.Engine SELECT \"Record\".is_old \nFROM \"Record\"\n2023-11-13 08:38:32,941 INFO sqlalchemy.engine.Engine [generated in 0.00138s] ()\n\n\n\n\n\n[1, 1, 0, 0, 1, 0, 1, 0, 0, 1]\n</code></pre>"},{"location":"documentation/c-ex_select/#conditional-select-statements","title":"Conditional Select Statements","text":"operator description <code>&amp;</code>, <code>doctable.exp.and_()</code> and <code>\\|</code>, <code>doctable.exp.or_()</code> or <code>==</code> equals <code>!=</code>, <code>doctable.exp.not_()</code> not equals <code>&gt;</code> greater than <code>&gt;=</code> greater than or equal to <code>&lt;</code> less than <code>&lt;=</code> less than or equal to <code>in_()</code> in list <code>contains()</code> check if item is substring <code>like()</code> like string <code>ilike()</code> case-insensitive like string <code>between()</code>, <code>doctable.exp.between()</code> between two values <code>is_()</code> is value <code>isnot()</code> is not value <code>startswith()</code> starts with string <pre><code>core.query().select(rtab.all_cols(), where=rtab['id']==2).df()\n</code></pre> <pre><code>2023-11-13 08:38:32,989 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:32,990 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\" \nWHERE \"Record\".id = ?\n2023-11-13 08:38:32,990 INFO sqlalchemy.engine.Engine [generated in 0.00131s] (2,)\n</code></pre> id age is_old name 0 2 75 1 user_1 <pre><code>core.query().select(rtab.all_cols(), where=rtab['id']&lt;rtab['id']).df()\n</code></pre> <pre><code>2023-11-13 08:38:33,037 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:33,038 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\" \nWHERE \"Record\".id &lt; \"Record\".id\n2023-11-13 08:38:33,038 INFO sqlalchemy.engine.Engine [generated in 0.00128s] ()\n</code></pre> <pre><code>core.query().select(rtab.all_cols(), where=(rtab['id']%2)==0).df()\n</code></pre> <pre><code>2023-11-13 08:38:33,089 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:33,090 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\" \nWHERE \"Record\".id % ? = ?\n2023-11-13 08:38:33,091 INFO sqlalchemy.engine.Engine [generated in 0.00138s] (2, 0)\n</code></pre> id age is_old name 0 2 75 1 user_1 1 4 25 0 user_3 2 6 40 0 user_5 3 8 30 0 user_7 4 10 58 1 user_9 <pre><code>condition = (rtab['id']&gt;=2) &amp; (rtab['id']&lt;=4) &amp; (rtab['name']!='user_2')\ncore.query().select(rtab.all_cols(), where=condition).df()\n</code></pre> <pre><code>2023-11-13 08:38:33,142 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:33,143 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\" \nWHERE \"Record\".id &gt;= ? AND \"Record\".id &lt;= ? AND \"Record\".name != ?\n2023-11-13 08:38:33,143 INFO sqlalchemy.engine.Engine [generated in 0.00147s] (2, 4, 'user_2')\n</code></pre> id age is_old name 0 2 75 1 user_1 1 4 25 0 user_3 <pre><code>condition = rtab['name'].in_(('user_2','user_3'))\ncore.query().select(rtab.all_cols(), where=condition).df()\n</code></pre> <pre><code>2023-11-13 08:38:33,194 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:33,195 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\" \nWHERE \"Record\".name IN (?, ?)\n2023-11-13 08:38:33,195 INFO sqlalchemy.engine.Engine [generated in 0.00146s] ('user_2', 'user_3')\n</code></pre> id age is_old name 0 3 42 0 user_2 1 4 25 0 user_3 <pre><code>core.query().select(rtab.all_cols(), where=rtab['id'].between(2,4)).df()\n</code></pre> <pre><code>2023-11-13 08:38:33,246 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:33,247 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\" \nWHERE \"Record\".id BETWEEN ? AND ?\n2023-11-13 08:38:33,247 INFO sqlalchemy.engine.Engine [generated in 0.00134s] (2, 4)\n</code></pre> id age is_old name 0 2 75 1 user_1 1 3 42 0 user_2 2 4 25 0 user_3 <pre><code>condition = ~(rtab['name'].in_(('user_2','user_3'))) &amp; (rtab['id'] &lt; 4)\ncore.query().select(rtab.all_cols(), where=condition).df()\n</code></pre> <pre><code>2023-11-13 08:38:33,298 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:33,299 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\" \nWHERE (\"Record\".name NOT IN (?, ?)) AND \"Record\".id &lt; ?\n2023-11-13 08:38:33,300 INFO sqlalchemy.engine.Engine [generated in 0.00171s] ('user_2', 'user_3', 4)\n</code></pre> id age is_old name 0 1 84 1 user_0 1 2 75 1 user_1 <pre><code>condition = doctable.exp.or_(doctable.exp.not_(rtab['id']==4)) &amp; (rtab['id'] &lt;= 2)\ncore.query().select(rtab.all_cols(), where=condition).df()\n</code></pre> <pre><code>2023-11-13 08:38:33,350 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:33,351 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\" \nWHERE \"Record\".id != ? AND \"Record\".id &lt;= ?\n2023-11-13 08:38:33,352 INFO sqlalchemy.engine.Engine [generated in 0.00181s] (4, 2)\n</code></pre> id age is_old name 0 1 84 1 user_0 1 2 75 1 user_1 <pre><code>with core.query() as q:\n    ages = q.select([rtab['age']]).scalars().all()\n    mean_age = sum(ages)/len(ages)\n    result = q.select(rtab.all_cols(), where=rtab['age']&gt;mean_age)\nresult.df()\n</code></pre> <pre><code>2023-11-13 08:38:33,402 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:33,404 INFO sqlalchemy.engine.Engine SELECT \"Record\".age \nFROM \"Record\"\n2023-11-13 08:38:33,404 INFO sqlalchemy.engine.Engine [generated in 0.00155s] ()\n2023-11-13 08:38:33,406 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\" \nWHERE \"Record\".age &gt; ?\n2023-11-13 08:38:33,406 INFO sqlalchemy.engine.Engine [generated in 0.00045s] (53.0,)\n2023-11-13 08:38:33,407 INFO sqlalchemy.engine.Engine COMMIT\n</code></pre> id age is_old name 0 1 84 1 user_0 1 2 75 1 user_1 2 7 78 1 user_6 3 10 58 1 user_9"},{"location":"documentation/c-ex_select/#column-operators","title":"Column Operators","text":"<p>In addition to any of the methods used for conditional selects, there are several additional methods that can be used to transform columns in the select statement.</p> Method Description <code>.label()</code> rename column in result (particularly useful after transformations) <code>.min()</code>, <code>doctable.f.min()</code> max of column values <code>.max()</code>, <code>doctable.f.max()</code> max of column values <code>.sum()</code>, <code>doctable.f.sum()</code> sum of column <code>.count()</code>, <code>doctable.f.count()</code> count number of results. NOTE: must use <code>f.count()</code> when counting transformed columns. <code>.distinct()</code>, <code>doctable.f.distinct()</code> get distinct values <code>/</code> divide <code>*</code> multiply <code>+</code> add <code>-</code> subtract <code>%</code> modulo <code>.concat()</code> concatenate strings <pre><code>columns = [\n    (rtab['id'] % 2).label('mod_id'), \n    rtab['name'].label('myname')\n]\ncore.query().select(columns, where=rtab['is_old']).df()\n</code></pre> <pre><code>2023-11-13 08:38:33,457 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:33,458 INFO sqlalchemy.engine.Engine SELECT \"Record\".id % ? AS mod_id, \"Record\".name AS myname \nFROM \"Record\" \nWHERE \"Record\".is_old\n2023-11-13 08:38:33,458 INFO sqlalchemy.engine.Engine [generated in 0.00153s] (2,)\n</code></pre> mod_id myname 0 1 user_0 1 0 user_1 2 1 user_4 3 1 user_6 4 0 user_9 <pre><code>formula = rtab['age'].sum() / rtab['age'].count()\ncore.query().select([formula]).scalar_one()\n</code></pre> <pre><code>2023-11-13 08:38:33,510 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:33,511 INFO sqlalchemy.engine.Engine SELECT sum(\"Record\".age) / (count(\"Record\".age) + 0.0) AS anon_1 \nFROM \"Record\"\n2023-11-13 08:38:33,511 INFO sqlalchemy.engine.Engine [generated in 0.00153s] ()\n\n\n\n\n\nDecimal('53.0000000000')\n</code></pre> <pre><code>formula = rtab['age'].max() - rtab['age'].min()\ncore.query().select([formula]).scalar_one()\n</code></pre> <pre><code>2023-11-13 08:38:33,561 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:33,562 INFO sqlalchemy.engine.Engine SELECT max(\"Record\".age) - min(\"Record\".age) AS anon_1 \nFROM \"Record\"\n2023-11-13 08:38:33,563 INFO sqlalchemy.engine.Engine [generated in 0.00155s] ()\n\n\n\n\n\n59\n</code></pre> <pre><code># average age of individuals over 30\nformula = rtab['age'].sum() / rtab['age'].count()\ncondition = rtab['age'] &gt; 30\ncore.query().select([formula], where=condition).scalar_one()\n</code></pre> <pre><code>2023-11-13 08:38:33,614 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:33,615 INFO sqlalchemy.engine.Engine SELECT sum(\"Record\".age) / (count(\"Record\".age) + 0.0) AS anon_1 \nFROM \"Record\" \nWHERE \"Record\".age &gt; ?\n2023-11-13 08:38:33,615 INFO sqlalchemy.engine.Engine [generated in 0.00140s] (30,)\n\n\n\n\n\nDecimal('59.3750000000')\n</code></pre> <pre><code># descriptive stats on age of individuals over 30\ncolumns = [\n    (rtab['age'].sum() / rtab['age'].count()).label('mean'),\n    rtab['age'].max().label('max'),\n    rtab['age'].min().label('min'),\n]\ncondition = rtab['age'] &gt; 30\ncore.query().select(columns, where=condition).df()\n</code></pre> <pre><code>2023-11-13 08:38:33,666 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:33,667 INFO sqlalchemy.engine.Engine SELECT sum(\"Record\".age) / (count(\"Record\".age) + 0.0) AS mean, max(\"Record\".age) AS max, min(\"Record\".age) AS min \nFROM \"Record\" \nWHERE \"Record\".age &gt; ?\n2023-11-13 08:38:33,667 INFO sqlalchemy.engine.Engine [generated in 0.00142s] (30,)\n</code></pre> mean max min 0 59.3750000000 84 40 <pre><code># all distinct values\nformula = rtab['is_old'].distinct()\ncore.query().select([formula]).df()\n</code></pre> <pre><code>2023-11-13 08:38:33,717 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:33,718 INFO sqlalchemy.engine.Engine SELECT distinct(\"Record\".is_old) AS distinct_1 \nFROM \"Record\"\n2023-11-13 08:38:33,718 INFO sqlalchemy.engine.Engine [generated in 0.00171s] ()\n</code></pre> distinct_1 0 1 1 0 <pre><code># count individuals over 30\ncore.query().select([doctable.f.count()], where=rtab['age']&gt;30).scalar_one()\n</code></pre> <pre><code>2023-11-13 08:38:33,765 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:33,766 INFO sqlalchemy.engine.Engine SELECT count(*) AS count_1 \nFROM \"Record\" \nWHERE \"Record\".age &gt; ?\n2023-11-13 08:38:33,767 INFO sqlalchemy.engine.Engine [generated in 0.00157s] (30,)\n\n\n\n\n\n8\n</code></pre> <pre><code># similar to previous\ncore.query().select([rtab['id'].count()], where=rtab['age']&gt;30).scalar_one()\n</code></pre> <pre><code>2023-11-13 08:38:33,817 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:33,818 INFO sqlalchemy.engine.Engine SELECT count(\"Record\".id) AS count_1 \nFROM \"Record\" \nWHERE \"Record\".age &gt; ?\n2023-11-13 08:38:33,818 INFO sqlalchemy.engine.Engine [generated in 0.00127s] (30,)\n\n\n\n\n\n8\n</code></pre>"},{"location":"documentation/c-ex_select/#additional-parameters-order-by-group-by-limit-offset","title":"Additional Parameters: Order By, Group By, Limit, Offset","text":"<p>More complicated queries involving ordering, grouping, limiting, and specifying offset can be specified using parameters to the <code>.select()</code> method.</p> Parameter Description <code>limit</code> limit number of results <code>order_by</code> list of columns to order by <code>group_by</code> list of columns to group by <code>offset</code> offset results by specified number"},{"location":"documentation/c-ex_select/#order-by-and-limits","title":"Order By and Limits","text":"<pre><code># get the five youngest individuals in order\ncore.query().select(rtab.all_cols(), order_by=rtab['age'], limit=5).df()\n</code></pre> <pre><code>2023-11-13 08:38:33,865 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:33,865 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\" ORDER BY \"Record\".age\n LIMIT ? OFFSET ?\n2023-11-13 08:38:33,866 INFO sqlalchemy.engine.Engine [generated in 0.00127s] (5, 0)\n</code></pre> id age is_old name 0 4 25 0 user_3 1 8 30 0 user_7 2 6 40 0 user_5 3 3 42 0 user_2 4 9 47 0 user_8 <pre><code># get the five oldest now\ncore.query().select(rtab.all_cols(), order_by=rtab['age'].desc(), limit=5).df()\n</code></pre> <pre><code>2023-11-13 08:38:33,913 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:33,914 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\" ORDER BY \"Record\".age DESC\n LIMIT ? OFFSET ?\n2023-11-13 08:38:33,915 INFO sqlalchemy.engine.Engine [generated in 0.00165s] (5, 0)\n</code></pre> id age is_old name 0 1 84 1 user_0 1 7 78 1 user_6 2 2 75 1 user_1 3 10 58 1 user_9 4 5 51 1 user_4 <pre><code># order by is_old, but preserve order of id otherwise\norder = [\n    rtab['is_old'].desc(),\n    rtab['id'].asc(),\n]\ncore.query().select(rtab.all_cols(), order_by=order, limit=5).df()\n</code></pre> <pre><code>2023-11-13 08:38:33,965 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:33,966 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\" ORDER BY \"Record\".is_old DESC, \"Record\".id ASC\n LIMIT ? OFFSET ?\n2023-11-13 08:38:33,966 INFO sqlalchemy.engine.Engine [generated in 0.00135s] (5, 0)\n</code></pre> id age is_old name 0 1 84 1 user_0 1 2 75 1 user_1 2 5 51 1 user_4 3 7 78 1 user_6 4 10 58 1 user_9"},{"location":"documentation/c-ex_select/#grouping-and-column-operators","title":"Grouping and Column Operators","text":"<pre><code># summary stats by is_old\ncols = [\n    #rtab['is_old'].count().label('count'),\n    doctable.f.count().label('count'),\n    rtab['age'].min().label('min'),\n    rtab['age'].max().label('max'),\n    (rtab['age'].sum()/rtab['age'].count()).label('mean'),\n]\ncore.query().select(cols, group_by=rtab['is_old']).df()\n</code></pre> <pre><code>2023-11-13 08:38:34,018 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:34,019 INFO sqlalchemy.engine.Engine SELECT count(*) AS count, min(\"Record\".age) AS min, max(\"Record\".age) AS max, sum(\"Record\".age) / (count(\"Record\".age) + 0.0) AS mean \nFROM \"Record\" GROUP BY \"Record\".is_old\n2023-11-13 08:38:34,019 INFO sqlalchemy.engine.Engine [generated in 0.00141s] ()\n</code></pre> count min max mean 0 5 25 47 36.8000000000 1 5 51 84 69.2000000000 <pre><code># summarize age by decade\ndecade_expression = doctable.f.round(rtab['age'] / 10)\ncols = [\n    decade_expression.label('decade'),\n    rtab['age'].count().label('count'),\n    rtab['age'].min().label('min'),\n    rtab['age'].max().label('max'),\n    (rtab['age'].sum()/rtab['age'].count()).label('mean'),\n]\ncore.query().select(cols, group_by=decade_expression).df()\n</code></pre> <pre><code>2023-11-13 08:38:34,070 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:34,071 INFO sqlalchemy.engine.Engine SELECT round(\"Record\".age / (? + 0.0)) AS decade, count(\"Record\".age) AS count, min(\"Record\".age) AS min, max(\"Record\".age) AS max, sum(\"Record\".age) / (count(\"Record\".age) + 0.0) AS mean \nFROM \"Record\" GROUP BY round(\"Record\".age / (? + 0.0))\n2023-11-13 08:38:34,072 INFO sqlalchemy.engine.Engine [generated in 0.00134s] (10, 10)\n</code></pre> decade count min max mean 0 3.0 2 25 30 27.5000000000 1 4.0 2 40 42 41.0000000000 2 5.0 2 47 51 49.0000000000 3 6.0 1 58 58 58.0000000000 4 8.0 3 75 84 79.0000000000"},{"location":"documentation/c-ex_select/#offset-and-selecting-chunks","title":"Offset and Selecting Chunks","text":"<p>The <code>offset</code> parameter is used to pagify results into multiple queries - something that is particularly useful if the result set is too larget to fit into memory.</p> <pre><code># get the three oldest individuals, offset by three\ncore.query().select(rtab.all_cols(), order_by=rtab['age'].desc(), limit=3, offset=3).df()\n</code></pre> <pre><code>2023-11-13 08:38:34,121 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:34,121 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\" ORDER BY \"Record\".age DESC\n LIMIT ? OFFSET ?\n2023-11-13 08:38:34,122 INFO sqlalchemy.engine.Engine [generated in 0.00137s] (3, 3)\n</code></pre> id age is_old name 0 10 58 1 user_9 1 5 51 1 user_4 2 9 47 0 user_8 <pre><code>for chunk in core.query().select_chunks(rtab.all_cols(), chunksize=3):\n    print(chunk)\n</code></pre> <pre><code>2023-11-13 08:38:34,169 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:34,169 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n LIMIT ? OFFSET ?\n2023-11-13 08:38:34,170 INFO sqlalchemy.engine.Engine [generated in 0.00117s] (3, 0)\n[(1, 84, 1, 'user_0'), (2, 75, 1, 'user_1'), (3, 42, 0, 'user_2')]\n2023-11-13 08:38:34,171 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n LIMIT ? OFFSET ?\n2023-11-13 08:38:34,171 INFO sqlalchemy.engine.Engine [cached since 0.002855s ago] (3, 3)\n[(4, 25, 0, 'user_3'), (5, 51, 1, 'user_4'), (6, 40, 0, 'user_5')]\n2023-11-13 08:38:34,172 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n LIMIT ? OFFSET ?\n2023-11-13 08:38:34,173 INFO sqlalchemy.engine.Engine [cached since 0.004328s ago] (3, 6)\n[(7, 78, 1, 'user_6'), (8, 30, 0, 'user_7'), (9, 47, 0, 'user_8')]\n2023-11-13 08:38:34,175 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n LIMIT ? OFFSET ?\n2023-11-13 08:38:34,175 INFO sqlalchemy.engine.Engine [cached since 0.006853s ago] (3, 9)\n[(10, 58, 1, 'user_9')]\n</code></pre>"},{"location":"documentation/c-ex_select/#tablequery-basics","title":"<code>TableQuery</code> Basics","text":"<p>The <code>TableQuery</code> interface is used to make table-specific queries, and, in exchange for this restriction, allows you to insert and select container objects directly. Queries on tables look much like their table-agnostic counterparts, with a few exceptions. Every query still begins with the <code>.query()</code> method, which returns a <code>TableQuery</code> object with methods for inserting and selecting container objects.</p> <ol> <li> <p>List of selected columns is optional - if you do not specify, the query will default to all columns in the table. Otherwise, you should provide a subset of the columns, where all attributes that were not received will refer to <code>doctable.MISSING</code>, which you may check for downstream.</p> </li> <li> <p>Results of a select query are returned as a list of container objects. This means that we have called the <code>.all()</code> method on the <code>sqlalchemy.CursorResult</code> object.</p> </li> <li> <p>All returned results must match the attributes of the container object. Most often you will want to select raw database rows, but transformations via <code>group_by</code> and other operators are also possible as long as the result set attributes match the container attributes - that is, they can be expanded to the container constructor.</p> </li> <li> <p>Behavior of <code>where</code>, <code>order_by</code>, <code>limit</code>, are <code>offset</code> all operate as-expected.</p> </li> </ol> <p>Below you can see a few examples demonstrating this behavior.</p> <pre><code>rtab.query().select(where=rtab['age']&gt;50)\n</code></pre> <pre><code>2023-11-13 08:38:34,217 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:34,218 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\" \nWHERE \"Record\".age &gt; ?\n2023-11-13 08:38:34,219 INFO sqlalchemy.engine.Engine [generated in 0.00128s] (50,)\n\n\n\n\n\n[Record(name='user_0', age=84, is_old=1, id=1),\n Record(name='user_1', age=75, is_old=1, id=2),\n Record(name='user_4', age=51, is_old=1, id=5),\n Record(name='user_6', age=78, is_old=1, id=7),\n Record(name='user_9', age=58, is_old=1, id=10)]\n</code></pre> <pre><code>result = rtab.query().select(rtab.cols('id', 'age'), where=rtab['is_old'])\nf'{result[0].name is doctable.MISSING=}', result\n</code></pre> <pre><code>2023-11-13 08:38:34,268 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:34,269 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age \nFROM \"Record\" \nWHERE \"Record\".is_old\n2023-11-13 08:38:34,270 INFO sqlalchemy.engine.Engine [generated in 0.00134s] ()\n\n\n\n\n\n('result[0].name is doctable.MISSING=True',\n [Record(name=MISSING, age=84, is_old=MISSING, id=1),\n  Record(name=MISSING, age=75, is_old=MISSING, id=2),\n  Record(name=MISSING, age=51, is_old=MISSING, id=5),\n  Record(name=MISSING, age=78, is_old=MISSING, id=7),\n  Record(name=MISSING, age=58, is_old=MISSING, id=10)])\n</code></pre> <pre><code># this is valid, although perhaps not recommended\ncols = [\n    (rtab['age'].sum()/rtab['age'].count()).label('age'),\n]\nrtab.query().select(cols=cols)\n</code></pre> <pre><code>2023-11-13 08:38:34,317 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:34,318 INFO sqlalchemy.engine.Engine SELECT sum(\"Record\".age) / (count(\"Record\".age) + 0.0) AS age \nFROM \"Record\"\n2023-11-13 08:38:34,318 INFO sqlalchemy.engine.Engine [generated in 0.00123s] ()\n\n\n\n\n\n[Record(name=MISSING, age=Decimal('53.0000000000'), is_old=MISSING, id=MISSING)]\n</code></pre> <p><code>.select_chunks()</code> also more or less works as-expected, with the chunks being converted to container objects.</p> <pre><code>for chunk in rtab.query().select_chunks(chunksize=3):\n    print(chunk)\n</code></pre> <pre><code>2023-11-13 08:38:34,364 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 08:38:34,365 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n LIMIT ? OFFSET ?\n2023-11-13 08:38:34,365 INFO sqlalchemy.engine.Engine [cached since 0.1964s ago] (3, 0)\n[Record(name='user_0', age=84, is_old=1, id=1), Record(name='user_1', age=75, is_old=1, id=2), Record(name='user_2', age=42, is_old=0, id=3)]\n2023-11-13 08:38:34,366 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n LIMIT ? OFFSET ?\n2023-11-13 08:38:34,367 INFO sqlalchemy.engine.Engine [cached since 0.198s ago] (3, 3)\n[Record(name='user_3', age=25, is_old=0, id=4), Record(name='user_4', age=51, is_old=1, id=5), Record(name='user_5', age=40, is_old=0, id=6)]\n2023-11-13 08:38:34,368 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n LIMIT ? OFFSET ?\n2023-11-13 08:38:34,368 INFO sqlalchemy.engine.Engine [cached since 0.1994s ago] (3, 6)\n[Record(name='user_6', age=78, is_old=1, id=7), Record(name='user_7', age=30, is_old=0, id=8), Record(name='user_8', age=47, is_old=0, id=9)]\n2023-11-13 08:38:34,369 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n LIMIT ? OFFSET ?\n2023-11-13 08:38:34,369 INFO sqlalchemy.engine.Engine [cached since 0.2007s ago] (3, 9)\n[Record(name='user_9', age=58, is_old=1, id=10)]\n</code></pre>"},{"location":"documentation/d-ex_insert_delete/","title":"Insert and Delete Queries","text":"<p>In this document I will describe the interface for performing insert and delete queries with doctable.</p> <pre><code>\nimport pandas as pd\nimport numpy as np\nimport typing\n\nimport sys\nsys.path.append('..')\nimport doctable\n</code></pre>"},{"location":"documentation/d-ex_insert_delete/#define-a-demonstration-schema","title":"Define a demonstration schema","text":"<p>The very first step is to define a table schema that will be appropriate for our examples. This table includes the typical <code>id</code> column (the first column, specified by <code>order=0</code>), as well as string, integer, and boolean attributes. The object used to specify the schema is called a container, and I will use that terminology as we go.</p> <pre><code>@doctable.table_schema\nclass Record:\n    name: str = doctable.Column(column_args=doctable.ColumnArgs(nullable=False, unique=True))\n    age: int = doctable.Column()\n    is_old: bool = doctable.Column()\n\n    id: int = doctable.Column(\n        column_args=doctable.ColumnArgs(\n            order = 0, \n            primary_key=True, \n            autoincrement=True\n        ),\n    )\n\ncore = doctable.ConnectCore.open(target=':memory:', dialect='sqlite', echo=True)\n\nwith core.begin_ddl() as ddl:\n    rtab = ddl.create_table_if_not_exists(container_type=Record)\n</code></pre> <pre><code>2023-11-13 14:46:03,418 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:46:03,418 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"Record\")\n2023-11-13 14:46:03,419 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-11-13 14:46:03,420 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"Record\")\n2023-11-13 14:46:03,421 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-11-13 14:46:03,422 INFO sqlalchemy.engine.Engine \nCREATE TABLE \"Record\" (\n    id INTEGER, \n    age INTEGER, \n    is_old INTEGER, \n    name VARCHAR NOT NULL, \n    PRIMARY KEY (id), \n    UNIQUE (name)\n)\n\n\n2023-11-13 14:46:03,423 INFO sqlalchemy.engine.Engine [no key 0.00050s] ()\n2023-11-13 14:46:03,424 INFO sqlalchemy.engine.Engine COMMIT\n</code></pre>"},{"location":"documentation/d-ex_insert_delete/#two-interfaces-connectquery-and-tablequery","title":"Two Interfaces: <code>ConnectQuery</code> and <code>TableQuery</code>","text":"<p>First, a little about the <code>doctable</code> query interface. There are two interfaces for performing queries: <code>ConnectQuery</code> and <code>TableQuery</code>. </p> <ul> <li> <p><code>ConnectQuery</code> table-agnostic interface for querying any table in any result format. Create this object using the <code>ConnectCore.query()</code> method.</p> </li> <li> <p><code>TableQuery</code> table-specific interface for querying a specific table. Insert and select from container objects used to define the schema. Create this object using the <code>DBTable.query()</code> method.</p> </li> </ul>"},{"location":"documentation/d-ex_insert_delete/#inserts-via-connectquery","title":"Inserts via <code>ConnectQuery</code>","text":"<p>First I will discuss the <code>ConnectQuery</code> interface, which is created via the <code>ConnectCore.query()</code> method. This object maintains a database connection, and, when used as a context manager, will commit all changes upon exit. It is fine to use the <code>ConnectQuery</code> object without a context manager for queries that do not require commits.</p> <p>There are two primary methods for insertions via the <code>ConnectQuery</code> interface, which you can see in this table. Both accept a single <code>DBTable</code> object, followed by one or multiple dictionaries of data to insert, depending on the method.</p> Method Description <code>insert_single()</code> Insert a single row into a table. <code>insert_multi()</code> Insert multiple rows into a table. <pre><code>with core.query() as q:\n    q.insert_single(rtab, {\n        'name': 'test_A',\n        'age': 10,\n        'is_old': False,\n    })\n\n    q.insert_multi(rtab, data = [\n        {\n            'name': 'test_B',\n            'age': 10,\n            'is_old': False,\n        },\n        {\n            'name': 'test_C',\n            'age': 10,\n            'is_old': False,\n        }\n    ])\n\nq.select(rtab.all_cols()).df()\n</code></pre> <pre><code>2023-11-13 14:46:03,478 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:46:03,480 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (age, is_old, name) VALUES (?, ?, ?)\n2023-11-13 14:46:03,482 INFO sqlalchemy.engine.Engine [generated in 0.00420s] (10, False, 'test_A')\n2023-11-13 14:46:03,487 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (age, is_old, name) VALUES (?, ?, ?)\n2023-11-13 14:46:03,489 INFO sqlalchemy.engine.Engine [generated in 0.00216s] [(10, False, 'test_B'), (10, False, 'test_C')]\n2023-11-13 14:46:03,491 INFO sqlalchemy.engine.Engine COMMIT\n2023-11-13 14:46:03,494 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:46:03,495 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n2023-11-13 14:46:03,497 INFO sqlalchemy.engine.Engine [generated in 0.00319s] ()\n</code></pre> id age is_old name 0 1 10 0 test_A 1 2 10 0 test_B 2 3 10 0 test_C"},{"location":"documentation/d-ex_insert_delete/#omit-attributes","title":"Omit Attributes","text":"<p>If some values are not provided, the database will decide which values they take. In this case, the database populates the ID column according to the schema (it acts as the primary key in this case).</p> <pre><code>core.query().insert_single(rtab, {\n    'name': 'test_D',\n})\ncore.query().select(rtab.all_cols()).df()\n</code></pre> <pre><code>2023-11-13 14:46:03,544 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:46:03,546 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (name) VALUES (?)\n2023-11-13 14:46:03,548 INFO sqlalchemy.engine.Engine [generated in 0.00389s] ('test_D',)\n2023-11-13 14:46:03,551 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:46:03,552 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n2023-11-13 14:46:03,554 INFO sqlalchemy.engine.Engine [cached since 0.06015s ago] ()\n</code></pre> id age is_old name 0 1 10.0 0.0 test_A 1 2 10.0 0.0 test_B 2 3 10.0 0.0 test_C 3 4 NaN NaN test_D <p>Note that in our schema we set <code>nullable=False</code> for the name column, so this must be provided in an insert otherwise there will be an error. This typically results in an <code>sqlalchemy.exc.IntegrityError</code>, which you may catch if needed.</p> <pre><code>import sqlalchemy.exc\n\ntry:\n    core.query().insert_single(rtab, {\n        'is_old': True,\n    })\nexcept sqlalchemy.exc.IntegrityError as e:\n    print(type(e), e)\n</code></pre> <pre><code>2023-11-13 14:46:03,605 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:46:03,607 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (is_old) VALUES (?)\n2023-11-13 14:46:03,609 INFO sqlalchemy.engine.Engine [generated in 0.00380s] (True,)\n&lt;class 'sqlalchemy.exc.IntegrityError'&gt; (sqlite3.IntegrityError) NOT NULL constraint failed: Record.name\n[SQL: INSERT OR FAIL INTO \"Record\" (is_old) VALUES (?)]\n[parameters: (True,)]\n(Background on this error at: https://sqlalche.me/e/20/gkpj)\n</code></pre>"},{"location":"documentation/d-ex_insert_delete/#ifnotunique-parameter","title":"<code>ifnotunique</code> Parameter","text":"<p>The <code>ifnotunique</code> paramter controls the behavior when a unique constraint is violated. </p> <p>The default value is <code>FAIL</code>, which will raise an error when a unique constraint is violated - it will raise an <code>sqlalchemy.exc.IntegrityError</code> exception in this case. The other options are <code>IGNORE</code>, meaning inserted rows that violate the constraints should be ignored, and <code>REPLACE</code>, which will replace the existing row with the new row.</p> <p>In the <code>Record</code> table we have created, there is a unique constraint on <code>name</code>. We will receive an integrity error if we try to insert a duplicate there when using the default <code>ifnotunique='ERROR'</code>.</p> <pre><code>try:\n    core.query().insert_single(rtab, {\n        'name': 'test_A',\n    })\nexcept sqlalchemy.exc.IntegrityError as e:\n    print(type(e), e)\n</code></pre> <pre><code>2023-11-13 14:46:03,665 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:46:03,668 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (name) VALUES (?)\n2023-11-13 14:46:03,669 INFO sqlalchemy.engine.Engine [cached since 0.1247s ago] ('test_A',)\n&lt;class 'sqlalchemy.exc.IntegrityError'&gt; (sqlite3.IntegrityError) UNIQUE constraint failed: Record.name\n[SQL: INSERT OR FAIL INTO \"Record\" (name) VALUES (?)]\n[parameters: ('test_A',)]\n(Background on this error at: https://sqlalche.me/e/20/gkpj)\n</code></pre> <p>When using <code>ifnotunique='REPLACE'</code>, the insert will replace the existing row with the new row. This is useful when you want to update a row if it already exists, but insert it if it does not.</p> <pre><code>core.query().insert_single(rtab, {\n    'name': 'test_A',\n}, ifnotunique='REPLACE')\ncore.query().select(rtab.all_cols()).df()\n</code></pre> <pre><code>2023-11-13 14:46:03,728 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:46:03,731 INFO sqlalchemy.engine.Engine INSERT OR REPLACE INTO \"Record\" (name) VALUES (?)\n2023-11-13 14:46:03,733 INFO sqlalchemy.engine.Engine [generated in 0.00453s] ('test_A',)\n2023-11-13 14:46:03,738 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:46:03,739 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n2023-11-13 14:46:03,741 INFO sqlalchemy.engine.Engine [cached since 0.2469s ago] ()\n</code></pre> id age is_old name 0 2 10.0 0.0 test_B 1 3 10.0 0.0 test_C 2 4 NaN NaN test_D 3 5 NaN NaN test_A"},{"location":"documentation/d-ex_insert_delete/#inserts-via-tablequery","title":"Inserts via <code>TableQuery</code>","text":"<p>The <code>TableQuery</code> interface is created via the <code>DBTable.query()</code> method. This object is table-specific, and is used to insert and select from a single table. As such, inserts ONLY accept the <code>Record</code> container objects used to define the schema. Following the <code>ConnectQuery</code> interface, there are two methods for inserting data into a table: <code>.insert_single()</code> and <code>.insert_multi()</code>.</p> <pre><code>rtab.query().insert_single(Record(name='test_E', is_old=False, age=10))\nrtab.query().select(rtab.all_cols())\n</code></pre> <pre><code>2023-11-13 14:46:03,806 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:46:03,809 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (age, is_old, name) VALUES (?, ?, ?)\n2023-11-13 14:46:03,811 INFO sqlalchemy.engine.Engine [cached since 0.3334s ago] (10, False, 'test_E')\n2023-11-13 14:46:03,814 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:46:03,815 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n2023-11-13 14:46:03,816 INFO sqlalchemy.engine.Engine [cached since 0.3218s ago] ()\n\n\n\n\n\n[Record(name='test_B', age=10, is_old=0, id=2),\n Record(name='test_C', age=10, is_old=0, id=3),\n Record(name='test_D', age=None, is_old=None, id=4),\n Record(name='test_A', age=None, is_old=None, id=5),\n Record(name='test_E', age=10, is_old=0, id=6)]\n</code></pre> <pre><code>rtab.query().insert_multi([\n    Record(name='test_F', is_old=False, age=10),\n    Record(name='test_G', is_old=True, age=80),\n])\nrtab.query().select(rtab.all_cols())\n</code></pre> <pre><code>2023-11-13 14:46:03,866 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:46:03,869 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (age, is_old, name) VALUES (?, ?, ?)\n2023-11-13 14:46:03,870 INFO sqlalchemy.engine.Engine [cached since 0.3834s ago] [(10, False, 'test_F'), (80, True, 'test_G')]\n2023-11-13 14:46:03,874 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:46:03,875 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n2023-11-13 14:46:03,876 INFO sqlalchemy.engine.Engine [cached since 0.3818s ago] ()\n\n\n\n\n\n[Record(name='test_B', age=10, is_old=0, id=2),\n Record(name='test_C', age=10, is_old=0, id=3),\n Record(name='test_D', age=None, is_old=None, id=4),\n Record(name='test_A', age=None, is_old=None, id=5),\n Record(name='test_E', age=10, is_old=0, id=6),\n Record(name='test_F', age=10, is_old=0, id=7),\n Record(name='test_G', age=80, is_old=1, id=8)]\n</code></pre>"},{"location":"documentation/d-ex_insert_delete/#the-doctablemissing-sentinel","title":"The <code>doctable.MISSING</code> Sentinel","text":"<p>Lets now take a closer look at the container object behavior. Notice that in the schema definition we gave default values of <code>doctable.Column</code>, which we used to specify additional attributes. This automatically sets the default value for the dataclass to be <code>doctable.MISSING</code>, which is special because it will be ignored when inserting - instead, it will let the database decide how to handle it. This is especially useful for columns like <code>id</code>, which are intended to be automatically generated by the database. We can see this when we omit attributes from the object.</p> <pre><code>test_record = Record(name='test_H')\ntest_record\n</code></pre> <pre><code>Record(name='test_H', age=MISSING, is_old=MISSING, id=MISSING)\n</code></pre> <p>Those values will be omitted in the insert, filled in by the db, and be returned upon selection.</p> <pre><code>rtab.query().insert_single(test_record)\nrtab.query().select(rtab.all_cols())\n</code></pre> <pre><code>2023-11-13 14:46:03,982 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:46:03,985 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (name) VALUES (?)\n2023-11-13 14:46:03,987 INFO sqlalchemy.engine.Engine [cached since 0.4426s ago] ('test_H',)\n2023-11-13 14:46:03,990 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:46:03,991 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name \nFROM \"Record\"\n2023-11-13 14:46:03,992 INFO sqlalchemy.engine.Engine [cached since 0.4979s ago] ()\n\n\n\n\n\n[Record(name='test_B', age=10, is_old=0, id=2),\n Record(name='test_C', age=10, is_old=0, id=3),\n Record(name='test_D', age=None, is_old=None, id=4),\n Record(name='test_A', age=None, is_old=None, id=5),\n Record(name='test_E', age=10, is_old=0, id=6),\n Record(name='test_F', age=10, is_old=0, id=7),\n Record(name='test_G', age=80, is_old=1, id=8),\n Record(name='test_H', age=None, is_old=None, id=9)]\n</code></pre> <p>If we select a subset of columns, the missing values will refer to <code>doctable.MISSING</code>, even though the attributes will continue to exist.</p> <pre><code>rtab.query().select(rtab.cols('id', 'name'))\n</code></pre> <pre><code>2023-11-13 14:46:04,041 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:46:04,044 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".name \nFROM \"Record\"\n2023-11-13 14:46:04,045 INFO sqlalchemy.engine.Engine [generated in 0.00350s] ()\n\n\n\n\n\n[Record(name='test_A', age=MISSING, is_old=MISSING, id=5),\n Record(name='test_B', age=MISSING, is_old=MISSING, id=2),\n Record(name='test_C', age=MISSING, is_old=MISSING, id=3),\n Record(name='test_D', age=MISSING, is_old=MISSING, id=4),\n Record(name='test_E', age=MISSING, is_old=MISSING, id=6),\n Record(name='test_F', age=MISSING, is_old=MISSING, id=7),\n Record(name='test_G', age=MISSING, is_old=MISSING, id=8),\n Record(name='test_H', age=MISSING, is_old=MISSING, id=9)]\n</code></pre> <p>Note that the <code>doctable.MISSING</code> will never be inserted into the databse because it will be ignored.</p>"},{"location":"documentation/d-ex_insert_delete/#deletion-interface","title":"Deletion Interface","text":"<p>Deleting rows is pretty straightforward when using either the <code>ConnectQuery</code> or <code>TableQuery</code> interfaces. In fact, it is the exact same for both. The only parameters are <code>where</code> and <code>wherestr</code> (where you can add additional conditionals as strings).</p> <pre><code>print(core.query().select([rtab['id'].count()]).scalar_one())\nwith rtab.query() as q:\n    q.delete(where=rtab['name']=='test_A')\ncore.query().select([rtab['id'].count()]).scalar_one()\n</code></pre> <pre><code>2023-11-13 14:46:31,486 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:46:31,489 INFO sqlalchemy.engine.Engine SELECT count(\"Record\".id) AS count_1 \nFROM \"Record\"\n2023-11-13 14:46:31,490 INFO sqlalchemy.engine.Engine [cached since 27.39s ago] ()\n7\n2023-11-13 14:46:31,493 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:46:31,495 INFO sqlalchemy.engine.Engine DELETE FROM \"Record\" WHERE \"Record\".name = ?\n2023-11-13 14:46:31,497 INFO sqlalchemy.engine.Engine [cached since 27.39s ago] ('test_A',)\n2023-11-13 14:46:31,498 INFO sqlalchemy.engine.Engine COMMIT\n2023-11-13 14:46:31,500 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:46:31,500 INFO sqlalchemy.engine.Engine SELECT count(\"Record\".id) AS count_1 \nFROM \"Record\"\n2023-11-13 14:46:31,501 INFO sqlalchemy.engine.Engine [cached since 27.4s ago] ()\n\n\n\n\n\n7\n</code></pre> <p>To delete all columns, pass the <code>all=True</code> flag. This prevents the user from accidentally deleting all rows.</p> <pre><code>with rtab.query() as q:\n    q.delete(all=True)\ncore.query().select([rtab['id'].count()]).scalar_one()\n</code></pre> <pre><code>2023-11-13 14:48:08,718 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:48:08,720 INFO sqlalchemy.engine.Engine DELETE FROM \"Record\"\n2023-11-13 14:48:08,722 INFO sqlalchemy.engine.Engine [cached since 13.97s ago] ()\n2023-11-13 14:48:08,724 INFO sqlalchemy.engine.Engine COMMIT\n2023-11-13 14:48:08,726 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-13 14:48:08,727 INFO sqlalchemy.engine.Engine SELECT count(\"Record\".id) AS count_1 \nFROM \"Record\"\n2023-11-13 14:48:08,729 INFO sqlalchemy.engine.Engine [cached since 124.6s ago] ()\n\n\n\n\n\n0\n</code></pre>"},{"location":"documentation/e-iris_example/","title":"Iris Vignette","text":"<p>Here I will demonstrate the use of <code>doctable</code> to store and analyze components of the classic iris dataset. This dataset is a collection of measurements of three different species of iris flowers.</p> <pre><code>from __future__ import annotations\nimport pprint\nimport pandas as pd\nimport sqlalchemy\n\nimport sys\nsys.path.append('../')\nimport doctable\n</code></pre> <p>The iris dataset is simply a list of flowers with information about the sepal, petal, and species.</p> <pre><code>iris_df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\niris_df.head()\n</code></pre> sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa <p>Start by opening a connection to the database with a <code>ConnectCore</code> object. This object maintains the sqlalchemy metatdata, engine, and connections and is used to access all objects representing tables, queries, and more.</p> <pre><code>core = doctable.ConnectCore.open(\n    target=':memory:', # use a filename for a sqlite to write to disk\n    dialect='sqlite',\n    echo=True,\n)\ncore\n</code></pre> <pre><code>ConnectCore(target=':memory:', dialect='sqlite', engine=Engine(sqlite:///:memory:), metadata=MetaData())\n</code></pre> <p>Define a table using the <code>table_schema</code> decorator and listing attributes as you would a dataframe. To give more detail about a column, you can set the default value to <code>Column()</code>, which accepts <code>FieldArgs</code> to control the behavior of the dataframe container object, and <code>ColumnArgs()</code> to control behavior related to the database schema.</p> <pre><code>import datetime\n\n@doctable.table_schema(table_name='iris', slots=True)\nclass Iris:\n    sepal_length: float\n    sepal_width: float\n    petal_length: float\n    petal_width: float\n    species: str\n\n    id: int = doctable.Column(\n        column_args=doctable.ColumnArgs(order=0, primary_key=True, autoincrement=True),\n    )\n    updated: datetime.datetime = doctable.Column(\n        column_args=doctable.ColumnArgs(default=datetime.datetime.utcnow),\n    )\n    added: datetime.datetime = doctable.Column(\n        column_args=doctable.ColumnArgs(\n            default=datetime.datetime.utcnow, \n            onupdate=datetime.datetime.utcnow\n        )\n    )\n\n    @classmethod\n    def from_row(cls, row: pd.Series):\n        return cls(**row)\n\nIris(sepal_length=1, sepal_width=2, petal_length=3, petal_width=4, species='setosa')\n</code></pre> <pre><code>Iris(sepal_length=1, sepal_width=2, petal_length=3, petal_width=4, species='setosa', id=MISSING, updated=MISSING, added=MISSING)\n</code></pre> <p>We can start by creating new container object instances using the factory method constructor we created.</p> <pre><code>irises = [Iris.from_row(row) for _, row in iris_df.iterrows()]\nprint(irises[0])\n</code></pre> <pre><code>Iris(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2, species='setosa', id=MISSING, updated=MISSING, added=MISSING)\n</code></pre> <p>Use the <code>begin_ddl()</code> context manager to create database tables.</p> <pre><code>with core.begin_ddl() as emitter:\n    itab = emitter.create_table_if_not_exists(Iris)\nitab.inspect_columns()\n</code></pre> <pre><code>2023-11-07 18:23:30,147 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-07 18:23:30,154 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"species\")\n2023-11-07 18:23:30,156 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-11-07 18:23:30,156 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"iris_entry\")\n2023-11-07 18:23:30,157 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-11-07 18:23:30,158 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"iris\")\n2023-11-07 18:23:30,159 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-11-07 18:23:30,160 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"iris\")\n2023-11-07 18:23:30,160 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-11-07 18:23:30,162 INFO sqlalchemy.engine.Engine \nCREATE TABLE iris (\n    id INTEGER, \n    added DATETIME, \n    petal_length FLOAT, \n    petal_width FLOAT, \n    sepal_length FLOAT, \n    sepal_width FLOAT, \n    species VARCHAR, \n    updated DATETIME, \n    PRIMARY KEY (id)\n)\n\n\n2023-11-07 18:23:30,163 INFO sqlalchemy.engine.Engine [no key 0.00088s] ()\n2023-11-07 18:23:30,164 INFO sqlalchemy.engine.Engine COMMIT\n2023-11-07 18:23:30,165 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-07 18:23:30,171 INFO sqlalchemy.engine.Engine PRAGMA main.table_xinfo(\"iris\")\n2023-11-07 18:23:30,172 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-11-07 18:23:30,173 INFO sqlalchemy.engine.Engine ROLLBACK\n\n\n\n\n\n[{'name': 'id',\n  'type': INTEGER(),\n  'nullable': True,\n  'default': None,\n  'primary_key': 1},\n {'name': 'added',\n  'type': DATETIME(),\n  'nullable': True,\n  'default': None,\n  'primary_key': 0},\n {'name': 'petal_length',\n  'type': FLOAT(),\n  'nullable': True,\n  'default': None,\n  'primary_key': 0},\n {'name': 'petal_width',\n  'type': FLOAT(),\n  'nullable': True,\n  'default': None,\n  'primary_key': 0},\n {'name': 'sepal_length',\n  'type': FLOAT(),\n  'nullable': True,\n  'default': None,\n  'primary_key': 0},\n {'name': 'sepal_width',\n  'type': FLOAT(),\n  'nullable': True,\n  'default': None,\n  'primary_key': 0},\n {'name': 'species',\n  'type': VARCHAR(),\n  'nullable': True,\n  'default': None,\n  'primary_key': 0},\n {'name': 'updated',\n  'type': DATETIME(),\n  'nullable': True,\n  'default': None,\n  'primary_key': 0}]\n</code></pre>"},{"location":"documentation/e-iris_example/#running-queries","title":"Running Queries","text":""},{"location":"documentation/e-iris_example/#general-queries","title":"General Queries","text":"<p>Use the <code>query()</code> method of <code>ConnectCore</code> to perform queries using the doctable interface.</p> <pre><code>with core.query() as q:\n    q.delete(itab, all=True)\n    q.insert_single(itab, {\n        'sepal_length': 1,'sepal_width': 2,'petal_length': 3,'petal_width': 4,'species': 'setosa'\n    })\n    print(q.insert_multi.__doc__)\n    q.insert_multi(itab, [\n        {'sepal_length': 1, 'sepal_width': 2, 'petal_length': 3, 'petal_width': 4, 'species': 'setosa'},\n        {'sepal_length': 1, 'sepal_width': 2, 'petal_length': 3, 'petal_width': 4, 'species': 'setosa'},\n    ])\n    print(q.select(itab.all_cols()).fetchall())\n</code></pre> <pre><code>2023-11-07 15:57:55,319 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-07 15:57:55,320 INFO sqlalchemy.engine.Engine DELETE FROM iris\n2023-11-07 15:57:55,321 INFO sqlalchemy.engine.Engine [generated in 0.00228s] ()\n2023-11-07 15:57:55,325 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO iris (added, petal_length, petal_width, sepal_length, sepal_width, species, updated) VALUES (?, ?, ?, ?, ?, ?, ?)\n2023-11-07 15:57:55,326 INFO sqlalchemy.engine.Engine [generated in 0.00199s] ('2023-11-07 20:57:55.324953', 3.0, 4.0, 1.0, 2.0, 'setosa', '2023-11-07 20:57:55.324955')\nInsert multiple rows into the database using executemany-style \n            parameter binding.\n\n2023-11-07 15:57:55,328 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO iris (added, petal_length, petal_width, sepal_length, sepal_width, species, updated) VALUES (?, ?, ?, ?, ?, ?, ?)\n2023-11-07 15:57:55,329 INFO sqlalchemy.engine.Engine [generated in 0.00088s] [('2023-11-07 20:57:55.328713', 3.0, 4.0, 1.0, 2.0, 'setosa', '2023-11-07 20:57:55.328715'), ('2023-11-07 20:57:55.328719', 3.0, 4.0, 1.0, 2.0, 'setosa', '2023-11-07 20:57:55.328720')]\n2023-11-07 15:57:55,332 INFO sqlalchemy.engine.Engine SELECT iris.id, iris.added, iris.petal_length, iris.petal_width, iris.sepal_length, iris.sepal_width, iris.species, iris.updated \nFROM iris\n2023-11-07 15:57:55,333 INFO sqlalchemy.engine.Engine [generated in 0.00102s] ()\n[(1, datetime.datetime(2023, 11, 7, 20, 57, 55, 324953), 3.0, 4.0, 1.0, 2.0, 'setosa', datetime.datetime(2023, 11, 7, 20, 57, 55, 324955)), (2, datetime.datetime(2023, 11, 7, 20, 57, 55, 328713), 3.0, 4.0, 1.0, 2.0, 'setosa', datetime.datetime(2023, 11, 7, 20, 57, 55, 328715)), (3, datetime.datetime(2023, 11, 7, 20, 57, 55, 328719), 3.0, 4.0, 1.0, 2.0, 'setosa', datetime.datetime(2023, 11, 7, 20, 57, 55, 328720))]\n2023-11-07 15:57:55,334 INFO sqlalchemy.engine.Engine COMMIT\n</code></pre> <p>Use <code>cols()</code> or <code>__call__()</code> to return a list of column objects associated with the given table. Column objects also have bound operators such as <code>sum()</code>, <code>max()</code>, and <code>distinct()</code> (see comment below for more).</p> <pre><code>with core.query() as q:\n    # use table.cols to specify which columns to select\n    columns = itab.cols('sepal_length', 'sepal_width')\n    pprint.pprint(q.select(columns).fetchall())\n\n    # use subscript to specify table for each column. use for table joins\n    columns = [itab['sepal_length'], itab['sepal_width']]\n    results = q.select(columns).fetchall()\n    pprint.pprint(results)\n\n    # use .sum(), .min(), .max(), .count(), .sum(), and .unique() to specify aggregate functions\n    columns = [itab['species'].distinct()]\n    result = q.select(columns).scalars()\n    pprint.pprint(f'{result=}')\n\n    # use in conjunction with group_by to specify groupings\n    columns = [itab['sepal_length'].sum()]\n    result = q.select(columns, group_by=[itab['species']]).scalar_one()\n    pprint.pprint(f'{result=}')\n</code></pre> <pre><code>2023-11-07 15:57:55,371 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-07 15:57:55,372 INFO sqlalchemy.engine.Engine SELECT iris.sepal_length, iris.sepal_width \nFROM iris\n2023-11-07 15:57:55,375 INFO sqlalchemy.engine.Engine [generated in 0.00435s] ()\n[(1.0, 2.0), (1.0, 2.0), (1.0, 2.0)]\n2023-11-07 15:57:55,377 INFO sqlalchemy.engine.Engine SELECT iris.sepal_length, iris.sepal_width \nFROM iris\n2023-11-07 15:57:55,377 INFO sqlalchemy.engine.Engine [cached since 0.006684s ago] ()\n[(1.0, 2.0), (1.0, 2.0), (1.0, 2.0)]\n2023-11-07 15:57:55,380 INFO sqlalchemy.engine.Engine SELECT distinct(iris.species) AS distinct_1 \nFROM iris\n2023-11-07 15:57:55,381 INFO sqlalchemy.engine.Engine [generated in 0.00088s] ()\n'result=&lt;sqlalchemy.engine.result.ScalarResult object at 0x7f40935dc730&gt;'\n2023-11-07 15:57:55,383 INFO sqlalchemy.engine.Engine SELECT sum(iris.sepal_length) AS sum_1 \nFROM iris GROUP BY iris.species\n2023-11-07 15:57:55,385 INFO sqlalchemy.engine.Engine [generated in 0.00185s] ()\n'result=3.0'\n2023-11-07 15:57:55,386 INFO sqlalchemy.engine.Engine COMMIT\n</code></pre>"},{"location":"documentation/e-iris_example/#table-specific-queries","title":"Table-specific Queries","text":"<p>Use the <code>query()</code> method on a table to reference column names as strings and wrap results in container instances.</p> <pre><code>with itab.query() as q:\n    pprint.pprint(q.select())\n</code></pre> <pre><code>2023-11-07 15:57:55,420 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-07 15:57:55,421 INFO sqlalchemy.engine.Engine SELECT iris.id, iris.added, iris.petal_length, iris.petal_width, iris.sepal_length, iris.sepal_width, iris.species, iris.updated \nFROM iris\n2023-11-07 15:57:55,422 INFO sqlalchemy.engine.Engine [cached since 0.08979s ago] ()\n[Iris(sepal_length=1.0,\n      sepal_width=2.0,\n      petal_length=3.0,\n      petal_width=4.0,\n      species='setosa',\n      id=1,\n      updated=datetime.datetime(2023, 11, 7, 20, 57, 55, 324955),\n      added=datetime.datetime(2023, 11, 7, 20, 57, 55, 324953)),\n Iris(sepal_length=1.0,\n      sepal_width=2.0,\n      petal_length=3.0,\n      petal_width=4.0,\n      species='setosa',\n      id=2,\n      updated=datetime.datetime(2023, 11, 7, 20, 57, 55, 328715),\n      added=datetime.datetime(2023, 11, 7, 20, 57, 55, 328713)),\n Iris(sepal_length=1.0,\n      sepal_width=2.0,\n      petal_length=3.0,\n      petal_width=4.0,\n      species='setosa',\n      id=3,\n      updated=datetime.datetime(2023, 11, 7, 20, 57, 55, 328720),\n      added=datetime.datetime(2023, 11, 7, 20, 57, 55, 328719))]\n2023-11-07 15:57:55,423 INFO sqlalchemy.engine.Engine COMMIT\n</code></pre> <p>All of the same query types can be used.</p> <pre><code>with itab.query() as q:\n    q.delete(all=True)\n\n    q.insert_multi(irises)\n\n    db_irises = q.select()\n    print(len(db_irises))\n    pprint.pprint(db_irises[:2])\n</code></pre> <pre><code>2023-11-07 15:57:55,471 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-07 15:57:55,473 INFO sqlalchemy.engine.Engine DELETE FROM iris\n2023-11-07 15:57:55,474 INFO sqlalchemy.engine.Engine [cached since 0.1547s ago] ()\n2023-11-07 15:57:55,486 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO iris (added, petal_length, petal_width, sepal_length, sepal_width, species, updated) VALUES (?, ?, ?, ?, ?, ?, ?)\n2023-11-07 15:57:55,487 INFO sqlalchemy.engine.Engine [cached since 0.159s ago] [('2023-11-07 20:57:55.484095', 1.4, 0.2, 5.1, 3.5, 'setosa', '2023-11-07 20:57:55.484099'), ('2023-11-07 20:57:55.484100', 1.4, 0.2, 4.9, 3.0, 'setosa', '2023-11-07 20:57:55.484101'), ('2023-11-07 20:57:55.484101', 1.3, 0.2, 4.7, 3.2, 'setosa', '2023-11-07 20:57:55.484102'), ('2023-11-07 20:57:55.484103', 1.5, 0.2, 4.6, 3.1, 'setosa', '2023-11-07 20:57:55.484103'), ('2023-11-07 20:57:55.484104', 1.4, 0.2, 5.0, 3.6, 'setosa', '2023-11-07 20:57:55.484104'), ('2023-11-07 20:57:55.484105', 1.7, 0.4, 5.4, 3.9, 'setosa', '2023-11-07 20:57:55.484106'), ('2023-11-07 20:57:55.484106', 1.4, 0.3, 4.6, 3.4, 'setosa', '2023-11-07 20:57:55.484107'), ('2023-11-07 20:57:55.484108', 1.5, 0.2, 5.0, 3.4, 'setosa', '2023-11-07 20:57:55.484108')  ... displaying 10 of 150 total bound parameter sets ...  ('2023-11-07 20:57:55.484270', 5.4, 2.3, 6.2, 3.4, 'virginica', '2023-11-07 20:57:55.484270'), ('2023-11-07 20:57:55.484271', 5.1, 1.8, 5.9, 3.0, 'virginica', '2023-11-07 20:57:55.484271')]\n2023-11-07 15:57:55,489 INFO sqlalchemy.engine.Engine SELECT iris.id, iris.added, iris.petal_length, iris.petal_width, iris.sepal_length, iris.sepal_width, iris.species, iris.updated \nFROM iris\n2023-11-07 15:57:55,490 INFO sqlalchemy.engine.Engine [cached since 0.1582s ago] ()\n150\n[Iris(sepal_length=5.1,\n      sepal_width=3.5,\n      petal_length=1.4,\n      petal_width=0.2,\n      species='setosa',\n      id=1,\n      updated=datetime.datetime(2023, 11, 7, 20, 57, 55, 484099),\n      added=datetime.datetime(2023, 11, 7, 20, 57, 55, 484095)),\n Iris(sepal_length=4.9,\n      sepal_width=3.0,\n      petal_length=1.4,\n      petal_width=0.2,\n      species='setosa',\n      id=2,\n      updated=datetime.datetime(2023, 11, 7, 20, 57, 55, 484101),\n      added=datetime.datetime(2023, 11, 7, 20, 57, 55, 484100))]\n2023-11-07 15:57:55,494 INFO sqlalchemy.engine.Engine COMMIT\n</code></pre> <p>Attributes that were not requested from the database reference the <code>doctable.MISSING</code> sentinel value.</p> <pre><code>with itab.query() as q:\n    db_irises = q.select(['id', 'sepal_width', 'sepal_length'])\n    pprint.pprint(db_irises[:2])\n</code></pre> <pre><code>2023-11-07 15:57:55,524 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-07 15:57:55,526 INFO sqlalchemy.engine.Engine SELECT iris.id, iris.sepal_width, iris.sepal_length \nFROM iris\n2023-11-07 15:57:55,527 INFO sqlalchemy.engine.Engine [generated in 0.00249s] ()\n[Iris(sepal_length=5.1,\n      sepal_width=3.5,\n      petal_length=MISSING,\n      petal_width=MISSING,\n      species=MISSING,\n      id=1,\n      updated=MISSING,\n      added=MISSING),\n Iris(sepal_length=4.9,\n      sepal_width=3.0,\n      petal_length=MISSING,\n      petal_width=MISSING,\n      species=MISSING,\n      id=2,\n      updated=MISSING,\n      added=MISSING)]\n2023-11-07 15:57:55,530 INFO sqlalchemy.engine.Engine COMMIT\n</code></pre>"},{"location":"documentation/e-iris_example/#working-with-multple-tables","title":"Working with Multple Tables","text":"<p>Now I'll demonstrate how to create and work with multi-table schemas with foreign key relationships.</p> <pre><code>print(iris_df['species'].unique())\n\nspecies_data = {\n    'setosa':'bristle-pointed iris',\n    'versicolor':'Southern blue flag',\n    'virginica':'Northern blue flag',\n}\n</code></pre> <pre><code>['setosa' 'versicolor' 'virginica']\n</code></pre> <p>Here we create a foreign key constraint on the iris entries table that references a new species table.</p> <pre><code>import typing\n\n@doctable.table_schema(table_name='species')\nclass Species:\n    name: str = doctable.Column(doctable.ColumnArgs(unique=True))\n    common_name: str = doctable.Column(\n        column_args=doctable.ColumnArgs(nullable=True),\n    )\n    id: int = doctable.Column(# will appear as the first column in the table\n        column_args=doctable.ColumnArgs(order=0, primary_key=True, autoincrement=True),\n    )\n\n    @classmethod\n    def from_dict(cls, data: typing.Dict[str, str]) -&gt; typing.List[Species]:\n        return [cls(name=n, common_name=cn) for n,cn in data.items()]\n\n@doctable.table_schema(\n    table_name='iris_entry',\n    constraints=[\n        doctable.ForeignKey(['species'], ['species.name'], ondelete='CASCADE', onupdate='CASCADE'),\n    ],\n)\nclass IrisEntry:\n    sepal_length: float\n    sepal_width: float\n    petal_length: float\n    petal_width: float\n    species: str = doctable.Column(\n        # NOTE: here I could add foreign_key='species.name' instead of adding fk constraint\n        column_args=doctable.ColumnArgs(nullable=False),\n    )\n\n    id: int = doctable.Column(# will appear as the first column in the table\n        column_args=doctable.ColumnArgs(order=0, primary_key=True, autoincrement=True),\n    )\n\n    @classmethod\n    def from_dataframe(cls, df: pd.DataFrame) -&gt; typing.List[IrisEntry]:\n        return [cls(**row) for _,row in df.iterrows()]\n\n\ncore = doctable.ConnectCore.open(\n    target=':memory:', # use a filename for a sqlite to write to disk\n    dialect='sqlite',\n    echo=True,\n)\n\nwith core.begin_ddl() as emitter:\n    core.enable_foreign_keys()\n    spec_tab = emitter.create_table_if_not_exists(Species)\n    iris_tab = emitter.create_table_if_not_exists(IrisEntry)\nprint(spec_tab.inspect_columns())\n</code></pre> <pre><code>2023-11-07 15:57:55,647 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-07 15:57:55,648 INFO sqlalchemy.engine.Engine pragma foreign_keys=ON\n2023-11-07 15:57:55,649 INFO sqlalchemy.engine.Engine [generated in 0.00072s] ()\n2023-11-07 15:57:55,650 INFO sqlalchemy.engine.Engine COMMIT\n2023-11-07 15:57:55,653 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-07 15:57:55,653 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"species\")\n2023-11-07 15:57:55,654 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-11-07 15:57:55,656 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"species\")\n2023-11-07 15:57:55,656 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-11-07 15:57:55,657 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"iris_entry\")\n2023-11-07 15:57:55,658 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-11-07 15:57:55,659 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"iris_entry\")\n2023-11-07 15:57:55,659 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-11-07 15:57:55,660 INFO sqlalchemy.engine.Engine \nCREATE TABLE species (\n    id INTEGER, \n    common_name VARCHAR, \n    name VARCHAR, \n    PRIMARY KEY (id), \n    UNIQUE (name)\n)\n\n\n2023-11-07 15:57:55,661 INFO sqlalchemy.engine.Engine [no key 0.00057s] ()\n2023-11-07 15:57:55,664 INFO sqlalchemy.engine.Engine \nCREATE TABLE iris_entry (\n    id INTEGER, \n    petal_length FLOAT, \n    petal_width FLOAT, \n    sepal_length FLOAT, \n    sepal_width FLOAT, \n    species VARCHAR NOT NULL, \n    PRIMARY KEY (id), \n    FOREIGN KEY(species) REFERENCES species (name) ON DELETE CASCADE ON UPDATE CASCADE\n)\n\n\n2023-11-07 15:57:55,664 INFO sqlalchemy.engine.Engine [no key 0.00054s] ()\n2023-11-07 15:57:55,665 INFO sqlalchemy.engine.Engine COMMIT\n2023-11-07 15:57:55,666 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-07 15:57:55,667 INFO sqlalchemy.engine.Engine PRAGMA main.table_xinfo(\"species\")\n2023-11-07 15:57:55,668 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-11-07 15:57:55,668 INFO sqlalchemy.engine.Engine ROLLBACK\n[{'name': 'id', 'type': INTEGER(), 'nullable': True, 'default': None, 'primary_key': 1}, {'name': 'common_name', 'type': VARCHAR(), 'nullable': True, 'default': None, 'primary_key': 0}, {'name': 'name', 'type': VARCHAR(), 'nullable': True, 'default': None, 'primary_key': 0}]\n</code></pre> <p>Start by populating the species table.</p> <pre><code>with spec_tab.query() as q:\n    q.insert_multi(Species.from_dict(species_data), ifnotunique='replace')\n    pprint.pprint(q.select())\n</code></pre> <pre><code>2023-11-07 15:57:55,766 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-07 15:57:55,767 INFO sqlalchemy.engine.Engine INSERT OR REPLACE INTO species (common_name, name) VALUES (?, ?)\n2023-11-07 15:57:55,768 INFO sqlalchemy.engine.Engine [generated in 0.00240s] [('bristle-pointed iris', 'setosa'), ('Southern blue flag', 'versicolor'), ('Northern blue flag', 'virginica')]\n2023-11-07 15:57:55,770 INFO sqlalchemy.engine.Engine SELECT species.id, species.common_name, species.name \nFROM species\n2023-11-07 15:57:55,771 INFO sqlalchemy.engine.Engine [generated in 0.00073s] ()\n[Species(name='setosa', common_name='bristle-pointed iris', id=1),\n Species(name='versicolor', common_name='Southern blue flag', id=2),\n Species(name='virginica', common_name='Northern blue flag', id=3)]\n2023-11-07 15:57:55,772 INFO sqlalchemy.engine.Engine COMMIT\n</code></pre> <p>We will get an error if the provided species does not correspond to a row in the species table.</p> <pre><code>try:\n    with iris_tab.query() as q:\n        q.insert_single(IrisEntry(\n            sepal_length=1, \n            sepal_width=2, \n            petal_length=3, \n            petal_width=4, \n            species='wrongname' # THIS PART CAUSED THE ERROR!\n        ))\nexcept sqlalchemy.exc.IntegrityError:\n    print('The species_name column is a foreign key to the species table, so it must be a valid species name.')\n</code></pre> <pre><code>2023-11-07 15:57:55,821 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-07 15:57:55,823 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO iris_entry (petal_length, petal_width, sepal_length, sepal_width, species) VALUES (?, ?, ?, ?, ?)\n2023-11-07 15:57:55,823 INFO sqlalchemy.engine.Engine [generated in 0.00239s] (3.0, 4.0, 1.0, 2.0, 'wrongname')\n2023-11-07 15:57:55,824 INFO sqlalchemy.engine.Engine COMMIT\nThe species_name column is a foreign key to the species table, so it must be a valid species name.\n</code></pre> <p>Now that the species table is populated, we can insert the iris data.</p> <pre><code>with iris_tab.query() as q:\n    q.insert_multi(IrisEntry.from_dataframe(iris_df))\n    print(q.select(limit=2))\n</code></pre> <pre><code>2023-11-07 15:57:55,939 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-07 15:57:55,940 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO iris_entry (petal_length, petal_width, sepal_length, sepal_width, species) VALUES (?, ?, ?, ?, ?)\n2023-11-07 15:57:55,941 INFO sqlalchemy.engine.Engine [generated in 0.00323s] [(1.4, 0.2, 5.1, 3.5, 'setosa'), (1.4, 0.2, 4.9, 3.0, 'setosa'), (1.3, 0.2, 4.7, 3.2, 'setosa'), (1.5, 0.2, 4.6, 3.1, 'setosa'), (1.4, 0.2, 5.0, 3.6, 'setosa'), (1.7, 0.4, 5.4, 3.9, 'setosa'), (1.4, 0.3, 4.6, 3.4, 'setosa'), (1.5, 0.2, 5.0, 3.4, 'setosa')  ... displaying 10 of 150 total bound parameter sets ...  (5.4, 2.3, 6.2, 3.4, 'virginica'), (5.1, 1.8, 5.9, 3.0, 'virginica')]\n2023-11-07 15:57:55,944 INFO sqlalchemy.engine.Engine SELECT iris_entry.id, iris_entry.petal_length, iris_entry.petal_width, iris_entry.sepal_length, iris_entry.sepal_width, iris_entry.species \nFROM iris_entry\n LIMIT ? OFFSET ?\n2023-11-07 15:57:55,944 INFO sqlalchemy.engine.Engine [generated in 0.00088s] (2, 0)\n[IrisEntry(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2, species='setosa', id=1), IrisEntry(sepal_length=4.9, sepal_width=3.0, petal_length=1.4, petal_width=0.2, species='setosa', id=2)]\n2023-11-07 15:57:55,945 INFO sqlalchemy.engine.Engine COMMIT\n</code></pre> <p>When the entry is deleted from the species tab, all associated irises are deleted.</p> <pre><code>with spec_tab.query() as q:\n    q.delete(spec_tab['name']=='setosa')\n    print(f'{len(q.select())=}')\n\nwith core.query() as q:\n    print(q.select([iris_tab['species'].distinct()]).fetchall())\n</code></pre> <pre><code>2023-11-07 15:59:34,029 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-07 15:59:34,031 INFO sqlalchemy.engine.Engine DELETE FROM species WHERE species.name = ?\n2023-11-07 15:59:34,031 INFO sqlalchemy.engine.Engine [cached since 98.05s ago] ('setosa',)\n2023-11-07 15:59:34,032 INFO sqlalchemy.engine.Engine SELECT species.id, species.common_name, species.name \nFROM species\n2023-11-07 15:59:34,033 INFO sqlalchemy.engine.Engine [cached since 98.26s ago] ()\nlen(q.select())=2\n2023-11-07 15:59:34,034 INFO sqlalchemy.engine.Engine COMMIT\n2023-11-07 15:59:34,035 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-11-07 15:59:34,036 INFO sqlalchemy.engine.Engine SELECT distinct(iris_entry.species) AS distinct_1 \nFROM iris_entry\n2023-11-07 15:59:34,037 INFO sqlalchemy.engine.Engine [cached since 98.05s ago] ()\n[('versicolor',), ('virginica',)]\n2023-11-07 15:59:34,038 INFO sqlalchemy.engine.Engine COMMIT\n</code></pre>"},{"location":"legacy_documentation/dataclass_example/","title":"Dataclass Schema Example","text":"<p>In this vignette I'll show how to use a Python dataclass (introduced in Python 3.7) to specify a schema for a DocTable. The advantage of this schema format is that you can use custom classes to represent each row, and easily convert your existing python objects into a format that it easy to store in a sqlite database.</p> <pre><code>from datetime import datetime\nfrom pprint import pprint\nimport pandas as pd\nimport sys\nsys.path.append('..')\nimport doctable\n</code></pre>"},{"location":"legacy_documentation/dataclass_example/#basic-dataclass-usage","title":"Basic dataclass usage","text":"<p>For our first example, we show how a basic dataclass object can be used as a DocTable schema. First we create a python dataclass using the <code>@dataclass</code> decorator. This object has three members, each defaulted to <code>None</code>. We can create this object using the constructor provided by <code>dataclass</code>.</p> <pre><code>from dataclasses import dataclass\n@doctable.schema\nclass User:\n    __slots__ = []\n    name: str = None\n    age: int = None\n    height: float = None\nUser()\n</code></pre> <pre><code>User(name=None, age=None, height=None)\n</code></pre> <p>And it is relatively easy to create a new doctable using the schema provided by our dataclass <code>User</code> by providing the class definition to the <code>schema</code> argument. We can see that <code>DocTable</code> uses the dataclass schema to create a new table that follows the specified Python datatypes.</p> <pre><code>db = doctable.DocTable(schema=User, target=':memory:')\ndb.schema_table()\n</code></pre> name type nullable default autoincrement primary_key 0 name VARCHAR True None auto 0 1 age INTEGER True None auto 0 2 height FLOAT True None auto 0 <p>Now we insert several new objects into the table and view them using <code>DocTable.head()</code>. Note that the datbase actually inserted the object's defaulted values into the table.</p> <pre><code>db.insert([User('kevin'), User('tyrone', age=12), User('carlos', age=25, height=6.5)])\ndb.head()\n</code></pre> name age height 0 kevin NaN NaN 1 tyrone 12.0 NaN 2 carlos 25.0 6.5 <p>Using a normal <code>select()</code>, we can extract the results as the original objects. With no parameters, the select statement extracts all columns as they are stored and they exactly match the original data we entered. As expected from the python object, we can access these as properties of the object. Due to the base class <code>doctable.DocTableRow</code>, we can also access properties using the <code>__getitem__</code> indexing. I'll show why there is a difference btween the two later.</p> <pre><code>users = db.select()\nfor user in users:\n    print(f\"{user.name}:\\n\\tage: {user.age}\\n\\theight: {user['height']}\")\n</code></pre> <pre><code>kevin:\n    age: None\n    height: None\ntyrone:\n    age: 12\n    height: None\ncarlos:\n    age: 25\n    height: 6.5\n</code></pre>"},{"location":"legacy_documentation/dataclass_example/#example-using-doctablecol","title":"Example using <code>doctable.Col</code>","text":"<p>In this example, we will show how to create a dataclass with functionality that supports more complicated database operations. A key to this approach is to use the <code>doctable.Col</code> function as default values for our parameters. Note that when we initialize the object, the default values of all columns except for <code>name</code> are set to <code>EmptyValue</code>. This is important, because <code>EmptyValue</code> will indicate values that are not meant to be inserted into the database or are not retrieved from the database after selecting.</p> <pre><code>@doctable.schema\nclass User:\n    __slots__ = []\n    name: str = doctable.Col()\n    age: int = doctable.Col()\n    height: float = doctable.Col()\nUser()\n</code></pre> <pre><code>User()\n</code></pre> <p>Given that the type specifications are the same as the previous example, we get exactly the same database schema. We insert entries just as before. The <code>User</code> data contained <code>EmptyValue</code>s, and so that column data was not presented to the database at all - instead, the schema's column defaults were used. Consistent with our schema (not the object defaults, the default values were set to None.</p> <pre><code>db = doctable.DocTable(schema=User, target=':memory:')\nprint(db.schema_table())\ndb.insert([User('kevin'), User('tyrone', age=12), User('carlos', age=25, height=6.5)])\nfor user in db.select():\n    print(f\"{user}\")\n</code></pre> <pre><code>     name     type  nullable default autoincrement  primary_key\n0    name  VARCHAR      True    None          auto            0\n1     age  INTEGER      True    None          auto            0\n2  height    FLOAT      True    None          auto            0\nUser(name='kevin', age=None, height=None)\nUser(name='tyrone', age=None, height=None)\nUser(name='carlos', age=None, height=None)\n</code></pre> <p>Now let's try to select only a subset of the columns - in this case, 'name' and 'age'.</p> <pre><code>users = db.select(['name', 'age'])\nusers[0]\n</code></pre> <pre><code>User(name='kevin', age=None)\n</code></pre> <p>Note that the user height was set to <code>EmptyValue</code>. When we try to access height as an index, we get an error indicating that the data was not retrived in the select statement.</p> <pre><code>try:\n    users[0]['height']\nexcept KeyError as e:\n    print(e)\n</code></pre> <pre><code>'The column \"height\" was not retreived in the select statement.'\n</code></pre> <p>On the contrary, if we try to access as an attribute, the actual <code>EmptyValue</code> object is retrieved. Object properties work as they always have, but indexing into columns will check for errors in the program logic. This implementation shows how dataclass schemas walk the line between regular python objects and database rows, and thus accessing these values can be done differently depending on how much the table entries should be treated like regular objects vs database rows. This is all determined based on how the dataclass columns are configured.</p> <pre><code>users[0].height\n</code></pre> <pre><code>EmptyValue()\n</code></pre>"},{"location":"legacy_documentation/dataclass_example/#special-column-types","title":"Special column types","text":"<p>Now I'll introduce two special data column types provided by doctable: <code>IDCol()</code>, which represents a regular id column in sqlite with autoindex and primary_key parameters set, and <code>UpdatedCol()</code>, which records the datetime that an object was added to the database. When we create a new user using the dataclass constructor, these values are set to <code>EmptyValue</code>, and are relevant primarily to the database. By setting the <code>repr</code> parameter in the <code>@dataclass</code> decorator, we can use the <code>__repr__</code> of the <code>DocTableRow</code> base class, which hides <code>EmptyValue</code> columns. This is optional.</p> <pre><code>from dataclasses import field, fields\n@doctable.schema(repr=False)\nclass User:\n    __slots__ = []\n    id: int = doctable.IDCol() # shortcut for autoindex, primary_key column.\n    updated: datetime = doctable.UpdatedCol() # shortcut for automatically \n\n    name: str = doctable.Col(nullable=False)\n    age: int = doctable.Col(None) # accessing sqlalchemy column keywords arguments\n\nuser = User(name='carlos', age=15)\nuser\n</code></pre> <pre><code>User(name='carlos', age=15)\n</code></pre> <p>And we can see the relevance of those columns by inserting them into the database and selecting them again. You can see from the result of <code>.head()</code> that the primary key <code>id</code> and the <code>updated</code> columns were appropriately filled upon insertion. After selecting, these objects also contain valid values.</p> <pre><code>db = doctable.DocTable(schema=User, target=':memory:')\nprint(db.schema_table())\ndb.insert([User(name='kevin'), User(name='tyrone', age=12), User(name='carlos', age=25)])\ndb.head()\n</code></pre> <pre><code>      name      type  nullable default autoincrement  primary_key\n0       id   INTEGER     False    None          auto            1\n1  updated  DATETIME      True    None          auto            0\n2     name   VARCHAR     False    None          auto            0\n3      age   INTEGER      True    None          auto            0\n</code></pre> id updated name age 0 1 2021-07-21 19:03:04.550804 kevin NaN 1 2 2021-07-21 19:03:04.550809 tyrone 12.0 2 3 2021-07-21 19:03:04.550811 carlos 25.0 <p>This was just an example of how regular Python dataclass objects can contain additional data which is relevant to the database, but which is otherwise unneeded. After retrieving from database, we can also use <code>.update()</code> to modify the entry.</p> <pre><code>user = db.select_first()\nuser.age = 10\ndb.update(user, where=db['id']==user['id'])\ndb.head()\n</code></pre> id updated name age 0 1 2021-07-21 19:03:04.550804 kevin 10 1 2 2021-07-21 19:03:04.550809 tyrone 12 2 3 2021-07-21 19:03:04.550811 carlos 25 <p>We can use the convenience function <code>update_dataclass()</code> to update a single row corresponding to the object.</p> <pre><code>user.age = 11\ndb.update_dataclass(user)\ndb.head()\n</code></pre> id updated name age 0 1 2021-07-21 19:03:04.550804 kevin 11 1 2 2021-07-21 19:03:04.550809 tyrone 12 2 3 2021-07-21 19:03:04.550811 carlos 25 <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"legacy_documentation/dataclass_vignette_advanced/","title":"Advanced dataclass schema vignette","text":"<p>In this vignette I'll show how to create more complicated database schemas from dataclasses.</p>"},{"location":"legacy_documentation/depric_vignette_newsgroups/","title":"NewsGroups Dataset Vignette","text":"<p>In this vignette, I will show you how to create a database for storing and manipulating </p>"},{"location":"legacy_documentation/depric_vignette_newsgroups/#introduction-to-dataset","title":"Introduction to dataset","text":"<p>We will be using the 20 Newsgroups dataset for this vignette. This is the sklearn website description:</p> <p>The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.</p> <p>We use sklearn's fetch_20newsgroups method to download and access articles from the politics newsgroup.</p> <pre><code>import sklearn.datasets\nnewsgroups = sklearn.datasets.fetch_20newsgroups(categories=['talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc'])\nnewsgroups.keys(), len(newsgroups['data'])\n</code></pre> <pre><code>(dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR']), 1575)\n</code></pre> <p>This is an example of a newsgroup post.</p> <pre><code>print(newsgroups['data'][0])\n</code></pre> <pre><code>From: golchowy@alchemy.chem.utoronto.ca (Gerald Olchowy)\nSubject: Re: Help fight the Clinton Administration's invasion of your privacy\nOrganization: University of Toronto Chemistry Department\nLines: 16\n\nIn article &lt;9308@blue.cis.pitt.edu&gt; cjp+@pitt.edu (Casimir J Palowitch) writes:\n&gt;The Clinton Administration wants to \"manage\" your use of digital\n&gt;encryption. This includes a proposal which would limit your use of\n&gt;encryption to a standard developed by the NSA, the technical details of \n&gt;which would remain classified with the government.\n&gt;\n&gt;This cannot be allowed to happen.\n&gt;\n\nIt is a bit unfair to call blame the Clinton Administration alone...this\ninitiative was underway under the Bush Administration...it is basically\na bipartisan effort of the establishment Demopublicans and\nRepublicrats...the same bipartisan effort that brought the S&amp;L scandal,\nand BCCI, etc.\n\nGerald\n</code></pre> <p>It looks very similar to an email, so we will use Python's <code>email</code> package to parse the text and return a dictionary containing the various relevant fields. Our <code>parse_email</code> function shows how we can extract metadata fields like author, subject, and organization from the message, as well as the main text body.</p> <pre><code>import email\n\ndef parse_newsgroup(email_text):\n    message = email.message_from_string(email_text)\n    return {\n        'author': message['from'],\n        'subject': message['Subject'],\n        'organization': message['Organization'],\n        'lines': int(message['Lines']),\n        'text': message.get_payload(),\n    }\n\nparse_newsgroup(newsgroups['data'][0])\n</code></pre> <pre><code>{'author': 'golchowy@alchemy.chem.utoronto.ca (Gerald Olchowy)',\n 'subject': \"Re: Help fight the Clinton Administration's invasion of your privacy\",\n 'organization': 'University of Toronto Chemistry Department',\n 'lines': 16,\n 'text': 'In article &lt;9308@blue.cis.pitt.edu&gt; cjp+@pitt.edu (Casimir J Palowitch) writes:\\n&gt;The Clinton Administration wants to \"manage\" your use of digital\\n&gt;encryption. This includes a proposal which would limit your use of\\n&gt;encryption to a standard developed by the NSA, the technical details of \\n&gt;which would remain classified with the government.\\n&gt;\\n&gt;This cannot be allowed to happen.\\n&gt;\\n\\nIt is a bit unfair to call blame the Clinton Administration alone...this\\ninitiative was underway under the Bush Administration...it is basically\\na bipartisan effort of the establishment Demopublicans and\\nRepublicrats...the same bipartisan effort that brought the S&amp;L scandal,\\nand BCCI, etc.\\n\\nGerald\\n'}\n</code></pre>"},{"location":"legacy_documentation/depric_vignette_newsgroups/#creating-a-database-schema","title":"Creating a database schema","text":"<p>The first step will be to create a database schema that is appropriate for the newsgroup dataset by defining a container dataclass using the <code>@schema</code> decorator.  The <code>schema</code> decorator will convert the class into a <code>dataclass</code> with slots enabled (provided <code>__slots__ = []</code> is given in the definition), and inherit from <code>DocTableRow</code> to add some additional functionality. The type hints associated with each variable will be used in the schema definition for the new tables, and arguments to <code>Col()</code>, <code>IDCol()</code>, <code>AddedCol()</code>, and <code>UpdatedCol()</code> will mostly be passed to <code>dataclasses.field</code> (see docs for more detail), so all dataclass functionality is maintained. The doctable schema guide explains more about schema and schema object definitions. </p> <p>Here I define a <code>NewsgroupDoc</code> class to represent a single document and define <code>__slots__</code> so the decorator can automatically create a slot class. Each member variable will act as a column in our database schema, and the first variable we define is an <code>id</code> column with the defaulted value <code>IDCol()</code>. This is a special function that will translate to a schema that uses the <code>id</code> colum as the primary key and enable auto-incrementing. Because <code>id</code> is defaulted, we must default our other variables as well.</p> <p>I also define a couple of methods as part of our schema class - they are ignored in the schema creation process, but allow us to manipulate the object within Python. The <code>author_email</code> property will extract just the email address from the author field. Note that even though it is a property, it is defined as a method and therefore will not be considered when creating the class schema. I also define a <code>classmethod</code> that can be used to create a new <code>NewsgroupDoc</code> from the newsgroup text - this replaces the functionality of the <code>parse_email</code> function we created above. This way, the class knows how to create itself from the raw newsgroup text.</p> <pre><code>import sys\nsys.path.append('..')\nimport doctable\n\nimport re\nimport email\nimport dataclasses\n\ndef try_int(text):\n    try:\n        return int(text.split()[0])\n    except:\n        return None\n\n\n@doctable.schema\nclass NewsgroupDoc:\n    __slots__ = []\n\n    # schema columns\n    id: int = doctable.IDCol()\n    author: str = None\n    subject: str = None\n    organization: str = None\n    length: int = None\n    text: str = None\n\n    @property\n    def author_email(self, pattern=re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')):\n        '''Get the author\\'s email address from the author field text.\n        '''\n        return re.search(pattern, self.author)[0]\n\n\n    @classmethod\n    def from_string(cls, newsgroup_text):\n        '''Code to create a NewsGroupDoc object from the original newsgroup string.\n        '''\n        message = email.message_from_string(newsgroup_text)\n        return cls(\n            author = message['from'],\n            subject = message['Subject'],\n            organization = message['Organization'],\n            length = len(message.get_payload()),\n            text = message.get_payload(),\n        )\n\n\n# for example, we create a new NewsGroupDoc from the first newsgroup article\nngdoc = NewsgroupDoc.from_string(newsgroups['data'][0])\nprint(ngdoc.author)\nngdoc.author_email\n</code></pre> <pre><code>golchowy@alchemy.chem.utoronto.ca (Gerald Olchowy)\n\n\n\n\n\n'golchowy@alchemy.chem.utoronto.ca'\n</code></pre> <p>To make sure the <code>NewsgroupDoc</code> will translate to the database schema we expect, we can create a new <code>DocTable</code> object that uses it as a schema. We use the <code>schema</code> argument of the <code>DocTable</code> constructor to specify the schema, and print it below. See that most fields were translated to <code>VARCHAR</code> type fields, but <code>id</code> and <code>length</code> were translated to <code>INTEGER</code> types based on their type hints.</p> <pre><code>ng_table = doctable.DocTable(target=':memory:', tabname='documents', schema=NewsgroupDoc)\nng_table.schema_table()\n</code></pre> name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 author VARCHAR True None auto 0 2 subject VARCHAR True None auto 0 3 organization VARCHAR True None auto 0 4 length INTEGER True None auto 0 5 text VARCHAR True None auto 0 <p>To better describe the data we are interested in, we now create a class that inherits from <code>DocTable</code>. This class will act as the main interface for working with our dataset. We use the <code>_tabname_</code> and <code>_schema_</code> properties to define the table name and schema so we don't need to include them in the constructor. We also define a method <code>count_author_emails</code> - we will describe the behavior of this method later.</p> <pre><code>import collections\n\nclass NewsgroupTable(doctable.DocTable):\n    _tabname_ = 'documents'\n    _schema_ = NewsgroupDoc\n\n    def count_author_emails(self, *args, **kwargs):\n        author_emails = self.select('author', *args, **kwargs)\n        return collections.Counter(author_emails)\n\n</code></pre> <p>Instead of using <code>target=':memory:'</code>, we want to create a database on our filesystem so we can store data. By default, <code>DocTable</code> uses sqlite as the database engine, so with <code>target</code> we need only specify a filename. Because this is just a demonstration, we will create the database in a temporary folder using the <code>tempfile</code> package. This database does not exist yet, so we use the <code>new_db</code> flag to indicate that a new one should be created.</p> <pre><code>import tempfile\n\ntempfolder = tempfile.TemporaryDirectory()\ntable_fname = f'{tempfolder.name}/tmp1.db'\nng_table = NewsgroupTable(target=table_fname, new_db=True)\nng_table.schema_table()\n</code></pre> name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 author VARCHAR True None auto 0 2 subject VARCHAR True None auto 0 3 organization VARCHAR True None auto 0 4 length INTEGER True None auto 0 5 text VARCHAR True None auto 0"},{"location":"legacy_documentation/depric_vignette_newsgroups/#parsing-and-storing-documents","title":"Parsing and storing documents","text":"<p>Now we would like to parse our documents for storage in the database. It is relatively straighforward to create a list of parsed texts using the <code>from_string</code> method. After doing this, we could potentially just insert them directly into the database.</p> <pre><code>%timeit [NewsgroupDoc.from_string(text) for text in newsgroups['data']]\n</code></pre> <pre><code>191 ms \u00b1 527 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre> <p>This is a relatively straigtforward task with a dataset of this size, but if we had a larger dataset or used more costly parsing algorithms, we would want to distribute parsing across multiple processes - we will take that approach for demonstration. First we define the <code>process_and_store</code> class to be used in each worker process.</p> <pre><code>def thread_func(numbers, db):\n    print(f'starting process')\n    db.reopen_engine() # create all new connections\n    db.insert([{'subject': i} for  i in numbers])\n    #for num in numbers:\n    #    db.insert({'process': process_id, 'number': num})\n    #    time.sleep(0.01)\n\nnumbers = list(range(100)) # these numbers are to be inserted into the database\n\nng_table.delete()\nwith doctable.Distribute(5) as d:\n    d.map_chunk(thread_func, numbers, ng_table)\nng_table.head(10)\n</code></pre> <pre><code>starting process\nstarting process\nstarting process\nstarting process\nstarting process\n</code></pre> id author subject organization length text 0 1 None 0 None None None 1 2 None 1 None None None 2 3 None 2 None None None 3 4 None 3 None None None 4 5 None 4 None None None 5 6 None 5 None None None 6 7 None 6 None None None 7 8 None 7 None None None 8 9 None 8 None None None 9 10 None 9 None None None <pre><code>def printer(x, table):\n    print(x, table)\n\nwith doctable.WorkerPool(3, verbose=False) as p:\n    assert(p.any_alive())\n    print(f'av efficiency: {p.av_efficiency()}')\n\n    p.map(printer, list(range(100)), table=ng_table)\n\n\n    # test most basic map function\n    #elements = list(range(100))\n    #assert(pool.map(example_func, elements) == [example_func(e) for e in elements])\n    print(f'av efficiency: {p.av_efficiency()}')\n</code></pre> <pre><code>---------------------------------------------------------------------------\n\nAssertionError                            Traceback (most recent call last)\n\n&lt;ipython-input-10-48aa27ce5dc0&gt; in &lt;module&gt;\n      3 \n      4 with doctable.WorkerPool(3, verbose=False) as p:\n----&gt; 5     assert(p.any_alive())\n      6     print(f'av efficiency: {p.av_efficiency()}')\n      7\n\n\nAssertionError:\n</code></pre> <pre><code>import pickle\npickle.dumps(ng_table.schema_info)\n</code></pre> <pre><code>import multiprocessing\nclass parse_thread:\n    def __init__(self, table: doctable.DocTable):\n        self.table = table\n\n    def __call__(self, texts):\n        with self.table as t:\n            #records = [NewsgroupDoc.from_string(text) for text in texts]\n\n            t.insert(NewsgroupDoc(1000))\n\ndef parse_thread2(x):\n    return None\n\nchunks = doctable.chunk(newsgroups['data'], chunk_size=100)\n#parse_func = parse_thread(ng_table)\nwith multiprocessing.Pool(4) as p:\n    %time p.map(parse_thread(ng_table), chunks, 100)\n#%time map(parse_thread(1), chunks)\n</code></pre> <pre><code>class process_and_store:\n    table: doctable.DocTable = None\n\n    def __init__(self, table_cls, *table_args, **table_kwargs):\n        '''Store info to construct the table.\n        '''\n        self.table_cls = table_cls\n        self.table_args = table_args\n        self.table_kwargs = table_kwargs\n\n    def connect_db(self):\n        '''Make a new connection to the database and return the associated table.\n        '''\n        if self.table is None:\n            self.table = self.table_cls(*self.table_args, **self.table_kwargs)\n        return self.table\n\n    def __call__(self, text):\n        '''Execute function in worker process.\n        '''\n        table = self.connect_db()\n\n        record = NewsgroupDoc.from_string(text)\n        table.insert(record)\n\nimport multiprocessing\nwith multiprocessing.Pool(4) as p:\n    %time p.map(process_and_store(NewsgroupTable, target=table_fname), newsgroups['data'])\n</code></pre> <p>Notice that this takes very little CPU time, but a long \"wall time\" (overall time it takes to run the program). This is because the threads are IO-starved - they spend a lot of time waiting on each other to commit database transactions. This might be a good opportunity to use variations on threading models, but most parsing classes </p> <pre><code>class process_and_store_chunk(process_and_store):\n    def __call__(self, texts):\n        '''Execute function in worker process.\n        '''\n        table = self.connect_db()\n\n        records = [NewsgroupDoc.from_string(text) for text in texts]\n        table.insert(records)\n\nchunked_newsgroups = doctable.chunk(newsgroups['data'], chunk_size=500)\nwith multiprocessing.Pool(4) as p:\n    %time p.map(process_and_store_chunk(NewsgroupTable, target=table_fname), chunked_newsgroups)\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre> <pre><code>parser = ParsePipeline([\n    parse_email\n])\n\n\nfor email_text in newsgroups['data']:\n    email_data = parse_email(email_text)\n</code></pre> <pre><code>\n\nimport multiprocessing\nwith multiprocessing.Pool(10) as p:\n    print(p)\n</code></pre>"},{"location":"legacy_documentation/distributed_basics/","title":"<code>Distribute</code> Parallel Processing Basics","text":"<p>Due to a number of limitations involving data passed to processes using <code>multiprocessing.Pool()</code>, I've implemented a similar class called <code>Distribute()</code>. The primary difference is that Distribute is meant to distribute chunks of data for parallel processing, so your map function should parse multiple values. There are currently two functions in Distribute:</p> <ul> <li><code>.map_chunk()</code> simply applies a function to a list of elements and returns a list of parsed elements.</li> <li><code>.map()</code> applies a function to a single element. Same as <code>multiprocessing.Pool().map()</code>.</li> </ul> <pre><code>#from IPython import get_ipython\nimport sys\nsys.path.append('..')\nimport doctable\n</code></pre>"},{"location":"legacy_documentation/distributed_basics/#map-method","title":"<code>.map()</code> Method","text":"<p>Unsurprisingly, this method works exactly like the <code>multiprocessing.Pool().map()</code>. Simply provide a sequence of elements and a function to apply to them, and this method will parse all the elements in parallel.</p> <pre><code>def multiply(x, y=2):\n    return x * y\n\nnums = list(range(5))\nwith doctable.Distribute(3) as d:\n    res = d.map(multiply, nums)\n    print(res)\n\n    # pass any argument to your function here.\n    # we try multiplying by 5 instead of 2.\n    res = d.map(multiply, nums, 5)\n    print(res)\n</code></pre> <pre><code>[0, 2, 4, 6, 8]\n[0, 5, 10, 15, 20]\n</code></pre>"},{"location":"legacy_documentation/distributed_basics/#map_chunk-method","title":"<code>.map_chunk()</code> Method","text":"<p>Allows you to write map functions that processes a chunk of your data at a time. This is the lowest-level method for distributed processing.</p> <pre><code># map function to multiply 1.275 by each num and return a list\ndef multiply_nums(nums):\n    return [num*1.275 for num in nums]\n\n# use Distribute(3) to create three separate processes\nnums = list(range(1000))\nwith doctable.Distribute(3) as d:\n    res = d.map_chunk(multiply_nums, nums)\n\n# won't create new process at all. good for testing\nwith doctable.Distribute(1) as d:\n    res = d.map_chunk(multiply_nums, nums)\nres[:3]\n</code></pre> <pre><code>CPU times: user 5.92 ms, sys: 13.3 ms, total: 19.2 ms\nWall time: 22.7 ms\nCPU times: user 63 \u00b5s, sys: 85 \u00b5s, total: 148 \u00b5s\nWall time: 154 \u00b5s\n\n\n\n\n\n[0.0, 1.275, 2.55]\n</code></pre>"},{"location":"legacy_documentation/doctable_basics/","title":"<code>DocTable</code> Overview","text":"<p>A <code>DocTable</code> acts as an object-oriented interface to a single database table. It combines the utility of <code>dataclasses</code> to create schemas from simple object definitions and sqlalchemy to create connections and execute queries to a database. It should be easy to convert existing data-oriented objects to database schemas, and use those objects when inserting/retrieving data. </p> <p>In this document I'll cover these topics:</p> <ol> <li>Creating Schemas</li> <li>Managing Connections</li> <li>Inserting, Deleting, and Selecting</li> <li>Select Queries</li> </ol> <p>You may also want to see the vignettes for more examples, the <code>DocTable</code> docs for more information about the class, or the schema guide for more information about creating schemas. I also recommend looking examples for insert, delete, select, and update methods.</p> <pre><code>import random\nrandom.seed(0)\nimport pandas as pd\nimport numpy as np\nfrom dataclasses import dataclass\n\nimport sys\nsys.path.append('..')\nimport doctable\n</code></pre>"},{"location":"legacy_documentation/doctable_basics/#1-creating-a-database-schema","title":"1. Creating a Database Schema","text":"<p><code>DocTable</code> schemas are created using the <code>doctable.schema</code> decorator on a class that uses <code>doctable.Col</code> for defaulted parameters. Check out the schema guide for more detail about schema classes. Our demonstration class will include three columns: <code>id</code>, <code>name</code>, and <code>age</code>, with an additional <code>.is_old</code> property derived from <code>age</code> for example.</p> <p>Note that the <code>id</code> column uses the default value <code>IDCol()</code> which sets the variable to be the primary key and to auto-increment. Arguments passed to the generic <code>Col()</code> function are passed directly to the sqlalchemy metadata to direct column creation. See more in the schema guide.</p> <pre><code>@doctable.schema\nclass Record:\n    __slots__ = []\n    id: int = doctable.IDCol()\n    name: str = doctable.Col(nullable=False)\n    age: int = doctable.Col()\n\n    @property\n    def is_old(self):\n        return self.age &gt;= 30 # lol\n</code></pre> <p>We can instantiate a <code>DocTable</code> by passing a <code>target</code> and <code>schema</code> (<code>Record</code> in our example) parameters, and I show the resulting schema using <code>.schema_table()</code>. Note that the type hints were used to describe column types, and <code>id</code> was used as the auto-incremented primary key.</p> <pre><code>table = doctable.DocTable(target=':memory:', schema=Record)\ntable.schema_table()\n</code></pre> name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 name VARCHAR False None auto 0 2 age INTEGER True None auto 0 <p>Probably a more common use case will be to subclass <code>DocTable</code> to provide some basic definitions.</p> <pre><code>class RecordTable(doctable.DocTable):\n    _tabname_ = 'records'\n    _schema_ = Record\n    _indices_ = (\n        doctable.Index('ind_age', 'age'),\n    )\n    _constraints_ = (\n        doctable.Constraint('check', 'age &gt; 0'),\n    )\n\ntable = RecordTable(target=':memory:')\ntable\n</code></pre> <pre><code>&lt;__main__.RecordTable at 0x7f5b40325940&gt;\n</code></pre>"},{"location":"legacy_documentation/doctable_basics/#2-maintaining-database-connections","title":"2. Maintaining Database Connections","text":"<p>Obviously a big part of working with databases involves managing connections with the database. By default, <code>DocTable</code> instances DO NOT maintain persistent connections to the database - instead, they open a connection as-needed when executing a query. Benchmark comparisons show that the cost of creating a connection is so low relative to an actual insertion that this probably the approach for most applications.</p> <p>Alternatively, there are several ways of working with connections: as a context manager, using the <code>persistent_conn</code> constructor parameter, manually calling <code>open_conn()</code> and <code>close_conn()</code> (not recommended), and manually requesting a connection to execute using your own sqlalchemy or raw sql library queries.</p> <ol> <li>As a context manager. Note that the <code>__enter__</code> method returns the doctable instance itself, so you can access it using with or without the \"as\" keyword.</li> </ol> <pre><code>tab = doctable.DocTable(target=':memory:', schema=Record)\nprint(tab._conn)\nwith tab as t:\n    r = Record(name = 'Devin Cornell', age = 32)\n    print(dir(r))\n    t.insert_single(Record(name = 'Devin Cornell', age = 32))\n    print(t._conn)\n\n# alternatively, no need to use \"as\"\nwith tab:\n    tab.insert_single(Record(name = 'Devin Cornell', age = 32))\n    print(tab._conn)\ntab.head()\n\n</code></pre> <pre><code>None\n['__annotations__', '__class__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__wrapped__', '_doctable__age', '_doctable__id', '_doctable__name', '_doctable_as_dict', '_doctable_from_db', '_doctable_get_val', 'age', 'as_dict', 'get_val', 'id', 'is_old', 'name']\n&lt;sqlalchemy.engine.base.Connection object at 0x7f5b40304550&gt;\n&lt;sqlalchemy.engine.base.Connection object at 0x7f5b403141c0&gt;\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead.\n  warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.')\n</code></pre> id name age 0 1 Devin Cornell 32 1 2 Devin Cornell 32 <ol> <li>Using the constructor argument <code>persistent_conn</code></li> </ol> <pre><code>tab = doctable.DocTable(target=':memory:', schema=Record, persistent_conn=False)\nprint(tab._conn)\ntab = doctable.DocTable(target=':memory:', schema=Record, persistent_conn=True)\ntab._conn\n</code></pre> <pre><code>None\n\n\n\n\n\n&lt;sqlalchemy.engine.base.Connection at 0x7f5b402cd280&gt;\n</code></pre> <ol> <li>Manually calling <code>.open_conn()</code> and <code>.close_conn()</code>. I recommend using a context manager if you go this route.</li> </ol> <pre><code>tab = doctable.DocTable(target=':memory:', schema=Record)\nprint(tab._conn)\ntab.open_conn()\nprint(tab._conn)\ntab.close_conn()\nprint(tab._conn)\n</code></pre> <pre><code>None\n&lt;sqlalchemy.engine.base.Connection object at 0x7f5b403097c0&gt;\nNone\n</code></pre> <ol> <li>grabbing a connection object to execute your own sqlalchemy queries</li> </ol> <pre><code>conn = tab.connect()\nconn\n</code></pre> <pre><code>&lt;sqlalchemy.engine.base.Connection at 0x7f5b40309550&gt;\n</code></pre>"},{"location":"legacy_documentation/doctable_basics/#3-insert-delete-and-select","title":"3. Insert, Delete, and Select","text":"<p>The nature of doctable schema definitions means the easiest way to work with database data is often to use the schema class as a normal dataclass. I recommend the schema guide for more detail about the relationship between dataclasses, schema classes, and behavior of the actual database. While the intent behind using dataclasses for database schemas is intuitive and valuable, it can be tricky.</p> <p>NOTE!!!: Unlike ORM-based applications, <code>DocTable</code> instances do not have any connection to instances of the schema class - they are simply used to encapsulate data to be stored and retrieved in the table. This is why the same object can be inserted multiple times in this example.</p> <p>Lets start off by creating some record objects and inserting them into the database with <code>.insert_single()</code> and <code>.insert_many()</code>. In the <code>Record</code> constructor here we do not specify the id value - this is because our database schema dictated that it will be automatically incremented by the database - if we omit the value in the constructor, by default it will simply not pass any value to the database at all (this can be changed later though). See that the results of our call to <code>.head()</code> shows that the rows were given id values upon insertion.</p> <pre><code>table = doctable.DocTable(target=':memory:', schema=Record)\n\no = Record(name='Devin Cornell', age=35)\ntable.insert_single(o, verbose=True)\ntable.insert_single(o)\ntable.insert_many([o, o], verbose=True)\ntable.head()\n</code></pre> <pre><code>DocTable: INSERT OR FAIL INTO _documents_ (id, name, age) VALUES (?, ?, ?)\nDocTable: INSERT OR FAIL INTO _documents_ (id, name, age) VALUES (?, ?, ?)\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: .insert_many() is depricated: please use .q.insert_multi() or .q.insert_multi_raw()\n  warnings.warn(f'.insert_many() is depricated: please use .q.insert_multi() or '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead.\n  warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.')\n</code></pre> id name age 0 1 Devin Cornell 35 1 2 Devin Cornell 35 2 3 Devin Cornell 35 3 4 Devin Cornell 35 <p>Now we use <code>.select()</code> to retrieve data from the database. Here we call it with no parameters to simply get all the objects we previously inserted, this time with the id values that the database provided.</p> <pre><code>results = table.select(verbose=True)\nresults\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age \nFROM _documents_\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:452: UserWarning: Method .select() is depricated. Please use .q.select() instead.\n  warnings.warn('Method .select() is depricated. Please use .q.select() instead.')\n\n\n\n\n\n[Record(id=1, name='Devin Cornell', age=35),\n Record(id=2, name='Devin Cornell', age=35),\n Record(id=3, name='Devin Cornell', age=35),\n Record(id=4, name='Devin Cornell', age=35)]\n</code></pre>"},{"location":"legacy_documentation/doctable_basics/#4-more-complicated-queries","title":"4. More Complicated Queries","text":"<p>And, of course, the most important part of any database library is to execute queries. To do this, <code>DocTable</code> objects keep track of sqlalchemy core <code>MetaData</code> and <code>Table</code> objects and build queries using the <code>select()</code>, <code>delete()</code>, <code>insert()</code>, and <code>update()</code> methods from sqlalchemy core.</p> <p>First, note that subscripting the table object allows you to access the underlying sqlalchemy <code>Column</code> objects, which, as I will show a bit later, can be used to create where conditionals for select and update queries. You can also access specific column data using the <code>.c</code> property of the doctable.</p> <pre><code>table = doctable.DocTable(target=':memory:', schema=Record)\n\ntable['id'], table.c.id\n</code></pre> <pre><code>(Column('id', Integer(), table=&lt;_documents_&gt;, primary_key=True, nullable=False),\n Column('id', Integer(), table=&lt;_documents_&gt;, primary_key=True, nullable=False))\n</code></pre> <p>As we'll show later, these column objects also have some operators defined such that they can be used to construct complex queries and functions. You can read more about this in the sqlalchemy operators documentation.</p> <pre><code>table.c.id &gt; 3, table.c.id.in_([1,2]), table.c.age == 4\n</code></pre> <pre><code>(&lt;sqlalchemy.sql.elements.BinaryExpression object at 0x7f5b4025cd00&gt;,\n &lt;sqlalchemy.sql.elements.BinaryExpression object at 0x7f5b4025cc70&gt;,\n &lt;sqlalchemy.sql.elements.BinaryExpression object at 0x7f5b4025c8e0&gt;)\n</code></pre> <p>You can use these expressions as part of <code>select()</code>, <code>update()</code>, and <code>delete()</code> operations by passing them to the <code>where</code> argument.</p> <pre><code>table.insert_single(Record(name='Devin Cornell', age=35))\ntable.insert_single(Record(name='Sam Adams', age=250))\ntable.insert_single(Record(name='Rando', age=500))\n\ntable.select(where=table.c.id &gt;= 3, verbose=True)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age \nFROM _documents_ \nWHERE _documents_.id &gt;= ?\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n\n\n\n\n\n[Record(id=3, name='Rando', age=500)]\n</code></pre> <pre><code>table.select_first(where=table.c.name=='Devin Cornell', verbose=True)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age \nFROM _documents_ \nWHERE _documents_.name = ?\n LIMIT ? OFFSET ?\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:427: UserWarning: Method .select_first() is depricated. Please use .q.select_first() instead.\n  warnings.warn('Method .select_first() is depricated. Please use .q.select_first() instead.')\n\n\n\n\n\nRecord(id=1, name='Devin Cornell', age=35)\n</code></pre>"},{"location":"legacy_documentation/doctable_basics/#select-statements","title":"Select Statements","text":"<p>Now we show how to select data from the table. Use the <code>.count()</code> method to check the number of rows. It also accepts some column conditionals to count entries that satisfy a given criteria</p> <pre><code>table.count(verbose=True), table.count(table['age']&gt;=30, verbose=True)\n</code></pre> <pre><code>DocTable: SELECT count(_documents_.id) AS count_1 \nFROM _documents_\n LIMIT ? OFFSET ?\nDocTable: SELECT count(_documents_.id) AS count_1 \nFROM _documents_ \nWHERE _documents_.age &gt;= ?\n LIMIT ? OFFSET ?\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead.\n  warnings.warn('Method .count() is depricated. Please use .q.count() instead.')\n\n\n\n\n\n(3, 3)\n</code></pre> <p>Use the <code>.select()</code> method with no arguments to retrieve all rows of the table. You can also choose to select one or more columns to select.</p> <pre><code>table.select(verbose=True)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age \nFROM _documents_\n\n\n\n\n\n[Record(id=1, name='Devin Cornell', age=35),\n Record(id=2, name='Sam Adams', age=250),\n Record(id=3, name='Rando', age=500)]\n</code></pre> <p>By specifying a column name, you can retrieve a list of column values, or by offering a list of data, you can request only those datas.</p> <pre><code>table.select('name', verbose=True)\n</code></pre> <pre><code>DocTable: SELECT _documents_.name \nFROM _documents_\n\n\n\n\n\n['Devin Cornell', 'Sam Adams', 'Rando']\n</code></pre> <pre><code># note we have no access to the ID column - just name, but still part of Record type.\ntable.select(['name'], verbose=True)\n</code></pre> <pre><code>DocTable: SELECT _documents_.name \nFROM _documents_\n\n\n\n\n\n[Record(name='Devin Cornell'), Record(name='Sam Adams'), Record(name='Rando')]\n</code></pre> <p>Accessing a property which was not retrieved from the database will raise an exception.</p> <pre><code>rec = table.select_first(['name'])\ntry:\n    rec.id\nexcept doctable.RowDataNotAvailableError as e:\n    print('This exception was raised:', e)\n</code></pre> <pre><code>This exception was raised: The \"id\" property is not available. This might happen if you did not retrieve the information from a database or if you did not provide a value in the class constructor.\n</code></pre> <p>You may also use aggregation functions like <code>.sum</code>.</p> <pre><code>table.select_first(table['age'].sum(), verbose=True)\n</code></pre> <pre><code>DocTable: SELECT sum(_documents_.age) AS sum_1 \nFROM _documents_\n LIMIT ? OFFSET ?\n\n\n\n\n\n785\n</code></pre> <p>The SUM() and COUNT() SQL functions have been mapped to <code>.sum</code> and <code>.count</code> attributes of columns. Use <code>as_dataclass=False</code> if you do retrieve data which does not fit into a <code>Record</code> object.</p> <pre><code>table.select_first([table['age'].sum(),table['age'].count()], verbose=True)\n</code></pre> <pre><code>DocTable: SELECT sum(_documents_.age) AS sum_1, count(_documents_.age) AS count_1 \nFROM _documents_\n LIMIT ? OFFSET ?\nDocTable: SELECT sum(_documents_.age) AS sum_1, count(_documents_.age) AS count_1 \nFROM _documents_\n LIMIT ? OFFSET ?\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Conversion from row to object failed according to the following error. Please use .q.select_first(..,raw_result=True) next time in the future to avoid this issue. e=RowDataConversionFailed(\"Conversion from &lt;class 'sqlalchemy.engine.row.LegacyRow'&gt; to &lt;class '__main__.Record'&gt; failed.\")\n  warnings.warn(f'Conversion from row to object failed according to the following '\n\n\n\n\n\n(785, 3)\n</code></pre> <p>Alternatively, to see the results as a pandas dataframe, we can use <code>.select_df()</code>.</p> <pre><code>table.select_df(verbose=True)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age \nFROM _documents_\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead.\n  warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.')\n</code></pre> id name age 0 1 Devin Cornell 35 1 2 Sam Adams 250 2 3 Rando 500 <p>Now we can select specific elements of the db using the <code>where</code> argument of the <code>.select()</code> method.</p> <pre><code>table.select(where=table['age'] &gt;= 1, verbose=True)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age \nFROM _documents_ \nWHERE _documents_.age &gt;= ?\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:452: UserWarning: Method .select() is depricated. Please use .q.select() instead.\n  warnings.warn('Method .select() is depricated. Please use .q.select() instead.')\n\n\n\n\n\n[Record(id=1, name='Devin Cornell', age=35),\n Record(id=2, name='Sam Adams', age=250),\n Record(id=3, name='Rando', age=500)]\n</code></pre> <pre><code>table.select(where=table['id']==3, verbose=True)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age \nFROM _documents_ \nWHERE _documents_.id = ?\n\n\n\n\n\n[Record(id=3, name='Rando', age=500)]\n</code></pre> <p>We can update the results in a similar way, using the <code>where</code> argument.</p> <pre><code>table.update({'name':'smartypants'}, where=table['id']==3, verbose=True)\ntable.select()\n</code></pre> <pre><code>DocTable: UPDATE _documents_ SET name=? WHERE _documents_.id = ?\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:504: UserWarning: Method .update() is depricated. Please use .q.update() instead.\n  warnings.warn('Method .update() is depricated. Please use .q.update() instead.')\n\n\n\n\n\n[Record(id=1, name='Devin Cornell', age=35),\n Record(id=2, name='Sam Adams', age=250),\n Record(id=3, name='smartypants', age=500)]\n</code></pre> <pre><code>print(table['age']*100)\ntable.update({'age':table['age']*100}, verbose=True)\ntable.select()\n</code></pre> <pre><code>_documents_.age * :age_1\nDocTable: UPDATE _documents_ SET age=(_documents_.age * ?)\n\n\n\n\n\n[Record(id=1, name='Devin Cornell', age=3500),\n Record(id=2, name='Sam Adams', age=25000),\n Record(id=3, name='smartypants', age=50000)]\n</code></pre> <p>And we can delete elements using the <code>.delete()</code> method.</p> <pre><code>table.delete(where=table['id']==3, verbose=True)\ntable.select()\n</code></pre> <pre><code>DocTable: DELETE FROM _documents_ WHERE _documents_.id = ?\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:509: UserWarning: Method .delete() is depricated. Please use .q.delete() instead.\n  warnings.warn('Method .delete() is depricated. Please use .q.delete() instead.')\n\n\n\n\n\n[Record(id=1, name='Devin Cornell', age=3500),\n Record(id=2, name='Sam Adams', age=25000)]\n</code></pre>"},{"location":"legacy_documentation/doctable_basics/#notes-on-db-interface","title":"Notes on DB Interface","text":"<p>DocTable2 allows you to access columns through direct subscripting, then relies on the power of sqlalchemy column objects to do most of the work of constructing queries. Here are a few notes on their use. For more demonstration, see the example in examples/dt2_select.ipynb</p> <pre><code># subscript is used to access underlying sqlalchemy column reference (without querying data)\ntable['id']\n</code></pre> <pre><code>Column('id', Integer(), table=&lt;_documents_&gt;, primary_key=True, nullable=False)\n</code></pre> <pre><code># conditionals are applied directly to the column objects (as we'll see with \"where\" clause)\ntable['id'] &lt; 3\n</code></pre> <pre><code>&lt;sqlalchemy.sql.elements.BinaryExpression object at 0x7f5b4022fc10&gt;\n</code></pre> <pre><code># can also access using .col() method\ntable.col('id')\n</code></pre> <pre><code>Column('id', Integer(), table=&lt;_documents_&gt;, primary_key=True, nullable=False)\n</code></pre> <pre><code># to access all column objects (only useful for working directly with sql info)\ntable.columns\n</code></pre> <pre><code>&lt;sqlalchemy.sql.base.ImmutableColumnCollection at 0x7f5b40388db0&gt;\n</code></pre> <pre><code># to access more detailed schema information\ntable.schema_table()\n</code></pre> name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 name VARCHAR False None auto 0 2 age INTEGER True None auto 0 <pre><code># If needed, you can also access the sqlalchemy table object using the .table property.\ntable.table\n</code></pre> <pre><code>Table('_documents_', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', Integer(), table=&lt;_documents_&gt;, primary_key=True, nullable=False), Column('name', String(), table=&lt;_documents_&gt;, nullable=False), Column('age', Integer(), table=&lt;_documents_&gt;), schema=None)\n</code></pre> <pre><code># the count method is also an easy way to count rows in the database\ntable.count()\n</code></pre> <pre><code>/DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead.\n  warnings.warn('Method .count() is depricated. Please use .q.count() instead.')\n\n\n\n\n\n2\n</code></pre> <pre><code># the print method makes it easy to see the table name and total row count\nprint(table)\n</code></pre> <pre><code>&lt;DocTable (3 cols)::sqlite:///:memory::_documents_&gt;\n</code></pre>"},{"location":"legacy_documentation/doctable_bootstrap/","title":"Document Bootstrapping Examples","text":"<p>When estimating machine learning or statistical models on your corpus, you may need to bootstrap documents (randomly sample with replacement). The <code>.bootstrap()</code> method of <code>DocTable</code> will act like a select statement but return a bootstrap object instead of a direct query result. Here I show how to do some basic bootstrapping using an example doctable.</p> <pre><code>import random\nimport pandas as pd\nimport numpy as np\nimport sys\nsys.path.append('..')\nimport doctable as dt\n</code></pre>"},{"location":"legacy_documentation/doctable_bootstrap/#create-example-doctable","title":"Create Example DocTable","text":"<p>First we define a DocTable that will be used for examples.</p> <pre><code>schema = (\n    ('integer','id',dict(primary_key=True, autoincrement=True)),\n    ('string','name', dict(nullable=False, unique=True)),\n    ('integer','age'),\n    ('boolean', 'is_old'),\n)\ndb = dt.DocTable(target=':memory:', schema=schema)\nprint(db)\n</code></pre> <pre><code>&lt;DocTable::sqlite:///:memory::_documents_ ct: 0&gt;\n</code></pre> <p>Then we add several example rows to the doctable.</p> <pre><code>for i in range(10):\n    age = random.random() # number in [0,1]\n    is_old = age &gt; 0.5\n    row = {'name':'user_'+str(i), 'age':age, 'is_old':is_old}\n    db.insert(row, ifnotunique='replace')\n\nfor doc in db.select(limit=3):\n    print(doc)\n</code></pre> <pre><code>(1, 'user_0', 0.16086747483303065, False)\n(2, 'user_1', 0.14322051505126332, False)\n(3, 'user_2', 0.22664393988892395, False)\n</code></pre>"},{"location":"legacy_documentation/doctable_bootstrap/#create-a-bootstrap","title":"Create a Bootstrap","text":"<p>We can use the doctable method <code>.bootstrap()</code> to return a bootstrap object using the keyword argument <code>n</code> to set the sample size (will use number of docs by default). This method acts like a select query, so we can specify columns and use the where argument to choose columns and rows to be bootstrapped. The bootsrap object contains the rows in the <code>.doc</code> property.</p> <p>Notice that while our select statement drew three documens, the sample size specified with <code>n</code> is 5. The boostrap object will always return 5 objects, even though the number of docs stays the same.</p> <pre><code>bs = db.bootstrap(['name','age'], where=db['id'] % 3 == 0, n=4)\nprint(type(bs))\nprint(len(bs.docs))\nbs.n\n</code></pre> <pre><code>&lt;class 'doctable.bootstrap.DocBootstrap'&gt;\n3\n\n\n\n\n\n4\n</code></pre> <p>Use the bootstrap object as an iterator to access the bootstrapped docs. The bootstrap object draws a sample upon instantiation, so the same sample is maintained until reset.</p> <pre><code>print('first run:')\nfor doc in bs:\n    print(doc)\nprint('second run:')\nfor doc in bs:\n    print(doc)\n</code></pre> <pre><code>first run:\n('user_5', 0.6473182290263347)\n('user_2', 0.22664393988892395)\n('user_2', 0.22664393988892395)\n('user_5', 0.6473182290263347)\nsecond run:\n('user_5', 0.6473182290263347)\n('user_2', 0.22664393988892395)\n('user_2', 0.22664393988892395)\n('user_5', 0.6473182290263347)\n</code></pre>"},{"location":"legacy_documentation/doctable_bootstrap/#draw-new-sample","title":"Draw New Sample","text":"<p>You can reset the internal sample of the bootstrap object using the <code>.set_new_sample()</code> method. See that we now sample 2 docs and the output is different from previous runs. The sample will still remain the same each time we iterate until we reset the sample.</p> <pre><code>bs.set_new_sample(2)\nprint('first run:')\nfor doc in bs:\n    print(doc)\nprint('second run:')\nfor doc in bs:\n    print(doc)\n</code></pre> <pre><code>first run:\n('user_5', 0.6473182290263347)\n('user_8', 0.5270190808172914)\nsecond run:\n('user_5', 0.6473182290263347)\n('user_8', 0.5270190808172914)\n</code></pre> <p>And we can iterate through a new sample using <code>.new_sample()</code>. Equivalent to calling <code>.set_new_sample()</code> and then iterating through elements.</p> <pre><code>print('drawing new sample:')\nfor doc in bs.new_sample(3):\n    print(doc)\nprint('repeating sample:')\nfor doc in bs:\n    print(doc)\n</code></pre> <pre><code>drawing new sample:\n('user_5', 0.6473182290263347)\n('user_5', 0.6473182290263347)\n('user_8', 0.5270190808172914)\nrepeating sample:\n('user_5', 0.6473182290263347)\n('user_5', 0.6473182290263347)\n('user_8', 0.5270190808172914)\n</code></pre> <p>I may add additional functionality in the future if I use this in any projects, but that's it for now.</p>"},{"location":"legacy_documentation/doctable_concurrency/","title":"Concurrent Database Connections","text":"<p>DocTable makes it easy to establish concurrent database connections from different processes. DocTable objects can be copied as-is from one process to another, except that you must call <code>.reopen_engine()</code> to initialize in process thread. This removes now stale database connections (which are not meant to traverse processes) from the engine connection pool.</p> <p>You may also want to use a large timeout using the timeout argument of the doctable constructor (provided in seconds).</p> <pre><code>import sqlalchemy\nfrom multiprocessing import Process\nimport os\nimport random\nimport string\nimport dataclasses\nimport time\nimport sys\nimport pathlib\nsys.path.append('..')\nimport doctable\n</code></pre> <pre><code>import datetime\n@doctable.schema\nclass SimpleRow:\n    __slots__ = []\n    id: int = doctable.IDCol()\n    updated: datetime.datetime = doctable.AddedCol()\n    process: str = doctable.Col()\n    number: int = doctable.Col()\n\n#tmp = doctable.TempFolder('exdb')\nimport tempfile\ntmpf = tempfile.TemporaryDirectory()\ntmp = pathlib.Path(tmpf.name)\ndb = doctable.DocTable(schema=SimpleRow, target=tmp.joinpath('tmp_concurrent.db'), new_db=True, connect_args={'timeout': 15})\n</code></pre> <pre><code>def thread_func(numbers, db):\n    process_id = ''.join(random.choices(string.ascii_uppercase, k=2))\n    print(f'starting process {process_id}')\n    db.reopen_engine() # create all new connections\n    for num in numbers:\n        db.insert({'process': process_id, 'number': num})\n        time.sleep(0.01)\n\nnumbers = list(range(100)) # these numbers are to be inserted into the database\n\ndb.delete()\nwith doctable.Distribute(5) as d:\n    d.map_chunk(thread_func, numbers, db)\ndb.head(10)\n</code></pre> <pre><code>/DataDrive/code/doctable/examples/../doctable/doctable.py:506: UserWarning: Method .delete() is depricated. Please use .q.delete() instead.\n  warnings.warn('Method .delete() is depricated. Please use .q.delete() instead.')\n\n\nstarting process OC\nstarting process BI\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead.\n  warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n\n\n\n\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead.\n  warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n\n\nstarting process FF\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead.\n  warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), '\n\n\nstarting process PC\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead.\n  warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), '\n\n\nstarting process YC\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n\n\n\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead.\n  warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead.\n  warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.')\n</code></pre> 10 0 10"},{"location":"legacy_documentation/doctable_connectengine/","title":"Manage SQL Connections with DocTable","text":"<p>This is meant to give a bit more depth describing how doctable works under-the-hood. I won't cover the details of DocTable methods or working with doctable objects, but I will try to give a clearer sense of how connections and tables are managed within a doctable instance.</p> <p>The driving motivator behind doctable is to create an object-oriented interface for working with sql tables by linking schemas described in your code with the structure of the databases you work with. This model is less ideal for the kinds of application-based frameworks where you would define the database schema once and build code around it separately, but works well for data science applications where you will be creating new tables and playing with different schemas regularly as your approach and end-goals change.</p> <p>When you instantiate a DocTable (or inheriting class), the object will convert your provided schema into a set of sqlalchemy objects which are then stored in-memory as part of the doctable instance. If the table does not already exist in the actual database, DocTable can create one that matches the provided schema, and then the schema will be used to work with the underlying database table. I will now discuss the lower-level objects that manage the metadata and connections to the database.</p> <pre><code>import sys\nsys.path.append('..')\nimport doctable\n</code></pre>"},{"location":"legacy_documentation/doctable_connectengine/#connectengine-class","title":"ConnectEngine Class","text":"<p>Each doctable maintains a <code>ConnectEngine</code> object to manage database connections and metadata that make all other database operations possible. I'll demonstrate how to instantiate this class manually to show how it works.</p> <p>The constructor takes arguments for dialect (sqlite, mysql, etc) and database target (filename or database server) to create new sqlalchemy engine and metadata objects. The engine object stores information about the target and can generate database connections, the metadata object stores schemas for registered tables. To work with a table, the metadata object must have the table schema registered, although it can be constructed from the database object itself.</p> <p>See here that the constructor requires a target (file or server where the database is located) and a dialect (flavor of database engine). This connection sits above individual table connections, and thus maintains no connections of it's own - only the engine that can create connections. We can, however list the tables in the database and perform other operations on the table.</p> <pre><code>engine = doctable.ConnectEngine(target=':memory:', dialect='sqlite')\nengine\n</code></pre> <pre><code>sqlite:///:memory:\n</code></pre>"},{"location":"legacy_documentation/doctable_connectengine/#working-with-tables","title":"Working with tables","text":"<p>You can also execute connectionless queries directly from this object, although normally you would create a connection object first and then execute queries from the connection. In this example I use a custom sql query to create a new table.</p> <p>As the ConnectEngine sits above the level of tables, we can list and drop tables from here.</p> <pre><code># see there are no tables here yet.\nengine.list_tables()\n</code></pre> <pre><code>[]\n</code></pre> <pre><code># run this raw sql query just for example\n# NOTE: Normally you would NOT create a table this way using doctable.\n# This is just for example purposes.\nquery = 'CREATE TABLE temp (id INTEGER PRIMARY KEY, number INTEGER NOT NULL)'\nengine.execute(query)\n</code></pre> <pre><code>&lt;sqlalchemy.engine.result.ResultProxy at 0x7f62a10e9370&gt;\n</code></pre> <pre><code># see that the table is now in the database\nengine.list_tables()\n</code></pre> <pre><code>['temp']\n</code></pre> <pre><code># uses inspect to ask the database directly for the schema\nengine.schema('temp')\n</code></pre> <pre><code>[{'name': 'id',\n  'type': INTEGER(),\n  'nullable': True,\n  'default': None,\n  'autoincrement': 'auto',\n  'primary_key': 1},\n {'name': 'number',\n  'type': INTEGER(),\n  'nullable': False,\n  'default': None,\n  'autoincrement': 'auto',\n  'primary_key': 0}]\n</code></pre> <pre><code># or as a dataframe\nengine.schema_df('temp')\n</code></pre> name type nullable default autoincrement primary_key 0 id INTEGER True None auto 1 1 number INTEGER False None auto 0 <p>All of these methods I've shown so far access the database tables directly, but currently our python objects do not have any idea of what the table schema looks like. You can view the sqlalchemy table objects actually registered with the engine by using the .tables property. See that it is currently empty! Our python code is not able to work with the table using objects because it does not have record of the schema. Now we'll show how to register tables with the engine.</p>"},{"location":"legacy_documentation/doctable_connectengine/#creating-and-accessing-tables","title":"Creating and accessing tables","text":"<p>To create a data structure internally representing the database structure, we can either ask sqlalchemy to read the database and create the schema, or we can provide lists of sqlalchemy column objects. Wee that we can access the registered tables using the .tables property.</p> <pre><code># see that currently our engine does not have information about the table we created above.\nengine.tables\n</code></pre> <pre><code>immutabledict({})\n</code></pre> <pre><code># now I ask doctable to read the database schema and register the table in metadata.\nengine.add_table('temp')\n</code></pre> <pre><code>Table('temp', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=&lt;temp&gt;, primary_key=True, nullable=False), Column('number', INTEGER(), table=&lt;temp&gt;, nullable=False), schema=None)\n</code></pre> <pre><code># and we can see that the table is registered\nengine.tables\n</code></pre> <pre><code>immutabledict({'temp': Table('temp', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=&lt;temp&gt;, primary_key=True, nullable=False), Column('number', INTEGER(), table=&lt;temp&gt;, nullable=False), schema=None)})\n</code></pre> <p>When add_table() is called, a new sqlalchemy.Table object is registered in the engine's metadata and returned. If add_table() is called again, it will return the table already registered in the metadata. Because we usually use doctable to manage tables, we'll just show a short example here.</p> <pre><code># while we can use doctable to do most of this work \n#  usually, I'll just show how sqlalchemy core objects \n#  can be used to create a table in ConnectEngine.\nfrom sqlalchemy import Column, Integer, String\n\n# create a list of columns\ncolumns = (\n    Column('id', Integer, primary_key = True), \n    Column('name', String), \n)\n\n# we similarly use the add_table() method to store the schema\n#  in the metadata\nengine.add_table('temp2', columns=columns)\n</code></pre> <pre><code>Table('temp2', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', Integer(), table=&lt;temp2&gt;, primary_key=True, nullable=False), Column('name', String(), table=&lt;temp2&gt;), schema=None)\n</code></pre> <pre><code># see now that the engine has information about both tables\nengine.tables\n</code></pre> <pre><code>immutabledict({'temp': Table('temp', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=&lt;temp&gt;, primary_key=True, nullable=False), Column('number', INTEGER(), table=&lt;temp&gt;, nullable=False), schema=None), 'temp2': Table('temp2', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', Integer(), table=&lt;temp2&gt;, primary_key=True, nullable=False), Column('name', String(), table=&lt;temp2&gt;), schema=None)})\n</code></pre> <pre><code># and see that you can get individual table object references like this\nengine.tables['temp']\n</code></pre> <pre><code>Table('temp', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=&lt;temp&gt;, primary_key=True, nullable=False), Column('number', INTEGER(), table=&lt;temp&gt;, nullable=False), schema=None)\n</code></pre>"},{"location":"legacy_documentation/doctable_connectengine/#dropping-tables","title":"Dropping tables","text":"<p>Dropping tables is simple enough, but remember that the schema stored in the database and the objects in code mirror each other, so it is best to manipulate them at the same time. Use .drop_table instead of issuing CREATE TABLE query to make sure they stay in sync. The method can also be used on tables that are not in the metadata engine.</p> <pre><code># by providing the argument as a string\nengine.drop_table('temp')\n</code></pre> <pre><code>engine.list_tables()\n</code></pre> <pre><code>['temp2']\n</code></pre> <p>In cases where an underlying table has been deleted but metadata is retained, the drop_table() method will still work but you may need to call clear_metadata() to flush all metadata and add_all_tables() to re-create the metadata from the actual data.</p> <pre><code># see this works although the temp3 table is not registered in engine metadata\nquery = 'CREATE TABLE temp3 (id INTEGER PRIMARY KEY, number INTEGER NOT NULL)'\nengine.execute(query)\nengine.drop_table('temp3')\n</code></pre> <pre><code># this will delete the underlying table even though the metadata information still exists.\nquery = 'CREATE TABLE temp4 (id INTEGER PRIMARY KEY, number INTEGER NOT NULL)'\nengine.execute(query)\nengine.execute(f'DROP TABLE IF EXISTS temp4')\nengine.list_tables()\n</code></pre> <pre><code>['temp2']\n</code></pre> <pre><code># see that the table is still registered in the metadata\nengine.tables\n</code></pre> <pre><code>immutabledict({'temp': Table('temp', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=&lt;temp&gt;, primary_key=True, nullable=False), Column('number', INTEGER(), table=&lt;temp&gt;, nullable=False), schema=None), 'temp2': Table('temp2', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', Integer(), table=&lt;temp2&gt;, primary_key=True, nullable=False), Column('name', String(), table=&lt;temp2&gt;), schema=None), 'temp3': Table('temp3', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=&lt;temp3&gt;, primary_key=True, nullable=False), Column('number', INTEGER(), table=&lt;temp3&gt;, nullable=False), schema=None)})\n</code></pre> <pre><code># in this case, it might be simplest just to clear all metadata\n# and re-build according to exising tables\nengine.clear_metadata()\nengine.reflect()\nengine.tables\n</code></pre> <pre><code>immutabledict({'temp2': Table('temp2', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=&lt;temp2&gt;, primary_key=True, nullable=False), Column('name', VARCHAR(), table=&lt;temp2&gt;), schema=None)})\n</code></pre>"},{"location":"legacy_documentation/doctable_connectengine/#managing-connections-with-connectengine","title":"Managing connections with ConnectEngine","text":"<p>ConnectEngine objects are used to create database connections which are maintained by individual doctable objects. Use the get_connection() function to retreive a new connection object which you can use to execute queries. While garbage collecting the connection objects will close the individual connection, sometimes all connections need to be closed simultaneously. This is especially important because garbage-collecting the ConnectEngine object doesn't mean the connections will be garbage-collected if they have references elsewhere in your code. You can close all connections using the close_connections() method.</p> <pre><code># make new connection\nconn = engine.connect()\nconn\n</code></pre> <pre><code>&lt;sqlalchemy.engine.base.Connection at 0x7f62a10301f0&gt;\n</code></pre> <pre><code># see here we just run a select query on the empty table, returning an empty list\nlist(conn.execute('SELECT * FROM temp2'))\n</code></pre> <pre><code>[]\n</code></pre> <p>An important use-case of this feature is when you have multiple processes accessing the same database. In general, each process should have separate connections to the database, but both the engine and metadata stored with the ConnectEngine can be copied. Here I'll show a basic multiprocessing case using the Distribute class (it works much like multiprocessing.Pool()).</p> <p>In using the map function we open two processes, and in the thread function we call the close_connections() method to delete existing connections which don't exist in this new memory space.</p> <pre><code>def thread(nums, engine: doctable.ConnectEngine):\n    # close connections that were opened in other thread\n    #engine.close_connections()\n    engine.dispose()\n\n    # create a new connection for this thread\n    thread_conn = engine.connect()\n\nnumbers = [1,2]\nwith doctable.Distribute(2) as d:\n    d.map(thread, numbers, engine)\n</code></pre> <pre><code>engine.list_tables()\n</code></pre> <pre><code>['temp2']\n</code></pre>"},{"location":"legacy_documentation/doctable_connectengine/#doctable-and-connectengine","title":"DocTable and ConnectEngine","text":"<p>Every DocTable object maintains a ConnectEngine to store information about the table they represent, and can be accessed through the engine property. When a target and dialect are provided to doctable, it will automatically initialize a new ConnectEngine and store a new connection object.</p> <pre><code># create a new doctable and view it's engine\nschema = (('idcol', 'id'), ('string', 'name'))\ndb = doctable.DocTable(target=':memory:', schema=schema)\nstr(db.engine)\n</code></pre> <pre><code>'&lt;ConnectEngine::sqlite:///:memory:&gt;'\n</code></pre> <p>The DocTable constructor can also accept an engine in place of a target and dialect, and thus share ConnectEngines between multiple DocTable objects. In this case, the doctable constructor will use the provided schema to insert the table information into the engine metadata and create the table if doesn't already exist. It will also generate a new connection object from the ConnectEngine.</p> <pre><code># a w\nengine.clear_metadata()\nprint(engine.tables.keys())\nprint(engine.list_tables())\n</code></pre> <pre><code>dict_keys([])\n['temp2']\n</code></pre> <pre><code># make a new doctable using the existing engine\nschema = (('idcol', 'id'), ('string', 'name'))\ndb = doctable.DocTable(engine=engine, schema=schema, tabname='tmp5')\ndb\n</code></pre> <pre><code>&lt;doctable.doctable.DocTable at 0x7f62a1096b20&gt;\n</code></pre> <pre><code># make another doctable using existing engine\nschema2 = (('idcol', 'id'), ('string', 'name'))\ndb2 = doctable.DocTable(engine=engine, schema=schema2, tabname='tmp6')\ndb2\n</code></pre> <pre><code>&lt;doctable.doctable.DocTable at 0x7f62a0fd4cd0&gt;\n</code></pre> <pre><code># we can see that both tables have been created in the database\nengine.list_tables()\n</code></pre> <pre><code>['temp2', 'tmp5', 'tmp6']\n</code></pre> <pre><code># and that both are registered in the metadata\nengine.tables.keys()\n</code></pre> <pre><code>dict_keys(['tmp5', 'tmp6'])\n</code></pre> <p>Some ConnectEngine methods are also accessable through the DocTable instances.</p> <pre><code>db.list_tables()\n</code></pre> <pre><code>['temp2', 'tmp5', 'tmp6']\n</code></pre> <pre><code>db.schema_table()\n</code></pre> name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 name VARCHAR True None auto 0 <pre><code># and this is equivalent to calling the engine method reopen(), which clears \n#  metadata and closes connection pool\ndb.reopen_engine()\n</code></pre>"},{"location":"legacy_documentation/doctable_file_column_types/","title":"DocTable File Column Types","text":"<p>It is often good advice to avoid storing large binary data in an SQL table because it will significantly impact the read performance of the entire table. I find, however, that it can be extremely useful in text analysis applications as a way to keep track of a large number of models with associated metadata. As an alternative to storing binary data in the table directly, <code>DocTable</code> includes a number of custom column types that can transparently store data into the filesystem and keep track of it using the schema definitions.</p> <p>I provide two file storage column types: (1) <code>TextFileCol</code> for storing text data, and (2) <code>PickleFileCol</code> for storing any python data that requires pickling.</p> <pre><code>import numpy as np\nfrom pathlib import Path\n\nimport sys\nsys.path.append('..')\nimport doctable\n\n# automatically clean up temp folder after python ends\nimport tempfile\ntempdir = tempfile.TemporaryDirectory()\ntmpfolder = tempdir.name\ntmpfolder = Path(tmpfolder)\ntmpfolder\n</code></pre> <pre><code>PosixPath('/tmp/tmpkoe57pma')\n</code></pre> <p>Now I create a new table representing a matrix. Notice that I use the <code>PickleFileCol</code> column shortcut to create the column. This column is equivalent to <code>Col(None, coltype='picklefile', type_args=dict(folder=folder))</code>. See that to SQLite, this column simply looks like a text column.</p> <pre><code>import dataclasses\n@doctable.schema(require_slots=False)\nclass MatrixRow:\n    id: int = doctable.IDCol()\n    array: np.ndarray = doctable.PickleFileCol(f'{tmpfolder}/matrix_pickle_files') # will store files in the tmp directory\n\ndb = doctable.DocTable(target=f'{tmpfolder}/test.db', schema=MatrixRow, new_db=True)\ndb.schema_info()\n</code></pre> <pre><code>[{'name': 'id',\n  'type': INTEGER(),\n  'nullable': False,\n  'default': None,\n  'autoincrement': 'auto',\n  'primary_key': 1},\n {'name': 'array',\n  'type': VARCHAR(),\n  'nullable': True,\n  'default': None,\n  'autoincrement': 'auto',\n  'primary_key': 0}]\n</code></pre> <p>Now we insert a new array. It appears to be inserted the same as any other object. </p> <pre><code>db.insert({'array': np.random.rand(10,10)})\ndb.insert({'array': np.random.rand(10,10)})\nprint(db.count())\ndb.select_df(limit=3)\n</code></pre> <pre><code>2\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead.\n  warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead.\n  warnings.warn('Method .count() is depricated. Please use .q.count() instead.')\n/DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead.\n  warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.')\n/DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: TypeDecorator PickleFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True.  This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions.  Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf)\n  return self._engine.execute(query, *args, **kwargs)\n</code></pre> id array 0 1 [[0.5329174823769329, 0.45399901667272, 0.4110... 1 2 [[0.6837499182333924, 0.40540705856582326, 0.6... <p>But when we actually look at the filesystem, we see that files have been created to store the array.</p> <pre><code>for fpath in tmpfolder.rglob('*.pic'):\n    print(str(fpath))\n</code></pre> <pre><code>/tmp/tmpkoe57pma/matrix_pickle_files/838448859815.pic\n/tmp/tmpkoe57pma/matrix_pickle_files/241946168596.pic\n</code></pre> <p>If we want to see the raw data stored in the table, we can create a new doctable without a defined schema. See that the raw filenames have been stored in the database. Recall that the directory indicating where to find these files was provided in the schema itself. </p> <pre><code>vdb = doctable.DocTable(f'{tmpfolder}/test.db')\nprint(vdb.count())\nvdb.head()\n</code></pre> <pre><code>2\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead.\n  warnings.warn('Method .count() is depricated. Please use .q.count() instead.')\n/DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead.\n  warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.')\n</code></pre> id array 0 1 838448859815.pic 1 2 241946168596.pic"},{"location":"legacy_documentation/doctable_file_column_types/#data-folder-consistency","title":"Data Folder Consistency","text":"<p>Now we try to delete a row from the database. We can see that it was deleted as expected.</p> <pre><code>db.delete(where=db['id']==1)\nprint(db.count())\ndb.head()\n</code></pre> <pre><code>1\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:495: UserWarning: Method .delete() is depricated. Please use .q.delete() instead.\n  warnings.warn('Method .delete() is depricated. Please use .q.delete() instead.')\n/DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead.\n  warnings.warn('Method .count() is depricated. Please use .q.count() instead.')\n/DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead.\n  warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.')\n/DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: TypeDecorator PickleFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True.  This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions.  Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf)\n  return self._engine.execute(query, *args, **kwargs)\n</code></pre> id array 0 2 [[0.6837499182333924, 0.40540705856582326, 0.6... <p>However, when we check the folder where the data was stored, we find that the file was, in fact, not deleted. This is the case for technical reasons.</p> <pre><code>for fpath in tmpfolder.rglob('*.pic'):\n    print(str(fpath))\n</code></pre> <pre><code>/tmp/tmpkoe57pma/matrix_pickle_files/838448859815.pic\n/tmp/tmpkoe57pma/matrix_pickle_files/241946168596.pic\n</code></pre> <p>We can clean up the unused files using <code>clean_col_files()</code> though. Note that the specific column to clean must be provided.</p> <pre><code>db.clean_col_files('array')\nfor fpath in tmpfolder.rglob('*.pic'):\n    print(str(fpath))\n</code></pre> <pre><code>/tmp/tmpkoe57pma/matrix_pickle_files/241946168596.pic\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:444: UserWarning: Method .select() is depricated. Please use .q.select() instead.\n  warnings.warn('Method .select() is depricated. Please use .q.select() instead.')\n/DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: TypeDecorator PickleFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True.  This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions.  Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf)\n  return self._engine.execute(query, *args, **kwargs)\n</code></pre> <p>There may be a situation where doctable cannot find the folder associated with an existing row. We can also use <code>clean_col_files()</code> to check for missing data. This might most frequently occur when the wrong folder is specified in the schema after moving the data file folder. For example, we delete all the pickle files in the directory and then run <code>clean_col_files()</code>.</p> <pre><code>[fp.unlink() for fp in tmpfolder.rglob('*.pic')]\nfor fpath in tmpfolder.rglob('*.pic'):\n    print(str(fpath))\n</code></pre> <pre><code># see that the exception was raised\ntry:\n    db.clean_col_files('array')\nexcept FileNotFoundError as e:\n    print(e)\n</code></pre> <pre><code>These files were not found while cleaning: {'/tmp/tmpkoe57pma/matrix_pickle_files/241946168596.pic'}\n</code></pre>"},{"location":"legacy_documentation/doctable_file_column_types/#text-file-types","title":"Text File Types","text":"<p>We can also store text files in a similar way. For this, use <code>TextFileCol</code> in the folder specification.</p> <pre><code>@doctable.schema(require_slots=False)\nclass TextFileRow:\n    id: int = doctable.IDCol()\n    text: str = doctable.TextFileCol(f'{tmpfolder}/my_text_files') # will store files in the tmp directory\n\ntdb = doctable.DocTable(target=f'{tmpfolder}/test_textfiles.db', schema=TextFileRow, new_db=True)\ntdb.insert({'text': 'Hello world. DocTable is the most useful python package of all time.'})\ntdb.insert({'text': 'Star Wars is my favorite movie.'})\ntdb.head()\n</code></pre> <pre><code>/DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead.\n  warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead.\n  warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.')\n/DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: TypeDecorator TextFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True.  This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions.  Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf)\n  return self._engine.execute(query, *args, **kwargs)\n</code></pre> id text 0 1 Hello world. DocTable is the most useful pytho... 1 2 Star Wars is my favorite movie. <pre><code># and they look like text files\nvdb = doctable.DocTable(f'{tmpfolder}/test_textfiles.db')\nprint(vdb.count())\nvdb.head()\n</code></pre> <pre><code>2\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead.\n  warnings.warn('Method .count() is depricated. Please use .q.count() instead.')\n/DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead.\n  warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.')\n</code></pre> id text 0 1 509620359442.txt 1 2 409663648614.txt <p>See that the text files were created, and they look like normal text files so we can read them normally.</p> <pre><code>for fpath in tmpfolder.rglob('*.txt'):\n    print(f\"{fpath}: {fpath.read_text()}\")\n</code></pre> <pre><code>/tmp/tmpkoe57pma/my_text_files/409663648614.txt: Star Wars is my favorite movie.\n/tmp/tmpkoe57pma/my_text_files/509620359442.txt: Hello world. DocTable is the most useful python package of all time.\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"legacy_documentation/doctable_insert_delete/","title":"DocTable Examples: Insert and Delete","text":"<p>Here we show basics of inserting and deleting data into a doctable.</p> <pre><code>import random\nimport pandas as pd\nimport numpy as np\n\nimport sys\nsys.path.append('..')\nimport doctable\n</code></pre> <pre><code>import dataclasses\n@doctable.schema\nclass Record:\n    __slots__ = []\n    id: int = doctable.IDCol()\n    name: str = doctable.Col(nullable=False)\n    age: int = None\n    is_old: bool = None\n</code></pre> <pre><code>def make_rows(N=3):\n    rows = list()\n    for i in range(N):\n        age = random.random() # number in [0,1]\n        is_old = age &gt; 0.5\n        yield {'name':'user_'+str(i), 'age':age, 'is_old':is_old}\n    return rows\n</code></pre>"},{"location":"legacy_documentation/doctable_insert_delete/#basic-inserts","title":"Basic Inserts","text":"<p>There are only two ways to insert: one at a time (pass single dict), or multiple at a time (pass sequence of dicts).</p> <pre><code>table = doctable.DocTable(target=':memory:', schema=Record, verbose=True)\nfor row in make_rows():\n    table.insert(row)\ntable.select_df()\n</code></pre> <pre><code>DocTable: INSERT OR FAIL INTO _documents_ (name, age, is_old) VALUES (?, ?, ?)\nDocTable: INSERT OR FAIL INTO _documents_ (name, age, is_old) VALUES (?, ?, ?)\nDocTable: INSERT OR FAIL INTO _documents_ (name, age, is_old) VALUES (?, ?, ?)\nDocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_\n</code></pre> id name age is_old 0 1 user_0 0.485860 False 1 2 user_1 0.661900 True 2 3 user_2 0.082627 False <pre><code>newrows = list(make_rows())\ntable.insert(newrows)\ntable.select_df(verbose=False)\n</code></pre> id name age is_old 0 1 user_0 0.485860 False 1 2 user_1 0.661900 True 2 3 user_2 0.082627 False 3 4 user_0 0.936185 True 4 5 user_1 0.082005 False 5 6 user_2 0.567260 True"},{"location":"legacy_documentation/doctable_insert_delete/#deletes","title":"Deletes","text":"<pre><code># delete all entries where is_old is false\ntable.delete(where=~table['is_old'])\ntable.select_df(verbose=False)\n</code></pre> <pre><code>DocTable: DELETE FROM _documents_ WHERE _documents_.is_old = 0\n</code></pre> id name age is_old 0 2 user_1 0.661900 True 1 4 user_0 0.936185 True 2 6 user_2 0.567260 True <pre><code># use vacuum to free unused space now\ntable.delete(where=~table['is_old'], vacuum=True)\ntable.select_df(verbose=False)\n</code></pre> <pre><code>DocTable: DELETE FROM _documents_ WHERE _documents_.is_old = 0\nDocTable: VACUUM\n</code></pre> id name age is_old 0 2 user_1 0.661900 True 1 4 user_0 0.936185 True 2 6 user_2 0.567260 True <pre><code># delete everything\ntable.delete()\ntable.count()\n</code></pre> <pre><code>DocTable: DELETE FROM _documents_\nDocTable: SELECT count() AS count_1 \nFROM _documents_\n LIMIT ? OFFSET ?\n\n\n\n\n\n0\n</code></pre>"},{"location":"legacy_documentation/doctable_multitable/","title":"Example: Multiple Tables","text":"<p>In this example, I show how doctable can be used with multiple relational tables to perform queries which automatically merge different aspects of your dataset when you use <code>.select()</code>. By integrating these relations into the schema, your database can automatically maintain consistency between tables by deleting irrelevant elements when their relations disappear. There are two important features of any multi-table schema using doctable:</p> <p>(1) Set the foreign_keys=True in the DocTable or ConnectEngine constructor. It is enabled by default. Otherwise sqlalchemy will not enable.</p> <p>(2) Use the \"foreignkey\" column type to set the constraint, probably with the onupdate and ondelete keywords specifiied.</p> <p>I will show two examples here: many-to-many relations, and many-to-one relations.</p> <pre><code>import datetime\nimport dataclasses\nimport tempfile\nimport sys\nsys.path.append('..')\nimport doctable\ntmp = tempfile.TemporaryDirectory()\n</code></pre>"},{"location":"legacy_documentation/doctable_multitable/#many-to-many-relationships","title":"Many-to-Many Relationships","text":"<p>The premise is that we have an imaginary API where we can get newly released books along with the libraries they are associted with (although they man, in some cases, not have library information). We want to keep track of the set of books with unique titles, and have book information exist on its own (i.e. we can insert book information if it does not have library information). We would also like to keep track of the libraries they belong to. We need this schema to be fast for selection, but it can be slow for insertion.</p> <p>Primary accesses methods:</p> <ul> <li>insert a book</li> <li>query books by year of publication</li> <li>insert a single library and associated books</li> <li>query books associated with libraries in certain zips</li> </ul> <p>In this example, we are going to use two tables with a many-to-many relationships and a table to handle relationships between them (required for a many-to-many relationship):</p> <ul> <li><code>BookTable</code>: keeps title and publication year of each book. Should exist independently of LibraryTable, because we may not want to use LibraryTable at all.</li> <li><code>LibraryTable</code>: keeps name of library, makes it easy to query by Library.</li> <li><code>BookLibraryRelationsTable</code>: keeps track of relationships between BookTable and LibraryTable.</li> </ul> <p>First we define the <code>BookTable</code> table. Because we are primarily interested in books, we will create a separate <code>Book</code> object for working with them.</p> <pre><code>@doctable.schema(frozen=True, eq=True)\nclass Book:\n    __slots__ = []\n    _id: int = doctable.IDCol()\n    isbn: str = doctable.Col(unique=True)\n    title: str = doctable.Col()\n    year: int = doctable.Col()\n    author: str = doctable.Col()\n    date_updated: datetime.datetime = doctable.UpdatedCol()\n\nclass BookTable(doctable.DocTable):\n    _tabname_ = 'books'\n    _schema_ = Book\n    _indices_ = [doctable.Index('isbn_index', 'isbn')]\n\nbook_table = BookTable(target=f'{tmp.name}/1.db', new_db=True)\n</code></pre> <p>We are not planning to work with author data outside of the schema definition, so we include it as part of the table definition.</p> <pre><code>@doctable.schema(frozen=True, eq=True)\nclass Library:\n    __slots__ = []\n    _id: int = doctable.IDCol()\n    name: str = doctable.Col()\n    zip: int = doctable.Col()\n\nclass LibraryTable(doctable.DocTable):\n    _tabname_ = 'libraries'\n    _schema_ = Library    \n    _constraints_ = [doctable.Constraint('unique', 'name', 'zip')]\n\n\nlibrary_table = LibraryTable(engine=book_table.engine)\n</code></pre> <pre><code>class BookLibraryRelationsTable(doctable.DocTable):\n    '''Link between books and libraries.'''\n    _tabname_ = 'book_library_relations'\n\n    @doctable.schema\n    class _schema_:\n        __slots__ = []\n        _id: int = doctable.IDCol()\n        book_isbn: int = doctable.Col(nullable=False)\n        library_id: int = doctable.Col(nullable=False)\n\n    _constraints_ = (\n        doctable.Constraint('foreignkey', ('book_isbn',), ('books.isbn',)),\n        doctable.Constraint('foreignkey', ('library_id',), ('libraries._id',)),\n        doctable.Constraint('unique', 'book_isbn', 'library_id'),\n    )\n\nrelations_table = BookLibraryRelationsTable(engine=book_table.engine)\nrelations_table.list_tables()\n</code></pre> <pre><code>['book_library_relations', 'books', 'libraries']\n</code></pre> <p>Now we create some random books that are not at libraries and add them into our database.</p> <pre><code>newly_published_books = [\n    Book(isbn='A', title='A', year=2020, author='Pierre Bourdieu'),\n    Book(isbn='E', title='E', year=2018, author='Jean-Luc Picard'),\n]\n\nfor book in newly_published_books:\n    print(book)\n</code></pre> <pre><code>Book(isbn='A', title='A', year=2020, author='Pierre Bourdieu')\nBook(isbn='E', title='E', year=2018, author='Jean-Luc Picard')\n</code></pre> <p>Now we insert the list of books that were published. It works as expected.</p> <pre><code>book_table.insert(newly_published_books, ifnotunique='replace')\nbook_table.head()\n</code></pre> _id isbn title year author date_updated 0 1 A A 2020 Pierre Bourdieu 2022-07-26 21:30:30.364805 1 2 E E 2018 Jean-Luc Picard 2022-07-26 21:30:30.364812 <p>And now lets add a bunch of books that are associated with library objects.</p> <pre><code>new_library_books = {\n    Library(name='Library1', zip=12345): [\n        Book(isbn='A', title='A', year=2020, author='Pierre Bourdieu'),\n        Book(isbn='B', title='B', year=2020, author='Pierre Bourdieu'),\n    ],\n    Library(name='Library2', zip=12345): [\n        Book(isbn='A', title='A', year=2020, author='Devin Cornell'),\n        Book(isbn='C', title='C', year=2021, author='Jean-Luc Picard'),\n    ],\n    Library(name='Library3', zip=67890): [\n        Book(isbn='A', title='A', year=2020, author='Pierre Bourdieu'),\n        Book(isbn='B', title='B', year=2020, author='Jean-Luc Picard'),\n        Book(isbn='D', title='D', year=2019, author='Devin Cornell'),\n    ],\n}\n\nfor library, books in new_library_books.items():\n    r = library_table.insert(library, ifnotunique='ignore')\n    book_table.insert(books, ifnotunique='replace')\n    relations_table.insert([{'book_isbn':b.isbn, 'library_id': r.lastrowid} for b in books], ifnotunique='ignore')\n</code></pre> <pre><code>book_table.select_df()\n</code></pre> _id isbn title year author date_updated 0 2 E E 2018 Jean-Luc Picard 2022-07-26 21:30:30.364812 1 6 C C 2021 Jean-Luc Picard 2022-07-26 21:30:30.482867 2 7 A A 2020 Pierre Bourdieu 2022-07-26 21:30:30.494686 3 8 B B 2020 Jean-Luc Picard 2022-07-26 21:30:30.494692 4 9 D D 2019 Devin Cornell 2022-07-26 21:30:30.494694 <pre><code>library_table.select_df()\n</code></pre> _id name zip 0 1 Library1 12345 1 2 Library2 12345 2 3 Library3 67890 <pre><code>relations_table.select_df()\n</code></pre> _id book_isbn library_id 0 1 A 1 1 2 B 1 2 3 A 2 3 4 C 2 4 5 A 3 5 6 B 3 6 7 D 3"},{"location":"legacy_documentation/doctable_multitable/#select-queries-that-join-tables","title":"Select Queries That Join Tables","text":"<p>Similar to sqlalchemy, <code>DocTable</code> joins are doen simply by replacing the where conditional. While not technically nessecary, typically you will be joining tables on foreign key columns because it is much faster.</p> <pre><code>bt, lt, rt = book_table, library_table, relations_table\n</code></pre> <p>For the first example, say we want to get the isbn numbers of books associated with each library in zip code 12345. We implement the join using a simple conditional  equating the associated keys in each table. Our database schema already knows that the foreign keys are in place, so this expression will give us the join we want.</p> <pre><code>lt.select([lt['name'], rt['book_isbn']], where=(lt['_id']==rt['library_id']) &amp; (lt['zip']==12345), as_dataclass=False)\n</code></pre> <pre><code>[('Library1', 'A'), ('Library1', 'B'), ('Library2', 'A'), ('Library2', 'C')]\n</code></pre> <p>Now say we want to characterize each library according to the age distribution of it's books. We use two conditionals for the join: one connecting library table to relations table, and another connecting relations table to books table. We also include the condition to get only libraries associated with the given zip.</p> <pre><code>conditions = (bt['isbn']==rt['book_isbn']) &amp; (rt['library_id']==lt['_id']) &amp; (lt['zip']==12345)\nbt.select([bt['title'], bt['year'], lt['name']], where=conditions, as_dataclass=False)\n</code></pre> <pre><code>[('C', 2021, 'Library2'),\n ('A', 2020, 'Library1'),\n ('A', 2020, 'Library2'),\n ('B', 2020, 'Library1')]\n</code></pre> <p>Alternatively we can use the <code>.join</code> method of doctable (although I recommend just using select statements).</p> <pre><code>jt = lt.join(rt, (lt['zip']==12345) &amp; (lt['_id']==rt['library_id']), isouter=False)\nbt.select(where=bt['isbn']==jt.c['book_library_relations_book_isbn'], as_dataclass=True)\nbt.select([bt['title'], jt.c['book_library_relations_library_id']], where=bt['isbn']==jt.c['book_library_relations_book_isbn'], as_dataclass=False)\nbt.select([bt['title'], jt.c['libraries_name']], where=bt['isbn']==jt.c['book_library_relations_book_isbn'], as_dataclass=False, limit=3)\n</code></pre> <pre><code>[('C', 'Library1'), ('C', 'Library2'), ('C', 'Library3')]\n</code></pre>"},{"location":"legacy_documentation/doctable_multitable/#many-to-one-relationships","title":"Many-to-One Relationships","text":"<p>Now we create an author class and table to demonstrate a many-to-one relationship.</p> <pre><code>@doctable.schema(frozen=True, eq=True)\nclass Author:\n    __slots__ = []\n    #_id: int = doctable.IDCol()\n    name: str = doctable.Col(primary_key=True, unique=True)\n    age: int = doctable.Col()\n\nclass AuthorTable(doctable.DocTable):\n    _tabname_ = 'authors'\n    _schema_ = Author  \n    _constraints_ = [doctable.Constraint('foreignkey', ('name',), ('books.author',))]\n\n#book_table_auth = BookTable(target=f'{tmp.name}/16.db', new_db=True)\n#author_table = AuthorTable(engine=book_table_auth.engine)\nauthor_table = AuthorTable(engine=book_table.engine)\n</code></pre> <pre><code>author_table.delete()\nauthor_table.insert([\n    Author(name='Devin Cornell', age=30),\n    Author(name='Pierre Bourdieu', age=99),\n    Author(name='Jean-Luc Picard', age=1000),\n])\nauthor_table.head()\n</code></pre> name age 0 Devin Cornell 30 1 Pierre Bourdieu 99 2 Jean-Luc Picard 1000 <pre><code>book_table.head()\n</code></pre> _id isbn title year author date_updated 0 2 E E 2018 Jean-Luc Picard 2022-07-26 21:30:30.364812 1 6 C C 2021 Jean-Luc Picard 2022-07-26 21:30:30.482867 2 7 A A 2020 Pierre Bourdieu 2022-07-26 21:30:30.494686 3 8 B B 2020 Jean-Luc Picard 2022-07-26 21:30:30.494692 4 9 D D 2019 Devin Cornell 2022-07-26 21:30:30.494694 <pre><code>columns = [book_table['year'], author_table['age'], author_table['name']]\nwhere = (book_table['author']==author_table['name']) &amp; (book_table['author'] &gt; 30)\nbook_table.select_df(columns, where=where)\n</code></pre> year age name 0 2018 1000 Jean-Luc Picard 1 2021 1000 Jean-Luc Picard 2 2020 99 Pierre Bourdieu 3 2020 1000 Jean-Luc Picard 4 2019 30 Devin Cornell"},{"location":"legacy_documentation/doctable_parsetreedoc_column/","title":"ParseTreeDoc Column Types","text":"<p>Creating parsetrees with spacy can be a computationally expensive task, so we may often want to store them in a database for better use. Because they may be large binary files, we will store them as pickle file column types, but with an additional serialization step.</p> <pre><code>import spacy\nnlp = spacy.load('en_core_web_sm')\nimport numpy as np\nfrom pathlib import Path\n\nimport sys\nsys.path.append('..')\nimport doctable\n\n# automatically clean up temp folder after python ends\nimport tempfile\ntempdir = tempfile.TemporaryDirectory()\ntmpfolder = tempdir.name\ntmpfolder\n\nimport tempfile\nwith tempfile.TemporaryDirectory() as tmp:\n    print(tmp)\n</code></pre> <pre><code>/tmp/tmpnmvhkw9t\n</code></pre> <p>Create some test data and make a new <code>ParseTreeDoc</code> object.</p> <pre><code>texts = [\n    'Help me Obi-Wan Kenobi. You\u2019re my only hope. ',\n    'I find your lack of faith disturbing. ',\n    'Do, or do not. There is no try. '\n]\nparser = doctable.ParsePipeline([nlp, doctable.Comp('get_parsetrees')])\ndocs = parser.parsemany(texts)\nfor doc in docs:\n    print(len(doc))\n</code></pre> <pre><code>2\n1\n2\n</code></pre> <p>Now we create a schema that includes the <code>doc</code> column and the <code>ParseTreeFileCol</code> default value. Notice that using the type hint <code>ParseTreeDoc</code> and giving a generic <code>Col</code> default value is also sufficient.</p> <pre><code>import dataclasses\n@doctable.schema(require_slots=False)\nclass DocRow:\n    id: int = doctable.IDCol()\n    doc: doctable.ParseTreeDoc = doctable.ParseTreeFileCol(f'{tmpfolder}/parsetree_pickle_files')\n\n    # could also use this:\n    #doc: doctable.ParseTreeDoc = doctable.Col(type_args=dict(folder=f'{tmp}/parsetree_pickle_files'))\n\ndb = doctable.DocTable(target=f'{tmpfolder}/test_ptrees.db', schema=DocRow, new_db=True)\ndb.schema_info()\n</code></pre> <pre><code>[{'name': 'id',\n  'type': INTEGER(),\n  'nullable': False,\n  'default': None,\n  'autoincrement': 'auto',\n  'primary_key': 1},\n {'name': 'doc',\n  'type': VARCHAR(),\n  'nullable': True,\n  'default': None,\n  'autoincrement': 'auto',\n  'primary_key': 0}]\n</code></pre> <pre><code>#db.insert([{'doc':doc} for doc in docs])\nfor doc in docs:\n    db.insert({'doc': doc})\ndb.head(3)\n</code></pre> <pre><code>/DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead.\n  warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead.\n  warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.')\n</code></pre> 3 0 3 <pre><code>for idx, doc in db.q.select_raw():\n    print(f\"doc id {idx}:\")\n    for i, sent in enumerate(doc):\n        print(f\"\\tsent {i}: {[t.text for t in sent]}\")\n</code></pre> <pre><code>doc id 1:\n    sent 0: ['Help', 'me', 'Obi', '-', 'Wan', 'Kenobi', '.']\n    sent 1: ['You', '\u2019re', 'my', 'only', 'hope', '.']\ndoc id 2:\n    sent 0: ['I', 'find', 'your', 'lack', 'of', 'faith', 'disturbing', '.']\ndoc id 3:\n    sent 0: ['Do', ',', 'or', 'do', 'not', '.']\n    sent 1: ['There', 'is', 'no', 'try', '.']\n\n\n/DataDrive/code/doctable/examples/../doctable/connectengine.py:70: SAWarning: TypeDecorator ParseTreeDocFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True.  This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions.  Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf)\n  return self._engine.execute(query, *args, **kwargs)\n</code></pre> <p>See that the files exist, and we can remove/clean them just as any other file column type.</p> <pre><code>for fpath in Path(tmpfolder).rglob('*.pic'):\n    print(str(fpath))\n</code></pre> <pre><code>/tmp/tmpvmthfr7t/parsetree_pickle_files/347692105083_parsetreedoc.pic\n/tmp/tmpvmthfr7t/parsetree_pickle_files/98072534351_parsetreedoc.pic\n/tmp/tmpvmthfr7t/parsetree_pickle_files/689952128879_parsetreedoc.pic\n</code></pre> <pre><code>db.delete(db['id']==1)\nfor fpath in Path(tmpfolder).rglob('*.pic'):\n    print(str(fpath))\ndb.head()\n</code></pre> <pre><code>/tmp/tmpvmthfr7t/parsetree_pickle_files/347692105083_parsetreedoc.pic\n/tmp/tmpvmthfr7t/parsetree_pickle_files/98072534351_parsetreedoc.pic\n/tmp/tmpvmthfr7t/parsetree_pickle_files/689952128879_parsetreedoc.pic\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:506: UserWarning: Method .delete() is depricated. Please use .q.delete() instead.\n  warnings.warn('Method .delete() is depricated. Please use .q.delete() instead.')\n/DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead.\n  warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.')\n</code></pre> id doc 0 2 [(I, find, your, lack, of, faith, disturbing, .)] 1 3 [(Do, ,, or, do, not, .), (There, is, no, try,... <pre><code>db.clean_col_files('doc')\nfor fpath in Path(tmpfolder).rglob('*.pic'):\n    print(str(fpath))\n</code></pre> <pre><code>/tmp/tmpvmthfr7t/parsetree_pickle_files/347692105083_parsetreedoc.pic\n/tmp/tmpvmthfr7t/parsetree_pickle_files/98072534351_parsetreedoc.pic\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:449: UserWarning: Method .select() is depricated. Please use .q.select() instead.\n  warnings.warn('Method .select() is depricated. Please use .q.select() instead.')\n/DataDrive/code/doctable/examples/../doctable/connectengine.py:70: SAWarning: TypeDecorator ParseTreeDocFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True.  This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions.  Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf)\n  return self._engine.execute(query, *args, **kwargs)\n</code></pre>"},{"location":"legacy_documentation/doctable_picklefile/","title":"DocTable Example: Pickle and Text Files","text":"<p>Here I show a bit about how to use <code>picklefile</code> and <code>textfile</code> column types. DocTable transparently handles saving and reading column data as separate files when data is large to improve performance of select queries. It will automatically create a folder in the same directory as your sqlite database and save or read file data as if you were working with a regular table entry.</p> <pre><code>import os\nimport sys\nsys.path.append('..')\nimport doctable\n</code></pre> <pre><code>#tmp = doctable.TempFolder('./tmp') # will delete folder upon destruction\nimport tempfile\nimport pathlib\nfkasdfjlaj = tempfile.TemporaryDirectory()\ntmp = fkasdfjlaj.name\n\n# create column schema: each row corresponds to a pickle\nimport dataclasses\n@doctable.schema(require_slots=False)\nclass FileEntry:\n    pic: list = doctable.Col(column_type='picklefile', type_kwargs=dict(folder=tmp))\n    idx: int = doctable.IDCol()\n\ndb = doctable.DocTable(schema=FileEntry, target=':memory:')\n</code></pre> <p>First we try inserting a basic object, where the data will be stored in a pickle file. We can see from the <code>select</code> statement that the data read/write is handled transparently by doctable.</p> <pre><code>a = [1, 2, 3, 4, 5]\ndb.insert(FileEntry(a))\ndb.select() # regular select using the picklefile datatype\n</code></pre> <pre><code>BINDING MF PARAMSSSSSSS\nPROCESSING MF PARAMSSSSSSS\n\n\n\n\n\n[FileEntry(pic=[1, 2, 3, 4, 5], idx=1)]\n</code></pre> <p>We can also try turning off the transparent conversion, and instead retrieve the regular directory.</p> <pre><code>with db['pic'].type.control:\n    r = db.select()\nr\n</code></pre> <pre><code>PROCESSING MF PARAMSSSSSSS\n\n\n\n\n\n[FileEntry(pic=f'{tmp}/564814847383.pic', idx=1)]\n</code></pre> <p>For performance reasons, DocTable never deletes stored file data unless you call the <code>.clean_col_files()</code> method directly. It will raise an exception if a referenced file is missing, and delete all files which are not referenced in the table. This is a costly function call, but a good way to make sure your database is 1-1 matched with your filesystem.</p> <pre><code># deletes files not in db and raise error if some db files not in filesystem\ndb.clean_col_files('pic')\n</code></pre> <pre><code>PROCESSING MF PARAMSSSSSSS\n</code></pre> <p>Now I create another DocTable with a changed <code>fpath</code> argument. Because the argument changed, DocTable will raise an exception when selecting or calling <code>.clean_col_files()</code>. Be wary of this!</p>"},{"location":"legacy_documentation/doctable_schema/","title":"DocTable Schemas","text":"<p>Your database table column names and types come from a schema class defined using the <code>@doctable.schema</code> decorator. In addition to providing a schema definition, this class can be used to encapsulate data when inserting or retrieving from the database. </p> <p>At its most basic, your schema class operates like a dataclass that uses slots for efficiency and allows for custom methods that will not affect the database schema.</p> <pre><code>from datetime import datetime\nfrom pprint import pprint\nimport pandas as pd\n\nimport sys\nsys.path.append('..')\nimport doctable\n</code></pre>"},{"location":"legacy_documentation/doctable_schema/#introduction","title":"Introduction","text":"<p>This is an example of a basic doctable schema. Note the use of the decorator <code>@doctable.schema</code>, the inclusion of <code>__slots__ = []</code>, and the type hints of the member variables - I will explain each of these later in this document.</p> <p>This class represents a database schema that includes two columns: <code>name</code> (an <code>int</code>) and <code>age</code> (a <code>str</code>).</p> <pre><code>@doctable.schema\nclass Record:\n    __slots__ = []\n    name: str\n    age: int\n</code></pre> <p>The schema class definition is then provided to the doctable constructor to create the database table. Here we create an in-memory sqlite table and show the schema resulting from our custom class. Note that doctable automatically inferred that <code>name</code> should be a <code>VARCHAR</code> and <code>age</code> should be an <code>INTEGER</code> based on the provided type hints.</p> <pre><code># the schema that would result from this dataclass:\ntable = doctable.DocTable(target=':memory:', schema=Record)\ntable.schema_table()\n</code></pre> name type nullable default autoincrement primary_key 0 name VARCHAR True None auto 0 1 age INTEGER True None auto 0 <p>We can also use the schema class to insert data into our <code>DocTable</code>. We simply create a new <code>Record</code> and pass it to the <code>DocTable.insert()</code> method. Using <code>.head()</code>, we see the contents of the database so far. Note that you may also pass a dictionary to insert data - this is just one way of inserting data.</p> <pre><code>new_record = Record(name='Devin Cornell', age=30)\nprint(new_record)\ntable.insert(new_record)\ntable.head()\n</code></pre> <pre><code>Record(name='Devin Cornell', age=30)\n</code></pre> name age 0 Devin Cornell 30 <p>And perhaps more usefully, we can use it to encapsulate results from <code>.select()</code> queries. Note that the returned object is exactly the same as the one we put in. Slot classes are more memory-efficient than dictionaries for storing data, but there is cpu time overhead from inserting that data into the slots.</p> <pre><code>first_record = table.select_first()\nprint(first_record)\n</code></pre> <pre><code>Record(name='Devin Cornell', age=30)\n</code></pre> <p>But, of course, the data can be returned in its raw format by passing the parameter <code>as_dataclass=False</code>.</p> <pre><code>first_record = table.select_first(as_dataclass=False)\nprint(first_record)\n</code></pre> <pre><code>('Devin Cornell', 30)\n</code></pre>"},{"location":"legacy_documentation/doctable_schema/#the-doctableschema-decorator","title":"The <code>doctable.schema</code> Decorator","text":"<p>The <code>@doctable.schema</code> decorator does the work to convert your custom class into a schema class. It transforms your schema class in three ways:</p> <ol> <li> <p>create slots: First, slot variable names will be added to <code>__slots__</code> automatically based on the fields in your class definition. This is why the default functionality requires you to add <code>__slots__ = []</code> with no variable names. You may also turn slots off by passing <code>require_slots=False</code> to the decorator (i.e. <code>@doctable.schema(require_slots=False)</code>), otherwise an exception will be raised.</p> </li> <li> <p>convert to dataclass: Second, your schema class will be converted to a dataclass that generates <code>__init__</code>, <code>__repr__</code>, and other boilerplate methods meant for classes that primarily store data. Any keyword arguments passed to the <code>schema</code> decorator, with the exception of <code>require_slots</code>, will be passed directly to the <code>@dataclasses.dataclass</code> decorator so you have control over the dataclass definition.</p> </li> <li> <p>inherit from <code>DocTableSchema</code>: Lastly, your schema class will inherit from <code>doctable.DocTableSchema</code>, which provides additional accessors that are used for storage in a <code>DocTable</code> and fine-grained control over retreived data. More on this later.</p> </li> </ol> <p>Column names and types will be inferred from the type hints in your schema class definition. Because <code>DocTable</code> is built on sqlalchemy core, all fields will eventually be converted to <code>sqlalchemy</code> column objects and added to the DocTable metadata. This table shows the type mappings implemented in doctable:</p> <pre><code>doctable.python_to_slqlchemy_type\n</code></pre> <pre><code>{int: sqlalchemy.sql.sqltypes.Integer,\n float: sqlalchemy.sql.sqltypes.Float,\n str: sqlalchemy.sql.sqltypes.String,\n bool: sqlalchemy.sql.sqltypes.Boolean,\n datetime.datetime: sqlalchemy.sql.sqltypes.DateTime,\n datetime.time: sqlalchemy.sql.sqltypes.Time,\n datetime.date: sqlalchemy.sql.sqltypes.Date,\n doctable.textmodels.parsetreedoc.ParseTreeDoc: doctable.schemas.custom_coltypes.ParseTreeDocFileType}\n</code></pre> <p>For example, see this example of the most basic possible schema class that can be used to create a doctable. We use static defaulted parameters and type hints including <code>str</code>, <code>int</code>, <code>datetime</code>, and <code>Any</code>, which you can see are converted to <code>VARCHAR</code>, <code>INTEGER</code>, <code>DATETIME</code>, and <code>BLOB</code> column types, respectively. <code>BLOB</code> was used because the provided type hint <code>Any</code> has no entry in the above table.</p> <pre><code>from typing import Any\nimport datetime\n\n@doctable.schema\nclass Record:\n    __slots__ = []\n    name: str = None\n    age: int = None\n    time: datetime.datetime = None\n    friends: Any = None\n\n# the schema that would result from this dataclass:\ndoctable.DocTable(target=':memory:', schema=Record).schema_table()\n</code></pre> name type nullable default autoincrement primary_key 0 name VARCHAR True None auto 0 1 age INTEGER True None auto 0 2 time DATETIME True None auto 0 3 friends BLOB True None auto 0 <p>You can see that this class operates much like a regular dataclass with slots. Thus, these defaulted parameters are applied in the constructor of the schema class, and NOT as the default value in the database schema.</p> <pre><code>Record('Devin Cornell', 30)\n</code></pre> <pre><code>Record(name='Devin Cornell', age=30, time=None, friends=None)\n</code></pre>"},{"location":"legacy_documentation/doctable_schema/#use-doctablecol-for-more-control-over-schema-creation","title":"Use <code>doctable.Col</code> For More Control Over Schema Creation","text":"<p>Using <code>doctable.Col()</code> as a default value in the schema class definition can give you more control over schema definitions. </p> <p>Firstly, this function returns a dataclass <code>field</code> object that can be used to set parameters like <code>default_factory</code> or <code>compare</code> as used by the dataclass. Pass arguments meant for <code>field</code> through the <code>Col</code> parameter <code>field_kwargs=dict(..)</code>. Other data passed to <code>Col</code> will be used to create the <code>DocTable</code> schema, which is stored as metadata inside the <code>field</code>.</p> <p>This example shows how <code>Col</code> can be used to set some parameters meant for <code>field</code>. These will affect your schema class behavior without affecting the produced DocTable schema.</p> <pre><code>@doctable.schema\nclass Record:\n    __slots__ = []\n    name: str = doctable.Col()\n    age: int = doctable.Col(field_kwargs=dict(default_factory=list, compare=True))\n\nRecord()\n</code></pre> <pre><code>Record(age=[])\n</code></pre> <p><code>Col</code> also allows you to explicitly specify a column type using a string, sqlalchemy type definition, or sqlalchemy instance passed to <code>column_type</code>. You can then pass arguments meant for the sqlalchemy type constructor through <code>type_kwargs</code>. You may also use <code>type_kwargs</code> with the column type inferred from the type hint.</p> <pre><code>import sqlalchemy\n\n@doctable.schema\nclass Record:\n    __slots__ = []\n\n    # providing only the type as first argument\n    age: int = doctable.Col(sqlalchemy.BigInteger)\n\n    # these are all quivalent\n    name1: str = doctable.Col(type_kwargs=dict(length=100)) # infers type from type hint\n    name2: str = doctable.Col(sqlalchemy.String, type_kwargs=dict(length=100)) # accepts provided type sqlalchemy.String, pass parameters through type_kwargs\n    name3: str = doctable.Col(sqlalchemy.String(length=100)) # accepts type instance (no need for type_kwargs this way)\n    name4: str = doctable.Col('string', type_kwargs=dict(length=100))\n\n\n# the schema that would result from this dataclass:\ndoctable.DocTable(target=':memory:', schema=Record).schema_table()\n</code></pre> name type nullable default autoincrement primary_key 0 age BIGINT True None auto 0 1 name1 VARCHAR(100) True None auto 0 2 name2 VARCHAR(100) True None auto 0 3 name3 VARCHAR(100) True None auto 0 4 name4 VARCHAR(100) True None auto 0 <p>A full list of string -&gt; sqlalchemy type mappings is shown below:</p> <pre><code>doctable.string_to_sqlalchemy_type\n</code></pre> <pre><code>{'biginteger': sqlalchemy.sql.sqltypes.BigInteger,\n 'boolean': sqlalchemy.sql.sqltypes.Boolean,\n 'date': sqlalchemy.sql.sqltypes.Date,\n 'datetime': sqlalchemy.sql.sqltypes.DateTime,\n 'enum': sqlalchemy.sql.sqltypes.Enum,\n 'float': sqlalchemy.sql.sqltypes.Float,\n 'integer': sqlalchemy.sql.sqltypes.Integer,\n 'interval': sqlalchemy.sql.sqltypes.Interval,\n 'largebinary': sqlalchemy.sql.sqltypes.LargeBinary,\n 'numeric': sqlalchemy.sql.sqltypes.Numeric,\n 'smallinteger': sqlalchemy.sql.sqltypes.SmallInteger,\n 'string': sqlalchemy.sql.sqltypes.String,\n 'text': sqlalchemy.sql.sqltypes.Text,\n 'time': sqlalchemy.sql.sqltypes.Time,\n 'unicode': sqlalchemy.sql.sqltypes.Unicode,\n 'unicodetext': sqlalchemy.sql.sqltypes.UnicodeText,\n 'json': doctable.schemas.custom_coltypes.JSONType,\n 'pickle': doctable.schemas.custom_coltypes.CpickleType,\n 'parsetree': doctable.schemas.custom_coltypes.ParseTreeDocFileType,\n 'picklefile': doctable.schemas.custom_coltypes.PickleFileType,\n 'textfile': doctable.schemas.custom_coltypes.TextFileType}\n</code></pre> <p>Finally, <code>Col</code> allows you to pass keyword arguments directly to the sqlalchemy <code>Column</code> constructor. This includes flags like <code>primary_key</code> or <code>default</code>, which are both used to construct the database schema but do not affect the python dataclass. Note that I recreated the classic <code>id</code> column below.</p> <pre><code>@doctable.schema\nclass Record:\n    __slots__ = []\n    id: int = doctable.Col(primary_key=True, autoincrement=True)\n    age: int = doctable.Col(nullable=False)\n    name: str = doctable.Col(default='MISSING_NAME')\n\n# the schema that would result from this dataclass:\ndoctable.DocTable(target=':memory:', schema=Record).schema_table()\n</code></pre> name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 age INTEGER False None auto 0 2 name VARCHAR True None auto 0 <p>I also included some shortcut <code>Col</code> functions like <code>IDCol</code>, <code>AddedCol</code>, and <code>UpdatedCol</code> - see below.</p> <pre><code>import datetime\n\n@doctable.schema\nclass Record:\n    __slots__ = []\n    id: int = doctable.IDCol() # auto-increment primary key\n    added: datetime.datetime = doctable.AddedCol() # record when row was added\n    updated: datetime.datetime = doctable.UpdatedCol() # record when row was updated\n\ndoctable.DocTable(target=':memory:', schema=Record).schema_table()\n</code></pre> name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 added DATETIME True None auto 0 2 updated DATETIME True None auto 0 <p>In this way, <code>Col</code> allows you to give fine-grained control to both the schema class behavior and the sql schema definition.</p>"},{"location":"legacy_documentation/doctable_schema/#working-with-schema-objects","title":"Working With Schema Objects","text":"<p>Using <code>Col</code> default parameters also has some additional side effects, primarily due to the inherited class <code>DocTableSchema</code>. Among other things, the <code>Col</code> method defines the default dataclass value to be a <code>doctable.EmptyValue()</code> object, which is essentially a placeholder for data that was not inserted into the class upon construction. The <code>__repr__</code> defined in <code>DocTableSchema</code> dictates that member objects containing this value not appear when printing the class, and furthermore, member variables with the value <code>EmptyValue()</code> will not be provided in the database insertion. This means that the database schema is allowed to use its own default value - an effect which is most obviously useful when inserting an object that does not have an <code>id</code> or other automatically provided values.</p> <p>The example below shows the <code>new_record.id</code> contains <code>EmptyValue()</code> as a default, and that the <code>id</code> column is not included in the insert query - only <code>name</code>.</p> <pre><code>@doctable.schema\nclass Record:\n    __slots__ = []\n    id: int = doctable.IDCol()\n    name: str = doctable.Col()\n\nnew_record = Record(name='Devin Cornell')\nprint(new_record)\ntry:\n    print(new_record.id)\nexcept doctable.DataNotAvailableError:\n    print(f'exception was raised')\n\ntable = doctable.DocTable(target=':memory:', schema=Record, verbose=True)\ntable.insert(new_record)\ntable.head()\n</code></pre> <pre><code>Record(name='Devin Cornell')\nexception was raised\nDocTable: INSERT OR FAIL INTO _documents_ (name) VALUES (?)\nDocTable: SELECT _documents_.id, _documents_.name \nFROM _documents_\n LIMIT ? OFFSET ?\n</code></pre> id name 0 1 Devin Cornell <p>Yet when we go to retrieve the inserted data, we can see that the value has been replaced by the defaulted value in the database. This is a useful feature if your pipeline involves the insertion of schema objects directly (as opposed to inserting dictionaries for each row).</p> <pre><code>table.select_first(verbose=False)\n</code></pre> <pre><code>Record(id=1, name='Devin Cornell')\n</code></pre> <p>The <code>EmptyValue()</code> feature is also useful when issuing select queries involving only a subset of columns. See here we run a select query where we just retrieve the name data, yet the result is still stored in a <code>Record</code> object.</p> <pre><code>returned_record = table.select_first(['name'], verbose=False)\nprint(returned_record)\n</code></pre> <pre><code>Record(name='Devin Cornell')\n</code></pre> <p>To avoid working with <code>EmptyValue()</code> objects directly, it is recommended that you use the <code>__getitem__</code> string subscripting to access column data. When using this subscript, the schema object will raise an exception if the returned value is an <code>EmptyValue()</code>.</p> <pre><code>try:\n    returned_record.id\nexcept doctable.DataNotAvailableError as e:\n    print(e)\n</code></pre> <pre><code>The \"id\" property is not available. This might happen if you did not retrieve the information from a database or if you did not provide a value in the class constructor.\n</code></pre>"},{"location":"legacy_documentation/doctable_schema/#indices-and-constraints","title":"Indices and Constraints","text":"<p>Indices and constraints are provided to the <code>DocTable</code> constructor or definition, as it is not part of the schema class. Here I create custom schema and table definitions where the table has some defined indices and constraints. <code>doctable.Index</code> is really just a direct reference to <code>sqlalchemy.Index</code>, and <code>doctable.Constraint</code> is a mapping to an sqlalchemy constraint type, with the first argument indicating which one.</p> <pre><code>@doctable.schema\nclass Record:\n    __slots__ = []\n    id: int = doctable.IDCol()\n    name: str = doctable.Col()\n    age: int = doctable.Col()\n\nclass RecordTable(doctable.DocTable):\n    _tabname_ = 'records'\n    _schema_ = Record\n\n    # table indices\n    _indices_ = (\n        doctable.Index('name_index', 'name'),\n        doctable.Index('name_age_index', 'name', 'age', unique=True),\n    )\n\n    # table constraints\n    _constraints_ = (\n        doctable.Constraint('unique', 'name', 'age', name='name_age_constraint'),\n        doctable.Constraint('check', 'age &gt; 0', name='check_age'),\n    )\n\ntable = RecordTable(target=':memory:')\n</code></pre> <p>And we can see that the constraints are working when we try to insert a record where age is less than 1.</p> <pre><code>try:\n    table.insert(Record(age=-1))\nexcept sqlalchemy.exc.IntegrityError as e:\n    print(e)\n</code></pre> <pre><code>(sqlite3.IntegrityError) CHECK constraint failed: check_age\n[SQL: INSERT OR FAIL INTO records (age) VALUES (?)]\n[parameters: (-1,)]\n(Background on this error at: http://sqlalche.me/e/13/gkpj)\n</code></pre> <p>This is a full list of the mappings between constraint names and the associated sqlalchemy objects.</p> <pre><code>doctable.constraint_lookup\n</code></pre> <pre><code>{'check': sqlalchemy.sql.schema.CheckConstraint,\n 'unique': sqlalchemy.sql.schema.UniqueConstraint,\n 'primarykey': sqlalchemy.sql.schema.PrimaryKeyConstraint,\n 'foreignkey': sqlalchemy.sql.schema.ForeignKeyConstraint}\n</code></pre>"},{"location":"legacy_documentation/doctable_schema/#conclusions","title":"Conclusions","text":"<p>In this guide, I tried to show some exmaples and give explanations for the ways that schema classes can be used to create doctables. The design is fairly efficent and flexible, and brings a more object-focused approach compared to raw sql queries without the overhead of ORM.</p>"},{"location":"legacy_documentation/doctable_schema_dataclass/","title":"DocTable Example: Schemas","text":"<p>In this example, we show column specifications for each available type, as well as the sqlalchemy equivalents on which they were based. Note that .</p> <p>Each column in the schema passed to doctable is a 2+ tuple containing, in order, the column type, name, and arguments, and optionally the sqlalchemy type arguemnts.</p> <pre><code>from datetime import datetime\nfrom pprint import pprint\nimport pandas as pd\nimport typing\n\nimport sys\nsys.path.append('..')\nimport doctable\n</code></pre> <pre><code>@doctable.schema\nclass MyClass:\n    __slots__ = []\n    # builtin column types\n    idx: int = doctable.IDCol()\n\n    # unique name\n    name: str = doctable.Col(unique=True) # want to be the first ordered argument\n\n\n    # special columns for added and updated\n    updated: datetime = doctable.UpdatedCol()\n    added: datetime = doctable.AddedCol()\n\n    # custom column types \n    lon: float = doctable.Col()\n    lat: float = doctable.Col()\n\n    # use Col to use factory to construct emtpy list\n    # will be stored as binary/pickle type, since no other available\n    elements: doctable.JSONType = doctable.Col(field_kwargs=dict(default_factory=list))\n\nclass MyTable(doctable.DocTable):\n    _schema_ = MyClass\n    # indices and constraints\n    _indices = (\n        doctable.Index('lonlat_index', 'lon', 'lat', unique=True),\n        doctable.Index('name_index', 'name'),\n    )\n    _constraints_ = (\n        doctable.Constraint('check', 'lon &gt; 0', name='check_lon'),\n        doctable.Constraint('check', 'lat &gt; 0'),\n    )\n\n\nmd = MyTable(target=':memory:', verbose=True)\n#pprint(md.schemainfo)\nmd.schema_table()\n</code></pre> name type nullable default autoincrement primary_key 0 idx INTEGER False None auto 1 1 name VARCHAR True None auto 0 2 updated DATETIME True None auto 0 3 added DATETIME True None auto 0 4 lon FLOAT True None auto 0 5 lat FLOAT True None auto 0 6 elements VARCHAR True None auto 0 <pre><code>\n</code></pre>"},{"location":"legacy_documentation/doctable_schema_legacy/","title":"DocTable Schemas","text":"<p>There are two ways to define schemas for a DocTable:</p> <ol> <li> <p>dataclass schema: column names and types come from a class created using the <code>@doctable.schema</code> decorator. This class represents a single row, and is returned by default when a select query is executed. <code>doctable</code> provides a thin layer over dataclasses with slots to reduce memory overhead from returned results. Custom methods can also be defined on the class that will not affect the database schema. When using this method, constraints and indices must be provided at the time of <code>DocTable</code> instantiation (or in the definition of an inheriting <code>DocTable</code>).</p> </li> <li> <p>list schema: column names and types come from sequences of strings according to a custom doctable format. This method requires less knowledge of doctable objects but otherwise has no advantages over dataclass schemas.</p> </li> </ol> <p>The doctable package builds on sqlalchemy, so both types of schema specifications ultimately result in a sequence of <code>sqlalchemy</code> column types that will be used to construct (or interface with) the database table.</p> <pre><code>from datetime import datetime\nfrom pprint import pprint\nimport pandas as pd\n\nimport sys\nsys.path.append('..')\nimport doctable\n</code></pre>"},{"location":"legacy_documentation/doctable_schema_legacy/#schema-type-mappings","title":"Schema Type Mappings","text":"<p>There are two lookup tables used to relate to sqlalchemy column types. The first is a map from Python datatypes to the sqlalchemy types. This is sufficient for the simplest possible dataclass schema specification.</p> <p>The second is a string lookup that is provided for the list schema format. You can see that this offers a larger number of types compared to the Python type conversion.</p> <p>There are several other custom column types I included for convenience.</p> <pre><code>doctable.string_to_sqlalchemy_type\n</code></pre> <pre><code>{'biginteger': sqlalchemy.sql.sqltypes.BigInteger,\n 'boolean': sqlalchemy.sql.sqltypes.Boolean,\n 'date': sqlalchemy.sql.sqltypes.Date,\n 'datetime': sqlalchemy.sql.sqltypes.DateTime,\n 'enum': sqlalchemy.sql.sqltypes.Enum,\n 'float': sqlalchemy.sql.sqltypes.Float,\n 'integer': sqlalchemy.sql.sqltypes.Integer,\n 'interval': sqlalchemy.sql.sqltypes.Interval,\n 'largebinary': sqlalchemy.sql.sqltypes.LargeBinary,\n 'numeric': sqlalchemy.sql.sqltypes.Numeric,\n 'smallinteger': sqlalchemy.sql.sqltypes.SmallInteger,\n 'string': sqlalchemy.sql.sqltypes.String,\n 'text': sqlalchemy.sql.sqltypes.Text,\n 'time': sqlalchemy.sql.sqltypes.Time,\n 'unicode': sqlalchemy.sql.sqltypes.Unicode,\n 'unicodetext': sqlalchemy.sql.sqltypes.UnicodeText,\n 'json': doctable.schemas.custom_coltypes.JSONType,\n 'pickle': doctable.schemas.custom_coltypes.CpickleType,\n 'parsetree': doctable.schemas.custom_coltypes.ParseTreeDocFileType,\n 'picklefile': doctable.schemas.custom_coltypes.PickleFileType,\n 'textfile': doctable.schemas.custom_coltypes.TextFileType}\n</code></pre>"},{"location":"legacy_documentation/doctable_schema_legacy/#list-schemas","title":"List Schemas","text":"<p>And this is another example showing the list schema format.</p> <pre><code>schema = (\n    # standard id column\n    #SQLAlchemy: Column('id', Integer, primary_key = True, autoincrement=True), \n    ('integer', 'id', dict(primary_key=True, autoincrement=True)),\n    # short form (can't provide any additional args though): ('idcol', 'id')\n\n    # make a category column with two options: \"FICTION\" and \"NONFICTION\"\n    #SQLAlchemy: Column('title', String,)\n    ('string', 'category', dict(nullable=False)),\n\n    # make a non-null title column\n    #SQLAlchemy: Column('title', String,)\n    ('string', 'title', dict(nullable=False)),\n\n    # make an abstract where the default is an empty string instead of null\n    #SQLAlchemy: Column('abstract', String, default='')\n    ('string', 'abstract',dict(default='')),\n\n    # make an age column where age must be greater than zero\n    #SQLAlchemy: Column('abstract', Integer)\n    ('integer', 'age'),\n\n    # make a column that keeps track of column updates\n    #SQLAlchemy: Column('updated_on', DateTime(), default=datetime.now, onupdate=datetime.now)\n    ('datetime', 'updated_on',  dict(default=datetime.now, onupdate=datetime.now)),\n    # short form to auto-record update date: ('date_updated', 'updated_on')\n\n    #SQLAlchemy: Column('updated_on', DateTime(), default=datetime.now)\n    ('datetime', 'updated_on',  dict(default=datetime.now)),\n    # short form to auto-record insertion date: ('date_added', 'added_on')\n\n    # make a string column with max of 500 characters\n    #SQLAlchemy: Column('abstract', String, default='')\n    ('string', 'text',dict(),dict(length=500)),\n\n\n    ##### Custom DocTable Column Types #####\n\n    # uses json.dump to convert python object to json when storing and\n    # json.load to convert json back to python when querying\n    ('json','json_data'),\n\n    # stores pickled python object directly in table as BLOB\n    # TokensType and ParagraphsType are defined in doctable/coltypes.py\n    # SQLAlchemy: Column('tokenized', TokensType), Column('sentencized', ParagraphsType)\n    ('pickle','tokenized'),\n\n    # store pickled data into a separate file, recording only filename directly in table\n    # the 'fpath' argument can specify where the files should be placed, but by\n    # default they are stored in &lt;dbname&gt;_&lt;tablename&gt;_&lt;columnname&gt;\n    #('picklefile', 'pickle_obj', dict(), dict(fpath='folder_for_picklefiles')),\n\n    # very similar to above, but use only when storing text data\n    #('textfile', 'text_file'), # similar to above\n\n\n    ##### Constraints #####\n\n    #SQLAlchemy: CheckConstraint('category in (\"FICTION\",\"NONFICTION\")', name='salary_check')\n    ('check_constraint', 'category in (\"FICTION\",\"NONFICTION\")', dict(name='salary_check')),\n\n    #SQLAlchemy: CheckConstraint('age &gt; 0')\n    ('check_constraint', 'age &gt; 0'),\n\n    # make sure each category/title entry is unique\n    #SQLAlchemy:  UniqueConstraint('category', 'title', name='work_key')\n    ('unique_constraint', ['category','title'], dict(name='work_key')),\n\n    # makes a foreign key from the 'subkey' column of this table to the 'id'\n    # column of ANOTHERDOCTABLE, setting the SQL onupdate and ondelete foreign key constraints\n    #('foreignkey_constraint', [['subkey'], [ANOTHERDOCTABLE['id']]], {}, dict(onupdate=\"CASCADE\", ondelete=\"CASCADE\")),\n    #NOTE: Can't show here because we didn't make ANOTHERDOCTABLE\n\n    ##### Indexes ######\n\n    # make index table\n    # SQLAlchemy: Index('ind0', 'category', 'title', unique=True)\n    ('index', 'ind0', ('category','title'),dict(unique=True)),\n\n)\nmd = doctable.DocTable(target=':memory:', schema=schema, verbose=True)\nmd.schema_table()\n</code></pre> name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 category VARCHAR False None auto 0 2 title VARCHAR False None auto 0 3 abstract VARCHAR True None auto 0 4 age INTEGER True None auto 0 5 updated_on DATETIME True None auto 0 6 text VARCHAR(500) True None auto 0 7 json_data VARCHAR True None auto 0 8 tokenized BLOB True None auto 0"},{"location":"legacy_documentation/doctable_select/","title":"DocTable Examples: Select","text":"<p>Here I show how to select data from a DocTable. We cover object-oriented conditional selects emulating the <code>WHERE</code> SQL clause, as well as some reduce functions.</p> <pre><code>import random\nimport pandas as pd\nimport numpy as np\nimport sys\nsys.path.append('..')\nimport doctable\n</code></pre> <pre><code>import dataclasses\n@doctable.schema\nclass Record:\n    __slots__ = []\n    id: int = doctable.IDCol()\n    name: str = doctable.Col(nullable=False)\n    age: int = None\n    is_old: bool = None\n\ntable = doctable.DocTable(target=':memory:', schema=Record, verbose=True)\nprint(table)\n</code></pre> <pre><code>&lt;DocTable (4 cols)::sqlite:///:memory::_documents_&gt;\n</code></pre> <pre><code>N = 10\nfor i in range(N):\n    age = random.random() # number in [0,1]\n    is_old = age &gt; 0.5\n    table.insert({'name':'user_'+str(i), 'age':age, 'is_old':is_old}, verbose=False)\nprint(table)\n</code></pre> <pre><code>&lt;DocTable (4 cols)::sqlite:///:memory::_documents_&gt;\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead.\n  warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n</code></pre>"},{"location":"legacy_documentation/doctable_select/#regular-selects","title":"Regular Selects","text":"<p>These functions all return lists of ResultProxy objects. As such, they can be accessed using numerical indices or keyword indices. For instance, if one select output row is <code>row=(1, 'user_0')</code> (after selecting \"id\" and \"user\"), it can be accessed such that <code>row[0]==row['id']</code> and <code>row[1]==row['user']</code>.</p> <pre><code># the limit argument means the result will only return some rows.\n# I'll use it for convenience in these examples.\n# this selects all rows\ntable.select(limit=2)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_\n LIMIT ? OFFSET ?\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:449: UserWarning: Method .select() is depricated. Please use .q.select() instead.\n  warnings.warn('Method .select() is depricated. Please use .q.select() instead.')\n\n\n\n\n\n[Record(id=1, name='user_0', age=0.30111935823671676, is_old=False),\n Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)]\n</code></pre> <pre><code>table.select(['id','name'], limit=1)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name \nFROM _documents_\n LIMIT ? OFFSET ?\n\n\n\n\n\n[Record(id=1, name='user_0', age=None, is_old=None)]\n</code></pre> <pre><code># can also select by accessing the column object (db['id']) itself\n# this will be useful later with more complex queries\ntable.select([table['id'],table['name']], limit=1)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name \nFROM _documents_\n LIMIT ? OFFSET ?\n\n\n\n\n\n[Record(id=1, name='user_0', age=None, is_old=None)]\n</code></pre> <pre><code>table.select_first()\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_\n LIMIT ? OFFSET ?\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:427: UserWarning: Method .select_first() is depricated. Please use .q.select_first() instead.\n  warnings.warn('Method .select_first() is depricated. Please use .q.select_first() instead.')\n\n\n\n\n\nRecord(id=1, name='user_0', age=0.30111935823671676, is_old=False)\n</code></pre> <pre><code>table.select('name',limit=5)\n</code></pre> <pre><code>DocTable: SELECT _documents_.name \nFROM _documents_\n LIMIT ? OFFSET ?\n\n\n\n\n\n['user_0', 'user_1', 'user_2', 'user_3', 'user_4']\n</code></pre> <pre><code>table.select_first('age')\n</code></pre> <pre><code>DocTable: SELECT _documents_.age \nFROM _documents_\n LIMIT ? OFFSET ?\n\n\n\n\n\nRecord(age=0.30111935823671676, is_old=None)\n</code></pre>"},{"location":"legacy_documentation/doctable_select/#conditional-selects","title":"Conditional Selects","text":"<pre><code>table.select(where=table['id']==2)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_ \nWHERE _documents_.id = ?\n\n\n\n\n\n[Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)]\n</code></pre> <pre><code>table.select(where=table['id']&lt;3)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_ \nWHERE _documents_.id &lt; ?\n\n\n\n\n\n[Record(id=1, name='user_0', age=0.30111935823671676, is_old=False),\n Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)]\n</code></pre> <pre><code># mod operator works too\ntable.select(where=(table['id']%2)==0, limit=2)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_ \nWHERE _documents_.id % ? = ?\n LIMIT ? OFFSET ?\n\n\n\n\n\n[Record(id=2, name='user_1', age=0.7524872495613466, is_old=True),\n Record(id=4, name='user_3', age=0.9011039173289395, is_old=True)]\n</code></pre> <pre><code># note parantheses to handle order of ops with overloaded bitwise ops\ntable.select(where= (table['id']&gt;=2) &amp; (table['id']&lt;=4) &amp; (table['name']!='user_2'))\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_ \nWHERE _documents_.id &gt;= ? AND _documents_.id &lt;= ? AND _documents_.name != ?\n\n\n\n\n\n[Record(id=2, name='user_1', age=0.7524872495613466, is_old=True),\n Record(id=4, name='user_3', age=0.9011039173289395, is_old=True)]\n</code></pre> <pre><code>table.select(where=table['name'].in_(('user_2','user_3')))\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_ \nWHERE _documents_.name IN (__[POSTCOMPILE_name_1])\n\n\n\n\n\n[Record(id=3, name='user_2', age=0.33272186856831554, is_old=False),\n Record(id=4, name='user_3', age=0.9011039173289395, is_old=True)]\n</code></pre> <pre><code>table.select(where=table['id'].between(2,4))\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_ \nWHERE _documents_.id BETWEEN ? AND ?\n\n\n\n\n\n[Record(id=2, name='user_1', age=0.7524872495613466, is_old=True),\n Record(id=3, name='user_2', age=0.33272186856831554, is_old=False),\n Record(id=4, name='user_3', age=0.9011039173289395, is_old=True)]\n</code></pre> <pre><code># use of logical not operator \"~\"\ntable.select(where= ~(table['name'].in_(('user_2','user_3'))) &amp; (table['id'] &lt; 4))\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_ \nWHERE (_documents_.name NOT IN (__[POSTCOMPILE_name_1])) AND _documents_.id &lt; ?\n\n\n\n\n\n[Record(id=1, name='user_0', age=0.30111935823671676, is_old=False),\n Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)]\n</code></pre> <pre><code># more verbose operators .and_, .or_, and .not_ are bound to the doctable package\ntable.select(where= doctable.f.or_(doctable.f.not_(table['id']==4)) &amp; (table['id'] &lt;= 2))\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_ \nWHERE _documents_.id != ? AND _documents_.id &lt;= ?\n\n\n\n\n\n[Record(id=1, name='user_0', age=0.30111935823671676, is_old=False),\n Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)]\n</code></pre> <pre><code># now with simple computation\nages = table.select(table['age'])\nmean_age = sum(ages)/len(ages)\ntable.select(table['name'], where=table['age']&gt;mean_age, limit=2)\n</code></pre> <pre><code>DocTable: SELECT _documents_.age \nFROM _documents_\nDocTable: SELECT _documents_.name \nFROM _documents_ \nWHERE _documents_.age &gt; ?\n LIMIT ? OFFSET ?\n\n\n\n\n\n['user_1', 'user_3']\n</code></pre> <pre><code># apply .label() method to columns\ndict(table.select_first([table['age'].label('myage'), table['name'].label('myname')], as_dataclass=False))\n</code></pre> <pre><code>DocTable: SELECT _documents_.age AS myage, _documents_.name AS myname \nFROM _documents_\n LIMIT ? OFFSET ?\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:429: UserWarning: The \"as_dataclass\" parameter has been depricated: please set get_raw=True or select_raw to specify that you would like to retrieve a raw RowProxy pobject.\n  warnings.warn(f'The \"as_dataclass\" parameter has been depricated: please set get_raw=True or '\n\n\n\n\n\n{'myage': 0.30111935823671676, 'myname': 'user_0'}\n</code></pre>"},{"location":"legacy_documentation/doctable_select/#column-operators","title":"Column Operators","text":"<p>I bind the .min, .max, .count, .sum, and .mode methods to the column objects. Additionally, I move the .count method to a separate DocTable2 method.</p> <pre><code># with labels now\ndict(table.select_first([table['age'].sum().label('sum'), table['age'].count().label('ct')], as_dataclass=False))\n</code></pre> <pre><code>DocTable: SELECT sum(_documents_.age) AS sum, count(_documents_.age) AS ct \nFROM _documents_\n LIMIT ? OFFSET ?\n\n\n\n\n\n{'sum': 4.99992719426638, 'ct': 10}\n</code></pre> <pre><code>table.select_first([table['age'].sum(), table['age'].count(), table['age']], as_dataclass=False)\n</code></pre> <pre><code>DocTable: SELECT sum(_documents_.age) AS sum_1, count(_documents_.age) AS count_1, _documents_.age \nFROM _documents_\n LIMIT ? OFFSET ?\n\n\n\n\n\n(4.99992719426638, 10, 0.30111935823671676)\n</code></pre>"},{"location":"legacy_documentation/doctable_select/#order-by-group-by-limit","title":"ORDER BY, GROUP BY, LIMIT","text":"<p>These additional arguments have also been provided.</p> <pre><code># the limit is obvious - it has been used throughout these examples\ntable.select(limit=2)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_\n LIMIT ? OFFSET ?\n\n\n\n\n\n[Record(id=1, name='user_0', age=0.30111935823671676, is_old=False),\n Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)]\n</code></pre> <pre><code>table.select([table['is_old'], doctable.f.count()], groupby=table['is_old'])\n</code></pre> <pre><code>DocTable: SELECT _documents_.is_old, count(*) AS count_1 \nFROM _documents_ GROUP BY _documents_.is_old\nDocTable: SELECT _documents_.is_old, count(*) AS count_1 \nFROM _documents_ GROUP BY _documents_.is_old\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:464: UserWarning: Conversion from row to object failed according to the following error. Please use .q.select_raw() when requesting non-object formatted data such as counts or sums in the future. For now it is automatically converted. e=RowDataConversionFailed(\"Conversion from &lt;class 'sqlalchemy.engine.row.LegacyRow'&gt; to &lt;class '__main__.Record'&gt; failed.\")\n  warnings.warn(f'Conversion from row to object failed according to the following '\n\n\n\n\n\n[(False, 5), (True, 5)]\n</code></pre> <pre><code># orderby clause\ntable.select(orderby=table['age'].desc(), limit=2)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_ ORDER BY _documents_.age DESC\n LIMIT ? OFFSET ?\n\n\n\n\n\n[Record(id=4, name='user_3', age=0.9011039173289395, is_old=True),\n Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)]\n</code></pre> <pre><code># compound orderby\ntable.select(orderby=(table['age'].desc(),table['is_old'].asc()), limit=2)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_ ORDER BY _documents_.age DESC, _documents_.is_old ASC\n LIMIT ? OFFSET ?\n\n\n\n\n\n[Record(id=4, name='user_3', age=0.9011039173289395, is_old=True),\n Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)]\n</code></pre> <pre><code>f = doctable.f\ncols = [table['is_old'], f.count().label('ct')]\ntable.q.select_raw(cols, groupby=table['is_old'], orderby=f.asc('ct'))\n</code></pre> <pre><code>DocTable: SELECT _documents_.is_old, count(*) AS ct \nFROM _documents_ GROUP BY _documents_.is_old ORDER BY ct ASC\n\n\n\n\n\n[(False, 5), (True, 5)]\n</code></pre> <pre><code># can also use column name directly\n# can only use ascending and can use only one col\ntable.select(orderby='age', limit=2)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_ ORDER BY _documents_.age\n LIMIT ? OFFSET ?\n\n\n\n\n\n[Record(id=7, name='user_6', age=0.04850236983248746, is_old=False),\n Record(id=6, name='user_5', age=0.300309388680601, is_old=False)]\n</code></pre> <pre><code># groupby clause\n# returns first row of each group without any aggregation functions\ntable.select(groupby=table['is_old'])\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_ GROUP BY _documents_.is_old\n\n\n\n\n\n[Record(id=1, name='user_0', age=0.30111935823671676, is_old=False),\n Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)]\n</code></pre> <pre><code># compound groupby (weird example bc name is unique - have only one cat var in this demo)\ntable.select(groupby=(table['is_old'],table['name']), limit=3)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_ GROUP BY _documents_.is_old, _documents_.name\n LIMIT ? OFFSET ?\n\n\n\n\n\n[Record(id=1, name='user_0', age=0.30111935823671676, is_old=False),\n Record(id=3, name='user_2', age=0.33272186856831554, is_old=False),\n Record(id=6, name='user_5', age=0.300309388680601, is_old=False)]\n</code></pre> <pre><code># groupby clause using max aggregation function\n# gets match age for both old and young groups\ntable.select(table['age'].max(), groupby=table['is_old'])\n</code></pre> <pre><code>DocTable: SELECT max(_documents_.age) AS max_1 \nFROM _documents_ GROUP BY _documents_.is_old\n\n\n\n\n\n[0.46166274965800924, 0.9011039173289395]\n</code></pre>"},{"location":"legacy_documentation/doctable_select/#sql-string-commands-and-additional-clauses","title":"SQL String Commands and Additional Clauses","text":"<p>For cases where DocTable2 does not provide a convenient interface, you may submit raw SQL commands. These may be a bit more unwieldly, but they offer maximum flexibility. They may be used either as simply an addition to the WHERE or arbitrary end clauses, or accessed in totality.</p> <pre><code>qstr = 'SELECT age,name FROM {} WHERE id==\"{}\"'.format(table.tabname, 1)\nresults = table.execute(qstr)\ndict(list(results)[0])\n</code></pre> <pre><code>DocTable: SELECT age,name FROM _documents_ WHERE id==\"1\"\n\n\n\n\n\n{'age': 0.30111935823671676, 'name': 'user_0'}\n</code></pre> <pre><code>wherestr = 'is_old==\"{}\"'.format('1')\ntable.select(wherestr=wherestr, limit=2)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_ \nWHERE (is_old==\"1\")\n LIMIT ? OFFSET ?\n\n\n\n\n\n[Record(id=2, name='user_1', age=0.7524872495613466, is_old=True),\n Record(id=4, name='user_3', age=0.9011039173289395, is_old=True)]\n</code></pre> <pre><code># combine whrstr with structured query where clause\nwherestr = 'is_old==\"{}\"'.format('1')\ntable.select(where=table['id']&lt;=5, wherestr=wherestr)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_ \nWHERE _documents_.id &lt;= ? AND (is_old==\"1\")\n\n\n\n\n\n[Record(id=2, name='user_1', age=0.7524872495613466, is_old=True),\n Record(id=4, name='user_3', age=0.9011039173289395, is_old=True),\n Record(id=5, name='user_4', age=0.6092744222076869, is_old=True)]\n</code></pre> <pre><code># combine whrstr with structured query where clause\nwherestr = 'is_old==\"{}\"'.format('1')\ntable.select(where=table['id']&lt;=5, wherestr=wherestr)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_ \nWHERE _documents_.id &lt;= ? AND (is_old==\"1\")\n\n\n\n\n\n[Record(id=2, name='user_1', age=0.7524872495613466, is_old=True),\n Record(id=4, name='user_3', age=0.9011039173289395, is_old=True),\n Record(id=5, name='user_4', age=0.6092744222076869, is_old=True)]\n</code></pre>"},{"location":"legacy_documentation/doctable_select/#count-method-and-get-next-id","title":"Count Method and Get Next ID","text":"<p><code>.count()</code> is a convenience method. Mostly the same could be accomplished by <code>db.select_first(db['id'].count())</code>, but this requires no reference to a specific column.</p> <p><code>.next_id()</code> is especially useful if one hopes to enter the id (or any primary key column) into new rows manually. Especially useful because SQL engines don't provide new ids except when a single insert is performed.</p> <pre><code>table.count()\n</code></pre> <pre><code>DocTable: SELECT count(_documents_.id) AS count_1 \nFROM _documents_\n LIMIT ? OFFSET ?\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead.\n  warnings.warn('Method .count() is depricated. Please use .q.count() instead.')\n\n\n\n\n\n10\n</code></pre> <pre><code>table.count(table['age'] &lt; 0.5)\n</code></pre> <pre><code>DocTable: SELECT count(_documents_.id) AS count_1 \nFROM _documents_ \nWHERE _documents_.age &lt; ?\n LIMIT ? OFFSET ?\n\n\n\n\n\n5\n</code></pre>"},{"location":"legacy_documentation/doctable_select/#select-as-pandas-series-and-dataframe","title":"Select as Pandas Series and DataFrame","text":"<p>These are especially useful when working with metadata because Pandas provides robust descriptive and plotting features than SQL alone. Good for generating sample information.</p> <pre><code># must provide only a single column\ntable.select_series(table['age']).head(2)\n</code></pre> <pre><code>DocTable: SELECT _documents_.age \nFROM _documents_\n\n\n\n\n\n0    0.301119\n1    0.752487\ndtype: float64\n</code></pre> <pre><code>table.select_series(table['age']).quantile([0.025, 0.985])\n</code></pre> <pre><code>DocTable: SELECT _documents_.age \nFROM _documents_\n\n\n\n\n\n0.025    0.105159\n0.985    0.881041\ndtype: float64\n</code></pre> <pre><code>table.select_df(['id','age']).head(2)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.age \nFROM _documents_\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead.\n  warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.')\n</code></pre> id age 0 1 0.301119 1 2 0.752487 <pre><code>table.select_df('age').head(2)\n</code></pre> <pre><code>DocTable: SELECT _documents_.age \nFROM _documents_\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead.\n  warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.')\n</code></pre> age 0 0.301119 1 0.752487 <pre><code># must provide list of cols (even for one col)\ntable.select_df([table['id'],table['age']]).corr()\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.age \nFROM _documents_\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead.\n  warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.')\n</code></pre> id age id 1.000000 0.006293 age 0.006293 1.000000 <pre><code>table.select_df([table['id'],table['age']]).describe().T\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.age \nFROM _documents_\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead.\n  warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.')\n</code></pre> count mean std min 25% 50% 75% max id 10.0 5.500000 3.027650 1.000000 3.25000 5.500000 7.750000 10.000000 age 10.0 0.499993 0.256825 0.048502 0.30902 0.535469 0.659958 0.901104 <pre><code>mean_age = table.select_series(table['age']).mean()\ndf = table.select_df([table['id'],table['age']])\ndf['old_grp'] = df['age'] &gt; mean_age\ndf.groupby('old_grp').describe()\n</code></pre> <pre><code>DocTable: SELECT _documents_.age \nFROM _documents_\nDocTable: SELECT _documents_.id, _documents_.age \nFROM _documents_\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:415: UserWarning: Method .select_series() is depricated. Please use .q.select_series() instead.\n  warnings.warn('Method .select_series() is depricated. Please use .q.select_series() instead.')\n/DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead.\n  warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.')\n</code></pre> id age count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max old_grp False 5.0 5.0 2.915476 1.0 3.0 6.0 7.0 8.0 5.0 0.288863 0.149865 0.048502 0.300309 0.301119 0.332722 0.461663 True 5.0 6.0 3.391165 2.0 4.0 5.0 9.0 10.0 5.0 0.711122 0.120456 0.609274 0.619203 0.673543 0.752487 0.901104 <pre><code># more complicated groupby aggregation.\n# calculates the variance both for entries above and below average age\nmean_age = table.select_series(table['age']).mean()\ndf = table.select_df([table['name'],table['age']])\ndf['old_grp'] = df['age']&gt;mean_age\ndf.groupby('old_grp').agg(**{\n    'first_name':pd.NamedAgg(column='name', aggfunc='first'),\n    'var_age':pd.NamedAgg(column='age', aggfunc=np.var),\n})\n</code></pre> <pre><code>DocTable: SELECT _documents_.age \nFROM _documents_\nDocTable: SELECT _documents_.name, _documents_.age \nFROM _documents_\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:415: UserWarning: Method .select_series() is depricated. Please use .q.select_series() instead.\n  warnings.warn('Method .select_series() is depricated. Please use .q.select_series() instead.')\n/DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead.\n  warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.')\n</code></pre> first_name var_age old_grp False user_0 0.022459 True user_1 0.014510"},{"location":"legacy_documentation/doctable_select/#select-with-buffer","title":"Select with Buffer","text":"<p>In cases where you have many rows or each row contains a lot of data, you may want to perform a select query which makes requests in chunks. This is performed using the SQL OFFSET command, and querying up to buffsize while yielding each returned row. This system is designed this way because the underlying sql engine buffers all rows retreived from a query, and thus there is no way to stream data into memory without this system.</p> <p>NOTE: The limit keyword is incompatible with this method - it will return all results. A workaround is to use the approx_max_rows param, which will return at minimum this number of rows, at max the specified number of rows plus buffsize.</p> <pre><code>for row_chunk in table.select_chunks(chunksize=2, where=(table['id']%2)==0, verbose=False):\n    print(row_chunk)\n</code></pre> <pre><code>[Record(id=2, name='user_1', age=0.7524872495613466, is_old=True), Record(id=4, name='user_3', age=0.9011039173289395, is_old=True)]\n[Record(id=6, name='user_5', age=0.300309388680601, is_old=False), Record(id=8, name='user_7', age=0.46166274965800924, is_old=False)]\n[Record(id=10, name='user_9', age=0.6192026652607745, is_old=True)]\n[]\n</code></pre>"},{"location":"legacy_documentation/doctable_update/","title":"DocTable Examples: Update","text":"<p>Here I show how to update data into a DocTable. In addition to providing updated values, DocTable also allows you to create map functions to transform existing data.</p> <pre><code>import random\nimport pandas as pd\nimport numpy as np\nimport sys\nsys.path.append('..')\nimport doctable\n</code></pre> <pre><code>import dataclasses\n@doctable.schema\nclass Record:\n    __slots__ = []\n    id: int = doctable.IDCol()\n    name: str = doctable.Col(nullable=False)\n    age: int = None\n    is_old: bool = None\n\ndef new_db():\n    table = doctable.DocTable(schema=Record, target=':memory:', verbose=True)\n    N = 10\n    for i in range(N):\n        age = random.random() # number in [0,1]\n        is_old = age &gt; 0.5\n        table.insert({'name':'user_'+str(i), 'age':age, 'is_old':is_old}, verbose=False)\n    return table\n\ntable = new_db()\nprint(table)\n</code></pre> <pre><code>&lt;DocTable (4 cols)::sqlite:///:memory::_documents_&gt;\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:324: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead.\n  warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:350: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n</code></pre> <pre><code>table.select_df(limit=3)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old \nFROM _documents_\n LIMIT ? OFFSET ?\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead.\n  warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.')\n</code></pre> id name age is_old 0 1 user_0 0.998030 True 1 2 user_1 0.210891 False 2 3 user_2 0.431233 False"},{"location":"legacy_documentation/doctable_update/#single-update","title":"Single Update","text":"<p>Update multiple (or single) rows with same values.</p> <pre><code>table = new_db()\ntable.select_df(where=table['is_old']==True, limit=3, verbose=False)\n</code></pre> <pre><code>/DataDrive/code/doctable/examples/../doctable/doctable.py:324: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead.\n  warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:350: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead.\n  warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.')\n</code></pre> id name age is_old 0 1 user_0 0.670833 True 1 2 user_1 0.895172 True 2 5 user_4 0.688209 True <pre><code>table = new_db()\ntable.update({'age':1},where=table['is_old']==True)\ntable.update({'age':0},where=table['is_old']==False)\ntable.select_df(limit=3, verbose=False)\n</code></pre> <pre><code>DocTable: UPDATE _documents_ SET age=? WHERE _documents_.is_old = 1\nDocTable: UPDATE _documents_ SET age=? WHERE _documents_.is_old = 0\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:324: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead.\n  warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:350: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:440: UserWarning: Method .update() is depricated. Please use .q.update() instead.\n  warnings.warn('Method .update() is depricated. Please use .q.update() instead.')\n/DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead.\n  warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.')\n</code></pre> id name age is_old 0 1 user_0 1 True 1 2 user_1 0 False 2 3 user_2 1 True"},{"location":"legacy_documentation/doctable_update/#apply-as-map-function","title":"Apply as Map Function","text":"<p>This feature allows you to update columns based on the values of old columns.</p> <pre><code>table = new_db()\nvalues = {table['name']:table['name']+'th', table['age']:table['age']+1, table['is_old']:True}\ntable.update(values)\ntable.select_df(limit=3, verbose=False)\n</code></pre> <pre><code>DocTable: UPDATE _documents_ SET name=(_documents_.name || ?), age=(_documents_.age + ?), is_old=?\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:324: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead.\n  warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:350: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:440: UserWarning: Method .update() is depricated. Please use .q.update() instead.\n  warnings.warn('Method .update() is depricated. Please use .q.update() instead.')\n/DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead.\n  warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.')\n</code></pre> id name age is_old 0 1 user_0th 1.566417 True 1 2 user_1th 1.434875 True 2 3 user_2th 1.422777 True"},{"location":"legacy_documentation/doctable_update/#apply-as-set-of-ordered-map-functions","title":"Apply as Set of Ordered Map Functions","text":"<p>This is useful for when the updating of one column might change the value of another, depending on the order in which it was applied.</p> <pre><code>table = new_db()\nvalues = [(table['name'],table['age']-1), (table['age'],table['age']+1),]\ntable.update(values)\ntable.select_df(limit=3, verbose=False)\n</code></pre> <pre><code>DocTable: UPDATE _documents_ SET name=(_documents_.age - ?), age=(_documents_.age + ?)\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:324: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead.\n  warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:350: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:440: UserWarning: Method .update() is depricated. Please use .q.update() instead.\n  warnings.warn('Method .update() is depricated. Please use .q.update() instead.')\n/DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead.\n  warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.')\n</code></pre> id name age is_old 0 1 -0.823513491706054 1.176487 False 1 2 -0.567734080088791 1.432266 False 2 3 -0.838314843815808 1.161685 False"},{"location":"legacy_documentation/doctable_update/#update-using-sql-where-string","title":"Update Using SQL WHERE String","text":"<pre><code>table = new_db()\ntable.update({'age':1.00}, wherestr='is_old==true')\ntable.select_df(limit=5, verbose=False)\n</code></pre> <pre><code>DocTable: UPDATE _documents_ SET age=? WHERE is_old==true\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:324: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead.\n  warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:350: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:440: UserWarning: Method .update() is depricated. Please use .q.update() instead.\n  warnings.warn('Method .update() is depricated. Please use .q.update() instead.')\n/DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead.\n  warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.')\n</code></pre> id name age is_old 0 1 user_0 0.488699 False 1 2 user_1 0.391556 False 2 3 user_2 1.000000 True 3 4 user_3 0.472176 False 4 5 user_4 0.154501 False"},{"location":"legacy_documentation/example_nss_1_intro/","title":"Vignette 1: Storing Document Metadata","text":"<p>In this example, I'll show how to create and manipulate two linked tables for storing document metadata using US National Security Strategy document metadata as an example. </p> <p>These are the vignettes I have created:</p> <ul> <li> <p>1: Storing Document Metadata</p> </li> <li> <p>2: Storing Document Text</p> </li> <li> <p>3: Storing Parsed Documents</p> </li> </ul> <pre><code>import sys\nsys.path.append('..')\nimport doctable\nimport spacy\nfrom tqdm import tqdm\nimport pandas as pd\nimport os\nfrom pprint import pprint\nimport urllib.request # used for downloading nss docs\n\n# automatically clean up temp folder after python ends\n#tmpfolder = doctable.TempFolder('tmp')\nimport tempfile\ntempdir = tempfile.TemporaryDirectory()\ntmpfolder = tempdir.name\ntmpfolder\n</code></pre> <pre><code>'/tmp/tmplxxguo16'\n</code></pre>"},{"location":"legacy_documentation/example_nss_1_intro/#introduction-to-nss-corpus","title":"Introduction to NSS Corpus","text":"<p>This dataset is the plain text version of the US National Security Strategy documents. I compiled the metadata you see below from a page hosted by the historical dept of the secretary's office. In short, each US President must release at least one NSS per term, up to one per-year. This is the metadata we will be inserting into the table:</p> <pre><code># information about each NSS document\ndocument_metadata = [\n    {'year': 2000, 'party': 'D', 'president': 'Clinton'},\n    {'year': 2002, 'party': 'R', 'president': 'W. Bush'}, \n    {'year': 2006, 'party': 'R', 'president': 'W. Bush'}, \n    {'year': 2010, 'party': 'D', 'president': 'Obama'}, \n    {'year': 2015, 'party': 'D', 'president': 'Obama'}, \n    {'year': 2017, 'party': 'R', 'president': 'Trump'}, \n]\n</code></pre>"},{"location":"legacy_documentation/example_nss_1_intro/#create-database-schemas","title":"Create database schemas","text":"<p>The first step will be to define a database schema that is appropriate for the data in <code>document_metadata</code>. We define an <code>NSSDoc</code> class to represent a single document. The <code>doctable.schema</code> decorator will convert the row objects into <code>dataclasses</code> with slots enabled, and inherit from doctable.DocTableRow to add some additional functionality. The type hints associated with each variable will be used in the schema definition for the new tables, and arguments to <code>doctable.Col</code> will mostly be passed to <code>dataclasses.field</code> (see docs for more detail), so all dataclass functionality is maintained.</p> <p>Also note that a method called <code>.is_old()</code> was defined. This method will not be included in a database schema, but I'll show later how it can be useful.</p> <pre><code># to be used as a database row representing a single NSS document\n@doctable.schema\nclass NSSDoc:\n    __slots__ = [] # include so that doctable.schema can create a slot class\n\n    id: int = doctable.Col(primary_key=True, autoincrement=True) # can also use doctable.IDCol() as a shortcut\n    year: int =  None\n    party: str = None\n    president: str = None\n\n    def is_old(self):\n        '''Return whether the document is old or not.'''\n        return self.year &lt; 2010\n</code></pre> <p>We can see that these are regular dataclass methods because their constructors are defined. Note that the dataclass defaults the values to None, so take note of this when inserting or retrieving from a database.</p> <pre><code>NSSDoc(year=1999)\n</code></pre> <pre><code>NSSDoc(year=1999, party=None, president=None)\n</code></pre> <p>And we will also likely want to create a class that inherits from <code>DocTable</code> to statically define the table name, schema object, and any indices or constraints that should be associated with our table. We set the table name and the schema definition class using the reserved member variables <code>_tabname_</code> and <code>_schema_</code>, respectively. Note that the <code>NSSDoc</code> class is provided as the schema.</p> <p>We also can use this definition to create indices and constraints using the <code>_indices_</code> and <code>_constraints_</code> member variables. The indices are provided as name-&gt;columns pairs, and the constraints are tuples of the form <code>(constraint_type, constraint_details)</code>. In this case, we limit the values for <code>check</code> to R or D.</p> <pre><code>class NSSDocTable(doctable.DocTable):\n    _tabname_ = 'nss_documents'\n    _schema_ = NSSDoc\n    _indices_ = (\n        doctable.Index('party_index', 'party'),\n    )\n    _constraints_ = (\n        doctable.Constraint('check', 'party in (\"R\", \"D\")'), # party can only take on values R or D.\n    )\n</code></pre> <p>And then we create an instance of the <code>NSSDocTable</code> table using <code>DocTable</code>\\'s default constructor. We set <code>target=f'{tmp}/nss_1.db'</code> to indicate we want to access an sqlite database at that path. We also use the <code>new_db=True</code> to indicate that the database does not exist, so we should create a new one.</p> <pre><code>fname = f'{tmpfolder}/nss_1.db'\n\n# clean up any old databases\ntry:\n    os.remove(fname)\nexcept:\n    pass\n\ndocs_table = NSSDocTable(target=fname, new_db=True)\ndocs_table\n</code></pre> <pre><code>&lt;__main__.NSSDocTable at 0x7f2a55b9b400&gt;\n</code></pre> <p>We can use <code>.schema_table()</code> to see information about the database schema. Note that doctable inferred column types based on the type hints.</p> <pre><code>docs_table.schema_table()\n</code></pre> name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 year INTEGER True None auto 0 2 party VARCHAR True None auto 0 3 president VARCHAR True None auto 0 <p>We are now ready to insert data into the new table. We simply add each document as a dictionary, and show the first <code>n</code> rows using <code>.head()</code>.</p> <pre><code>docs_table.delete() # remove old entries if needed\nfor doc in document_metadata:\n    print(doc)\n    docs_table.insert(doc)\ndocs_table.head()\n</code></pre> <pre><code>{'year': 2000, 'party': 'D', 'president': 'Clinton'}\n{'year': 2002, 'party': 'R', 'president': 'W. Bush'}\n{'year': 2006, 'party': 'R', 'president': 'W. Bush'}\n{'year': 2010, 'party': 'D', 'president': 'Obama'}\n{'year': 2015, 'party': 'D', 'president': 'Obama'}\n{'year': 2017, 'party': 'R', 'president': 'Trump'}\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:494: UserWarning: Method .delete() is depricated. Please use .q.delete() instead.\n  warnings.warn('Method .delete() is depricated. Please use .q.delete() instead.')\n/DataDrive/code/doctable/examples/../doctable/doctable.py:364: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead.\n  warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:390: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:407: UserWarning: Method .head() is depricated. Please use .q.select_head() instead.\n  warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.')\n</code></pre> id year party president 0 1 2000 D Clinton 1 2 2002 R W. Bush 2 3 2006 R W. Bush 3 4 2010 D Obama 4 5 2015 D Obama <p>We can verify that the constraint was defined by attempting to insert a row with an unknown party code.</p> <pre><code>import sqlalchemy\ntry:\n    docs_table.insert({'party':'whateva'})\nexcept sqlalchemy.exc.IntegrityError as e:\n    print(e)\n</code></pre> <pre><code>/DataDrive/code/doctable/examples/../doctable/doctable.py:364: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead.\n  warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:390: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n</code></pre> <p>And we can use all the expected select (see select examples) methods.</p> <pre><code>democrats = docs_table.select(where=docs_table['party']=='D')\ndemocrats\n</code></pre> <pre><code>/DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Method .select() is depricated. Please use .q.select() instead.\n  warnings.warn('Method .select() is depricated. Please use .q.select() instead.')\n\n\n\n\n\n[NSSDoc(id=1, year=2000, party='D', president='Clinton'),\n NSSDoc(id=4, year=2010, party='D', president='Obama'),\n NSSDoc(id=5, year=2015, party='D', president='Obama')]\n</code></pre> <pre><code>clinton_doc = docs_table.select_first(where=docs_table['president']=='Clinton')\nclinton_doc\n</code></pre> <pre><code>/DataDrive/code/doctable/examples/../doctable/doctable.py:426: UserWarning: Method .select_first() is depricated. Please use .q.select_first() instead.\n  warnings.warn('Method .select_first() is depricated. Please use .q.select_first() instead.')\n\n\n\n\n\nNSSDoc(id=1, year=2000, party='D', president='Clinton')\n</code></pre> <p>Along with the methods we defined on the schema objects.</p> <pre><code>clinton_doc.is_old()\n</code></pre> <pre><code>True\n</code></pre>"},{"location":"legacy_documentation/example_nss_1_intro/#adding-political-party-data","title":"Adding political party data","text":"<p>Of course, relational database schemas often involve the use of more than one linked table. Now we'll attempt to integrate the data in <code>party_metadata</code> into our schema.</p> <pre><code># full name of party (we will use later)\nparty_metadata = [\n    {'code': 'R', 'name': 'Republican'},\n    {'code': 'D', 'name': 'Democrat'},\n]\n</code></pre> <p>First, we create the <code>Party</code> dataclass just as before.</p> <pre><code># to be used as a database row representing a single political party\n@doctable.schema\nclass Party:\n    __slots__ = []\n\n    id: int = doctable.Col(primary_key=True, autoincrement=True) # can also use doctable.IDCol() as a shortcut    \n    code: str = None\n    name: str = None\n</code></pre> <p>And then define a <code>DocTable</code> with a 'foreignkey' constraint that indicates it\\'s relationship to the document table. We can use the reference to the \"party\" column using <code>nss_documents.party</code>.</p> <pre><code>class PartyTable(doctable.DocTable):\n    _tabname_ = 'political_parties'\n    _schema_ = Party\n    _indices_ = {\n        doctable.Index('code_index', 'code')\n    }\n    _constraints_ = (\n        doctable.Constraint('foreignkey', ('code',), ('nss_documents.party',)),\n    )\n\nparty_table = PartyTable(target=fname)\nparty_table\n</code></pre> <pre><code>&lt;__main__.PartyTable at 0x7f2a55afa310&gt;\n</code></pre> <pre><code>party_table.delete() # remove old entries if needed\nfor party in party_metadata:\n    print(party)\n    party_table.insert(party)\nparty_table.head()\n</code></pre> <pre><code>{'code': 'R', 'name': 'Republican'}\n{'code': 'D', 'name': 'Democrat'}\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:494: UserWarning: Method .delete() is depricated. Please use .q.delete() instead.\n  warnings.warn('Method .delete() is depricated. Please use .q.delete() instead.')\n/DataDrive/code/doctable/examples/../doctable/doctable.py:407: UserWarning: Method .head() is depricated. Please use .q.select_head() instead.\n  warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.')\n</code></pre> id code name 0 1 R Republican 1 2 D Democrat"},{"location":"legacy_documentation/example_nss_1_intro/#performing-join-select-queries","title":"Performing \"join\" select queries","text":"<p>In contrast to sql, the type of join is inferred from the way the select query is used. Using a <code>select</code> method with columns for both tables will issue an outer join in lieu of other parameters. Also note that we must use <code>as_dataclass</code> to indicate the data should not use a dataclass for the results, since joined results includes fields from both </p> <pre><code>party_table.select(['name', docs_table['president']], as_dataclass=False)\n</code></pre> <pre><code>/DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Method .select() is depricated. Please use .q.select() instead.\n  warnings.warn('Method .select() is depricated. Please use .q.select() instead.')\n/DataDrive/code/doctable/examples/../doctable/doctable.py:445: UserWarning: The \"as_dataclass\" parameter has been depricated: please set get_raw=True or select_raw to specify that you would like to retrieve a raw RowProxy pobject.\n  warnings.warn(f'The \"as_dataclass\" parameter has been depricated: please set get_raw=True or '\n/DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: SELECT statement has a cartesian product between FROM element(s) \"nss_documents\" and FROM element \"political_parties\".  Apply join condition(s) between each element to resolve.\n  return self._engine.execute(query, *args, **kwargs)\n/DataDrive/code/doctable/examples/../doctable/doctable.py:453: UserWarning: Conversion from row to object failed according to the following error. Please use .q.select_raw() next time in the future to avoid this issue. e=RowDataConversionFailed(\"Conversion from &lt;class 'sqlalchemy.engine.row.LegacyRow'&gt; to &lt;class '__main__.Party'&gt; failed.\")\n  warnings.warn(f'Conversion from row to object failed according to the following '\n\n\n\n\n\n[('Republican', 'Clinton'),\n ('Republican', 'W. Bush'),\n ('Republican', 'W. Bush'),\n ('Republican', 'Obama'),\n ('Republican', 'Obama'),\n ('Republican', 'Trump'),\n ('Republican', None),\n ('Democrat', 'Clinton'),\n ('Democrat', 'W. Bush'),\n ('Democrat', 'W. Bush'),\n ('Democrat', 'Obama'),\n ('Democrat', 'Obama'),\n ('Democrat', 'Trump'),\n ('Democrat', None)]\n</code></pre> <p>To perform an inner join, use a where conditional indicating the columns to be matched.</p> <pre><code>docs_table.select(['year', 'president', party_table['name']], as_dataclass=False, where=docs_table['party']==party_table['code'])\n</code></pre> <pre><code>/DataDrive/code/doctable/examples/../doctable/doctable.py:453: UserWarning: Conversion from row to object failed according to the following error. Please use .q.select_raw() next time in the future to avoid this issue. e=RowDataConversionFailed(\"Conversion from &lt;class 'sqlalchemy.engine.row.LegacyRow'&gt; to &lt;class '__main__.NSSDoc'&gt; failed.\")\n  warnings.warn(f'Conversion from row to object failed according to the following '\n\n\n\n\n\n[(2000, 'Clinton', 'Democrat'),\n (2002, 'W. Bush', 'Republican'),\n (2006, 'W. Bush', 'Republican'),\n (2010, 'Obama', 'Democrat'),\n (2015, 'Obama', 'Democrat'),\n (2017, 'Trump', 'Republican')]\n</code></pre> <p>And this works approximately the same when we switch the tables being selected.</p> <pre><code>party_table.select(['code', 'name', docs_table['president']], as_dataclass=False, where=docs_table['party']==party_table['code'])\n</code></pre> <pre><code>[('R', 'Republican', 'Trump'),\n ('R', 'Republican', 'W. Bush'),\n ('R', 'Republican', 'W. Bush'),\n ('D', 'Democrat', 'Clinton'),\n ('D', 'Democrat', 'Obama'),\n ('D', 'Democrat', 'Obama')]\n</code></pre> <p>And that is all for this vignette! See the list of vignettes at the top of this page for more examples.</p>"},{"location":"legacy_documentation/example_nss_2_parsing/","title":"Vignette 2: Storing Document Text","text":"<p>In this example, I'll show how to create a database for document text + metadata storage using the <code>DocTable</code> class, and a parser class using a <code>ParsePipeline</code>. We will store the metadata you see below with the raw text and parsed tokens in the same DocTable.</p> <p>These are the vignettes I have created:</p> <ul> <li> <p>1: Storing Document Metadata</p> </li> <li> <p>2: Storing Document Text</p> </li> <li> <p>3: Storing Parsed Documents</p> </li> </ul> <pre><code>import sys\nsys.path.append('..')\nimport doctable\nimport spacy\nfrom tqdm import tqdm\nimport pandas as pd\nimport os\nfrom pprint import pprint\nimport urllib.request # used for downloading nss docs\n\n# automatically clean up temp folder after python ends\nimport tempfile\ntempdir = tempfile.TemporaryDirectory()\ntmpfolder = tempdir.name\ntmpfolder\n</code></pre> <pre><code>'/tmp/tmpm_ciuemc'\n</code></pre>"},{"location":"legacy_documentation/example_nss_2_parsing/#introduction-to-nss-corpus","title":"Introduction to NSS Corpus","text":"<p>This dataset is the plain text version of the US National Security Strategy documents. During the parsing process, all plain text files will be downloaded from my github project hosting the nss docs. I compiled the metadata you see below from a page hosted by the historical dept of the secretary's office. In short, each US President must release at least one NSS per term, with some (namely Clinton) producing more.</p> <p>I've defined the document metadata as <code>nss_metadata</code>, which contains the year (which I used to make the url), the president name, and the political party they belong to. We will later use <code>download_nss()</code> to actually download the text and store it into the database.</p> <pre><code>def download_nss(year):\n    ''' Simple helper function for downloading texts from my nssdocs repo.'''\n    baseurl = 'https://raw.githubusercontent.com/devincornell/nssdocs/master/docs/{}.txt'\n    url = baseurl.format(year)\n    text = urllib.request.urlopen(url).read().decode('utf-8')\n    return text\n</code></pre> <pre><code>document_metadata = [\n    {'year': 2000, 'party': 'D', 'president': 'Clinton'},\n    {'year': 2002, 'party': 'R', 'president': 'W. Bush'}, \n    {'year': 2006, 'party': 'R', 'president': 'W. Bush'}, \n    {'year': 2010, 'party': 'D', 'president': 'Obama'}, \n    {'year': 2015, 'party': 'D', 'president': 'Obama'}, \n    {'year': 2017, 'party': 'R', 'president': 'Trump'}, \n]\n</code></pre> <pre><code># downloader example: first 100 characters of 1993 NSS document\ntext = download_nss(1993)\ntext[:100]\n</code></pre> <pre><code>'Preface \\n\\nAmerican Leadership for Peaceful Change \\n\\nOur great Nation stands at a crossroads in histo'\n</code></pre>"},{"location":"legacy_documentation/example_nss_2_parsing/#1-create-a-table-schema","title":"1. Create a Table Schema","text":"<p>The first step will be to define a database schema that is appropriate for the data in <code>document_metadata</code>. We define an <code>NSSDoc</code> class to represent a single document. The <code>doctable.schema</code> decorator will convert the row objects into <code>dataclasses</code> with slots enabled, and inherit from doctable.DocTableRow to add some additional functionality. The type hints associated with each variable will be used in the schema definition for the new tables, and arguments to <code>doctable.Col</code> will mostly be passed to <code>dataclasses.field</code> (see docs for more detail), so all dataclass functionality is maintained.</p> <p>See the schema guide for examples of the full range of column types.</p> <pre><code>from typing import Any # import generic type hint\n\n# to be used as a database row representing a single NSS document\n@doctable.schema\nclass NSSDoc:\n    __slots__ = [] # include so that doctable.schema can create a slot class\n\n    id: int = doctable.IDCol() # this is an alias for doctable.Col(primary_key=True, autoincrement=True)\n    year: int =  doctable.Col()\n    party: str = doctable.Col()\n    president: str = doctable.Col()\n    text: str = doctable.Col()\n    tokens: Any = doctable.Col() # this will be used as a binary type that stores pickled data\n\n    @property\n    def num_tokens(self):\n        return len(self.tokens)\n\n    def paragraphs(self):\n        return self.text.split('\\n\\n')\n</code></pre>"},{"location":"legacy_documentation/example_nss_2_parsing/#2-define-a-custom-doctable","title":"2. Define a Custom DocTable","text":"<p>Now we define a class called <code>NSSDocTable</code> to represent the database table. This table must inherit from <code>DocTable</code> and will store connection and schema information. The <code>DocTable</code> class is often used by subclassing. Our <code>NSSDocs</code> class inherits from <code>DocTable</code> and will store connection and schema information. Because the default constructor checks for statically define member variables <code>tabname</code> and <code>schema</code> (as well as others), we can simply add them to the class definition. </p> <p>We also can use this definition to create indices and constraints using the <code>_indices_</code> and <code>_constraints_</code> member variables. The indices are provided as name-&gt;columns pairs, and the constraints are tuples of the form <code>(constraint_type, constraint_details)</code>. In this case, we limit the values for <code>check</code> to R or D.</p> <pre><code>class NSSDocTable(doctable.DocTable):\n    _tabname_ = 'nss_documents'\n    _schema_ = NSSDoc\n    _indices_ = (\n        doctable.Index('party_index', 'party'),\n    )\n    _constraints_ = (\n        doctable.Constraint('check', 'party in (\"R\", \"D\")'),\n    )\n</code></pre> <p>We can then create a connection to a database by instantiating the <code>NSSDocTable</code> class. We used <code>target=':memory:'</code> to indicate that the sqlite table should be created in-memory.</p> <pre><code># printing the DocTable object itself shows how many entries there are\nnss_table = NSSDocTable(target=':memory:')\nprint(nss_table.count())\nprint(nss_table)\nnss_table.schema_table()\n</code></pre> <pre><code>0\n&lt;NSSDocTable (6 cols)::sqlite:///:memory::nss_documents&gt;\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:402: UserWarning: Method .count() is depricated. Please use .q.count() instead.\n  warnings.warn('Method .count() is depricated. Please use .q.count() instead.')\n</code></pre> name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 year INTEGER True None auto 0 2 party VARCHAR True None auto 0 3 president VARCHAR True None auto 0 4 text VARCHAR True None auto 0 5 tokens BLOB True None auto 0"},{"location":"legacy_documentation/example_nss_2_parsing/#2-insert-data-into-the-table","title":"2. Insert Data Into the Table","text":"<p>Now let's download and store the text into the database. Each loop downloads a text document and inserts it into the doctable, and we use the <code>.insert()</code> method to insert a single row at a time. The row to be inserted is represented as a dictionary, and any missing column information is left as NULL. The <code>ifnotunique</code> argument is set to false because if we were to re-run this code, it needs to replace the existing document of the same year. Recall that in the schema we placed a unique constraint on the year column.</p> <pre><code>for docmeta in tqdm(document_metadata):\n    text = download_nss(docmeta['year'])\n    nss_table.insert({**docmeta, **{'text': text}}, ifnotunique='replace')\nnss_table.head()\n</code></pre> <pre><code>  0%|                                                                                                                                                      | 0/6 [00:00&lt;?, ?it/s]/DataDrive/code/doctable/examples/../doctable/doctable.py:364: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead.\n  warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:390: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:00&lt;00:00, 12.32it/s]\n/DataDrive/code/doctable/examples/../doctable/doctable.py:407: UserWarning: Method .head() is depricated. Please use .q.select_head() instead.\n  warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.')\n</code></pre> id year party president text tokens 0 1 2000 D Clinton As we enter the new millennium, we are blessed... None 1 2 2002 R W. Bush The great struggles of the twentieth century b... None 2 3 2006 R W. Bush My fellow Americans, \\n\\nAmerica is at war. Th... None 3 4 2010 D Obama Time and again in our Nation's history, Americ... None 4 5 2015 D Obama Today, the United States is stronger and bette... None"},{"location":"legacy_documentation/example_nss_2_parsing/#3-query-table-data","title":"3. Query Table Data","text":"<p>Now that we have inserted the NSS documents into the table, there are a few ways we can query the data. To select the first entry of the table use <code>.select_first()</code>. This method returns a simple <code>sqlalchemy.RowProxy</code> object which can be accessed like a dictionary or like a tuple.</p> <pre><code>row = nss_table.select_first(['id', 'year', 'party', 'president'])\nprint(row)\nprint(row.president)\n</code></pre> <pre><code>/DataDrive/code/doctable/examples/../doctable/doctable.py:426: UserWarning: Method .select_first() is depricated. Please use .q.select_first() instead.\n  warnings.warn('Method .select_first() is depricated. Please use .q.select_first() instead.')\n\n\nNSSDoc(id=1, year=2000, party='D', president='Clinton')\nClinton\n</code></pre> <p>To select more than one row, use the <code>.select()</code> method. If you'd only like to return the first few rows, you can use the <code>limit</code> argument.</p> <pre><code>rows = nss_table.select(limit=2)\nprint(rows[0].year)\nprint(rows[1].year)\n</code></pre> <pre><code>2000\n2002\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Method .select() is depricated. Please use .q.select() instead.\n  warnings.warn('Method .select() is depricated. Please use .q.select() instead.')\n</code></pre> <p>We can also select only a few columns.</p> <pre><code>nss_table.select(['year', 'president'], limit=3)\n</code></pre> <pre><code>[NSSDoc(year=2000, president='Clinton'),\n NSSDoc(year=2002, president='W. Bush'),\n NSSDoc(year=2006, president='W. Bush')]\n</code></pre> <p>For convenience, we can also use the <code>.select_df()</code> method to return directly as a pandas dataframe.</p> <pre><code># use select_df to show a couple rows of our database\nnss_table.select_df(limit=2)\n</code></pre> <pre><code>/DataDrive/code/doctable/examples/../doctable/doctable.py:419: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead.\n  warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.')\n</code></pre> id year party president text tokens 0 1 2000 D Clinton As we enter the new millennium, we are blessed... None 1 2 2002 R W. Bush The great struggles of the twentieth century b... None <p>And access the <code>.paragraphs()</code> method we defined in <code>NSSDoc</code>.</p> <pre><code>for row in nss_table.select(limit=3):\n    print(f\"{row.president} ({row.year}): num_paragraphs={len(row.paragraphs())}\")\n</code></pre> <pre><code>Clinton (2000): num_paragraphs=569\nW. Bush (2002): num_paragraphs=199\nW. Bush (2006): num_paragraphs=474\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Method .select() is depricated. Please use .q.select() instead.\n  warnings.warn('Method .select() is depricated. Please use .q.select() instead.')\n</code></pre>"},{"location":"legacy_documentation/example_nss_2_parsing/#4-create-a-parser-for-tokenization","title":"4. Create a Parser for Tokenization","text":"<p>Now that the text is in the doctable, we can extract it using <code>.select()</code>, parse it, and store the parsed text back into the table using <code>.update()</code>.</p> <p>Now we create a parser using <code>ParsePipeline</code> and a list of functions to apply to the text sequentially. The <code>Comp</code> function returns a doctable parse function with additional keyword arguments. For instance, the following two expressions would be the same.</p> <pre><code>doctable.Comp('keep_tok', keep_punct=True) # is equivalent to\nlambda x: doctable.parse.parse_tok_func(x, keep_punct=True)\n</code></pre> <p>Note in this example that the 'tokenize' function takes two function arguments: <code>keep_tok_func</code> and <code>parse_tok_func</code>, which are also specified using the <code>.Comp()</code> function. The available pipeline components are listed in the parse function documentation.</p> <pre><code># add pipeline components\nparser = doctable.ParsePipeline([\n    spacy.load('en_core_web_sm'), # load a spacy parser as first step in pipeline\n    doctable.Comp('tokenize', **{\n        'split_sents': False,\n        'keep_tok_func': doctable.Comp('keep_tok'),\n        'parse_tok_func': doctable.Comp('parse_tok'),\n    })\n])\n\nparser.components\n</code></pre> <pre><code>[&lt;spacy.lang.en.English at 0x7f37ce907f70&gt;,\n functools.partial(&lt;function tokenize at 0x7f37d3d74e50&gt;, split_sents=False, keep_tok_func=functools.partial(&lt;function keep_tok at 0x7f37d3d74f70&gt;), parse_tok_func=functools.partial(&lt;function parse_tok at 0x7f37d3d74ee0&gt;))]\n</code></pre> <p>Now we loop through rows in the doctable and for each iteration parse the text and insert it back into the table using <code>.update()</code>. We use the <code>ParsePipeline</code> method <code>.parsemany()</code> to parse paragraphs from each document in parallel.</p> <pre><code>for doc in tqdm(nss_table.select(['id','year','text'])):\n    paragraphs = parser.parsemany(doc.text.split('\\n\\n'), workers=30) # parse paragraphs in parallel\n    nss_table.update({'tokens': [t for p in paragraphs for t in p]}, where=nss_table['id']==doc.id)\n</code></pre> <pre><code>  0%|                                                                                                                                                      | 0/6 [00:00&lt;?, ?it/s]/DataDrive/code/doctable/examples/../doctable/doctable.py:489: UserWarning: Method .update() is depricated. Please use .q.update() instead.\n  warnings.warn('Method .update() is depricated. Please use .q.update() instead.')\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:07&lt;00:00,  1.18s/it]\n</code></pre> <pre><code>nss_table.select_df(limit=3)\n</code></pre> <pre><code>/DataDrive/code/doctable/examples/../doctable/doctable.py:419: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead.\n  warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.')\n</code></pre> id year party president text tokens 0 1 2000 D Clinton As we enter the new millennium, we are blessed... [as, we, enter, the, new, millennium, ,, we, a... 1 2 2002 R W. Bush The great struggles of the twentieth century b... [the, great, struggles, of, the, twentieth, ce... 2 3 2006 R W. Bush My fellow Americans, \\n\\nAmerica is at war. Th... [my, fellow, Americans, ,, America, is, at, wa... <pre><code>for doc in nss_table.select():\n    print(f\"{doc.president} ({doc.year}): {len(doc.tokens)} tokens.\")\n</code></pre> <pre><code>Clinton (2000): 50156 tokens.\nW. Bush (2002): 14493 tokens.\nW. Bush (2006): 21590 tokens.\nObama (2010): 31997 tokens.\nObama (2015): 16611 tokens.\nTrump (2017): 24420 tokens.\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Method .select() is depricated. Please use .q.select() instead.\n  warnings.warn('Method .select() is depricated. Please use .q.select() instead.')\n</code></pre> <p>And that is all for this vignette! See the list of vignettes at the top of this page for more examples.</p>"},{"location":"legacy_documentation/example_nss_3_parsetrees/","title":"Vignette 3: Storing Parsed Documents","text":"<p>Here I'll show how to make a DocTable for storing NSS documents at the paragraph level, and parse the documents in parallel.</p> <p>For context, check out Example 1 - here we'll just use some shortcuts for code used there. These come from the util.py code in the repo examples folder.</p> <p>These are the vignettes I have created:</p> <ul> <li> <p>1: Storing Document Metadata</p> </li> <li> <p>2: Storing Document Text</p> </li> <li> <p>3: Storing Parsed Documents</p> </li> </ul> <pre><code>import sys\nsys.path.append('..')\n#import util\nimport doctable\nimport spacy\nfrom tqdm import tqdm\n\n# automatically clean up temp folder after python ends\nimport tempfile\ntempdir = tempfile.TemporaryDirectory()\ntmpfolder = tempdir.name\ntmpfolder\n</code></pre> <pre><code>'/tmp/tmp1isfmada'\n</code></pre> <p>First we define the metadata and download the text data.</p> <pre><code>import urllib\ndef download_nss(year):\n    ''' Simple helper function for downloading texts from my nssdocs repo.'''\n    baseurl = 'https://raw.githubusercontent.com/devincornell/nssdocs/master/docs/{}.txt'\n    url = baseurl.format(year)\n    text = urllib.request.urlopen(url).read().decode('utf-8')\n    return text\n\ndocument_metadata = [\n    {'year': 2000, 'party': 'D', 'president': 'Clinton'},\n    {'year': 2006, 'party': 'R', 'president': 'W. Bush'}, \n    {'year': 2015, 'party': 'D', 'president': 'Obama'}, \n    {'year': 2017, 'party': 'R', 'president': 'Trump'}, \n]\n\nsep = '\\n\\n'\nfirst_n = 10\nfor md in document_metadata:\n    text = download_nss(md['year'])\n    md['text'] = sep.join(text.split(sep)[:first_n])\nprint(f\"{len(document_metadata[0]['text'])=}\")\n</code></pre> <pre><code>len(document_metadata[0]['text'])=6695\n</code></pre>"},{"location":"legacy_documentation/example_nss_3_parsetrees/#1-define-the-doctable-schema","title":"1. Define the DocTable Schema","text":"<p>Now we define a doctable schema using the <code>doctable.schema</code> class decorator and the pickle file column type to prepare to store parsetrees as binary data.</p> <pre><code># to be used as a database row representing a single NSS document\n@doctable.schema\nclass NSSDoc:\n    __slots__ = [] # include so that doctable.schema can create a slot class\n\n    id: int = doctable.IDCol() # this is an alias for doctable.Col(primary_key=True, autoincrement=True)\n    year: int =  doctable.Col()\n    party: str = doctable.Col()\n    president: str = doctable.Col()\n    text: str = doctable.Col()\n    doc: doctable.ParseTreeDoc = doctable.ParseTreeFileCol(f'{tmpfolder}/parsetree_pickle_files')\n</code></pre> <p>And a class to represent an NSS DocTable.</p> <pre><code>class NSSDocTable(doctable.DocTable):\n    _tabname_ = 'nss_documents'\n    _schema_ = NSSDoc\n\nnss_table = NSSDocTable(target=f'{tmpfolder}/nss_3.db', new_db=True)\nprint(nss_table.count())\nnss_table.schema_table()\n</code></pre> <pre><code>0\n\n\n/DataDrive/code/doctable/examples/../doctable/doctable.py:402: UserWarning: Method .count() is depricated. Please use .q.count() instead.\n  warnings.warn('Method .count() is depricated. Please use .q.count() instead.')\n</code></pre> name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 year INTEGER True None auto 0 2 party VARCHAR True None auto 0 3 president VARCHAR True None auto 0 4 text VARCHAR True None auto 0 5 doc VARCHAR True None auto 0 <pre><code>for md in document_metadata:\n    nss_table.insert(md)\nnss_table.head()\n</code></pre> <pre><code>/DataDrive/code/doctable/examples/../doctable/doctable.py:364: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead.\n  warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:390: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw()\n  warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '\n/DataDrive/code/doctable/examples/../doctable/doctable.py:407: UserWarning: Method .head() is depricated. Please use .q.select_head() instead.\n  warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.')\n/DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: TypeDecorator ParseTreeDocFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True.  This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions.  Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf)\n  return self._engine.execute(query, *args, **kwargs)\n</code></pre> id year party president text doc 0 1 2000 D Clinton As we enter the new millennium, we are blessed... None 1 2 2006 R W. Bush My fellow Americans, \\n\\nAmerica is at war. Th... None 2 3 2015 D Obama Today, the United States is stronger and bette... None 3 4 2017 R Trump An America that is safe, prosperous, and free ... None"},{"location":"legacy_documentation/example_nss_3_parsetrees/#2-create-a-parser-class-using-a-pipeline","title":"2. Create a Parser Class Using a Pipeline","text":"<p>Now we create a small <code>NSSParser</code> class that keeps a <code>doctable.ParsePipeline</code> object for doing the actual text processing. As you can see from our init method, instantiating the package will load a spacy module into memory and construct the pipeline from the selected components. We also create a wrapper over the pipeline <code>.parse</code> and <code>.parsemany</code> methods. Here we define, instantiate, and view the components of <code>NSSParser</code>.</p> <pre><code>class NSSParser:\n    ''' Handles text parsing for NSS documents.'''\n    def __init__(self):\n        nlp = spacy.load('en_core_web_sm')\n\n        # this determines all settings for tokenizing\n        self.pipeline = doctable.ParsePipeline([\n            nlp, # first run spacy parser\n            doctable.Comp('merge_tok_spans', merge_ents=True),\n            doctable.Comp('get_parsetrees', **{\n                'text_parse_func': doctable.Comp('parse_tok', **{\n                    'format_ents': True,\n                    'num_replacement': 'NUM',\n                })\n            })\n        ])\n\n    def parse(self, text):\n        return self.pipeline.parse(text)\n\nparser = NSSParser() # creates a parser instance\nparser.pipeline.components\n</code></pre> <pre><code>[&lt;spacy.lang.en.English at 0x7fedee1c2cd0&gt;,\n functools.partial(&lt;function merge_tok_spans at 0x7fedf2d8f040&gt;, merge_ents=True),\n functools.partial(&lt;function get_parsetrees at 0x7fedf2d8f1f0&gt;, text_parse_func=functools.partial(&lt;function parse_tok at 0x7fedf2d82ee0&gt;, format_ents=True, num_replacement='NUM'))]\n</code></pre> <p>Now we parse the paragraphs of each document in parallel.</p> <pre><code>for doc in tqdm(nss_table.select(['id','year','text'])):\n    parsed = parser.parse(doc.text)\n    #print(parsed.as_dict())\n    #break\n    print(nss_table['doc'])\n    nss_table.update({'doc': parsed}, where=nss_table['id']==doc.id, verbose=True)\n#nss_table.select_df(limit=2)\n</code></pre> <pre><code>/DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Method .select() is depricated. Please use .q.select() instead.\n  warnings.warn('Method .select() is depricated. Please use .q.select() instead.')\n  0%|                                                                                                                                                      | 0/4 [00:00&lt;?, ?it/s]/DataDrive/code/doctable/examples/../doctable/doctable.py:489: UserWarning: Method .update() is depricated. Please use .q.update() instead.\n  warnings.warn('Method .update() is depricated. Please use .q.update() instead.')\n 25%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                          | 1/4 [00:00&lt;00:00,  3.83it/s]\n\nnss_documents.doc\nDocTable: UPDATE nss_documents SET doc=? WHERE nss_documents.id = ?\nnss_documents.doc\nDocTable: UPDATE nss_documents SET doc=? WHERE nss_documents.id = ?\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00&lt;00:00,  4.89it/s]\n\nnss_documents.doc\nDocTable: UPDATE nss_documents SET doc=? WHERE nss_documents.id = ?\nnss_documents.doc\nDocTable: UPDATE nss_documents SET doc=? WHERE nss_documents.id = ?\n</code></pre>"},{"location":"legacy_documentation/example_nss_3_parsetrees/#3-work-with-parsetrees","title":"3. Work With Parsetrees","text":"<p>Now that we have stored our parsed text as files in the database, we can manipulate the parsetrees. This example shows the 5 most common nouns from each national security strategy document. This is possible because the <code>doctable.ParseTree</code> data structures contain <code>pos</code> information originally provided by the spacy parser. Using <code>ParseTreeFileType</code> allows us to more efficiently store pickled binary data so that we can perform these kinds of analyses at scale.</p> <pre><code>from collections import Counter # used to count tokens\n\nfor nss in nss_table.select():\n    noun_counts = Counter([tok.text for pt in nss.doc for tok in pt if tok.pos == 'NOUN'])\n    print(f\"{nss.president} ({nss.year}): {noun_counts.most_common(5)}\")\n</code></pre> <pre><code>Clinton (2000): [('world', 9), ('security', 9), ('prosperity', 7), ('threats', 5), ('efforts', 5)]\nW. Bush (2006): [('people', 4), ('world', 3), ('war', 2), ('security', 2), ('strategy', 2)]\nObama (2015): [('security', 15), ('world', 9), ('opportunities', 7), ('strength', 7), ('challenges', 7)]\nTrump (2017): [('government', 5), ('principles', 4), ('peace', 3), ('people', 3), ('world', 3)]\n\n\n/DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: TypeDecorator ParseTreeDocFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True.  This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions.  Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf)\n  return self._engine.execute(query, *args, **kwargs)\n</code></pre> <p>Definitely check out this example on parsetreedocs if you're interested in more applications.</p> <p>And that is all for this vignette! See the list of vignettes at the top of this page for more examples.</p>"},{"location":"legacy_documentation/example_nss_intro_dataclass/","title":"Example 1: US National Security Strategy Document Corpus","text":"<p>In this example, I'll show how to create a database for document + metadata storage using the <code>DocTable</code> class, and a parser class using a <code>ParsePipeline</code>. We will store the metadata you see below with the raw text and parsed tokens in the same DocTable.</p> <pre><code>import sys\nsys.path.append('..')\nimport doctable\nimport spacy\nfrom tqdm import tqdm\nimport pandas as pd\nimport os\nfrom pprint import pprint\nimport urllib.request # used for downloading nss docs\n</code></pre>"},{"location":"legacy_documentation/example_nss_intro_dataclass/#introduction-to-nss-corpus","title":"Introduction to NSS Corpus","text":"<p>This dataset is the plain text version of the US National Security Strategy documents. During the parsing process, all plain text files will be downloaded from my github project hosting the nss docs. I compiled the metadata you see below from a page hosted by the historical dept of the secretary's office. In short, each US President must release at least one NSS per term, with some (namely Clinton) producing more.</p> <p>Here I've created the function <code>download_nss</code> to download the text data from my nssdocs github repository, and the python dictionary <code>nss_metadata</code> to store information about each document to be stored in the database.</p> <pre><code>def download_nss(year):\n    ''' Simple helper function for downloading texts from my nssdocs repo.'''\n    baseurl = 'https://raw.githubusercontent.com/devincornell/nssdocs/master/docs/{}.txt'\n    url = baseurl.format(year)\n    text = urllib.request.urlopen(url).read().decode('utf-8')\n    return text\n</code></pre> <pre><code>nss_metadata = {\n    1987: {'party': 'R', 'president': 'Reagan'}, \n    1993: {'party': 'R', 'president': 'H.W. Bush'}, \n    2002: {'party': 'R', 'president': 'W. Bush'}, \n    2015: {'party': 'D', 'president': 'Obama'}, \n    1994: {'party': 'D', 'president': 'Clinton'}, \n    1990: {'party': 'R', 'president': 'H.W. Bush'}, \n    1991: {'party': 'R', 'president': 'H.W. Bush'}, \n    2006: {'party': 'R', 'president': 'W. Bush'}, \n    1997: {'party': 'D', 'president': 'Clinton'}, \n    1995: {'party': 'D', 'president': 'Clinton'}, \n    1988: {'party': 'R', 'president': 'Reagan'}, \n    2017: {'party': 'R', 'president': 'Trump'}, \n    1996: {'party': 'D', 'president': 'Clinton'}, \n    2010: {'party': 'D', 'president': 'Obama'}, \n    1999: {'party': 'D', 'president': 'Clinton'}, \n    1998: {'party': 'D', 'president': 'Clinton'}, \n    2000: {'party': 'D', 'president': 'Clinton'}\n}\n</code></pre> <pre><code># downloader example: first 100 characters of 1993 NSS document\ntext = download_nss(1993)\ntext[:100]\n</code></pre> <pre><code>'Preface \\n\\nAmerican Leadership for Peaceful Change \\n\\nOur great Nation stands at a crossroads in histo'\n</code></pre>"},{"location":"legacy_documentation/example_nss_intro_dataclass/#1-create-a-doctable-schema","title":"1. Create a DocTable Schema","text":"<p>The <code>DocTable</code> class is often used by subclassing. Our <code>NSSDocs</code> class inherits from <code>DocTable</code> and will store connection and schema information. Because the default constructor checks for statically define member variables <code>tabname</code> and <code>schema</code> (as well as others), we can simply add them to the class definition. </p> <p>In this example, we create the 'id' column as a unique index, the 'year', 'president', and 'party' columns for storing the metadata we defined above in <code>nss_metadata</code>, and columns for raw and parse text. See the schema guide for examples of the full range of column types.</p> <pre><code>from dataclasses import dataclass\nfrom typing import Any\n\n@doctable.schema(require_slots=False)\nclass NSSDoc:\n    id: int = doctable.IDCol()\n    year: int = doctable.Col(nullable=False)\n    president: str = doctable.Col()\n    party: str = doctable.Col()\n    text: str = doctable.Col()\n    parsed: Any = doctable.Col()\n\n</code></pre> <p>We can then create a connection to a database by instantiating the <code>NSSDocs</code> class. Since the <code>fname</code> parameter was not provided, this doctable exists only in memory using sqlite (uses special sqlite name \":memory:\"). We will use this for these examples.</p> <p>We can check the sqlite table schema using <code>.schema_table()</code>. You can see that the 'pickle' datatype we chose above is represented as a BLOB column. This is because DocTable, using SQLAlchemy core, creates an interface on top of sqlite to handle the data conversion. You can view the number of documents using <code>.count()</code> or by viewing the db instance as a string (in this case with print function).</p> <pre><code># printing the DocTable object itself shows how many entries there are\ndb = doctable.DocTable(schema=NSSDoc, target=':memory:', verbose=True)\nprint(db.count())\nprint(db)\ndb.schema_table()\n</code></pre> <pre><code>DocTable: SELECT count() AS count_1 \nFROM _documents_\n LIMIT ? OFFSET ?\n0\n&lt;DocTable (6 cols)::sqlite:///:memory::_documents_&gt;\n</code></pre> name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 year INTEGER False None auto 0 2 president VARCHAR True None auto 0 3 party VARCHAR True None auto 0 4 text VARCHAR True None auto 0 5 parsed BLOB True None auto 0"},{"location":"legacy_documentation/example_nss_intro_dataclass/#2-insert-data-into-the-table","title":"2. Insert Data Into the Table","text":"<p>Now let's download and store the text into the database. Each loop downloads a text document and inserts it into the doctable, and we use the <code>.insert()</code> method to insert a single row at a time. The row to be inserted is represented as a dictionary, and any missing column information is left as NULL. The <code>ifnotunique</code> argument is set to false because if we were to re-run this code, it needs to replace the existing document of the same year. Recall that in the schema we placed a unique constraint on the year column.</p> <pre><code>for year, docmeta in tqdm(nss_metadata.items()):\n    text = download_nss(year)\n    new_doc = NSSDoc(\n        year=year, \n        party=docmeta['party'], \n        president=docmeta['president'], \n        text=text\n    )\n    db.insert(new_doc, ifnotunique='replace', verbose=False)\ndb.head()\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17/17 [00:01&lt;00:00, 12.31it/s]\n\nDocTable: SELECT _documents_.id, _documents_.year, _documents_.president, _documents_.party, _documents_.text, _documents_.parsed \nFROM _documents_\n LIMIT ? OFFSET ?\n</code></pre> id year president party text parsed 0 1 1987 Reagan R I. An American Perspective \\n\\nIn the early da... [[I., An, American, Perspective], [in, the, ea... 1 2 1993 H.W. Bush R Preface \\n\\nAmerican Leadership for Peaceful C... [[preface], [American, leadership, for, peacef... 2 3 2002 W. Bush R The great struggles of the twentieth century b... [[the, great, struggles, of, the, twentieth, c... 3 4 2015 Obama D Today, the United States is stronger and bette... [[Today, ,, the, United, States, is, stronger,... 4 5 1994 Clinton D Preface \\n\\nProtecting our nation's security -... [[preface], [protecting, our, nation, 's, secu..."},{"location":"legacy_documentation/example_nss_intro_dataclass/#3-query-table-data","title":"3. Query Table Data","text":"<p>Now that we have inserted the NSS documents into the table, there are a few ways we can query the data. To select the first entry of the table use <code>.select_first()</code>. This method returns a simple <code>sqlalchemy.RowProxy</code> object which can be accessed like a dictionary or like a tuple.</p> <pre><code>row = db.select_first()\n#print(row)\nprint(row['president'])\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.year, _documents_.president, _documents_.party, _documents_.text, _documents_.parsed \nFROM _documents_\n LIMIT ? OFFSET ?\nReagan\n</code></pre> <p>To select more than one row, use the <code>.select()</code> method. If you'd only like to return the first few rows, you can use the <code>limit</code> argument.</p> <pre><code>rows = db.select(limit=2)\nprint(rows[0]['year'])\nprint(rows[1]['year'])\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.year, _documents_.president, _documents_.party, _documents_.text, _documents_.parsed \nFROM _documents_\n LIMIT ? OFFSET ?\n1987\n1993\n</code></pre> <p>We can also select only a few columns.</p> <pre><code>db.select(['year', 'president'], limit=3)\n</code></pre> <pre><code>DocTable: SELECT _documents_.year, _documents_.president \nFROM _documents_\n LIMIT ? OFFSET ?\n\n\n\n\n\n[NSSDoc(year=1987, president='Reagan'),\n NSSDoc(year=1993, president='H.W. Bush'),\n NSSDoc(year=2002, president='W. Bush')]\n</code></pre> <p>For convenience, we can also use the <code>.select_df()</code> method to return directly as a pandas dataframe.</p> <pre><code># use select_df to show a couple rows of our database\ndb.select_df(limit=2)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.year, _documents_.president, _documents_.party, _documents_.text, _documents_.parsed \nFROM _documents_\n LIMIT ? OFFSET ?\n</code></pre> id year president party text parsed 0 1 1987 Reagan R I. An American Perspective \\n\\nIn the early da... [[I., An, American, Perspective], [in, the, ea... 1 2 1993 H.W. Bush R Preface \\n\\nAmerican Leadership for Peaceful C... [[preface], [American, leadership, for, peacef..."},{"location":"legacy_documentation/example_nss_intro_dataclass/#4-create-a-parser-for-tokenization","title":"4. Create a Parser for Tokenization","text":"<p>Now that the text is in the doctable, we can extract it using <code>.select()</code>, parse it, and store the parsed text back into the table using <code>.update()</code>.</p> <p>Now we create a parser using <code>ParsePipeline</code> and a list of functions to apply to the text sequentially. The <code>Comp</code> function returns a doctable parse function with additional keyword arguments. For instance, the following two expressions would be the same.</p> <pre><code>doctable.component('keep_tok', keep_punct=True) # is equivalent to\nlambda x: doctable.parse.parse_tok_func(x, keep_punct=True)\n</code></pre> <p>Note in this example that the 'tokenize' function takes two function arguments <code>keep_tok_func</code> and <code>parse_tok_func</code> which are also specified using the <code>.Comp()</code> function. The available pipeline components are listed in the parse function documentation.</p> <pre><code># first load a spacy model\nnlp = spacy.load('en_core_web_sm')\n\n# add pipeline components\nparser = doctable.ParsePipeline([\n    nlp, # first run spacy parser\n    doctable.Comp('tokenize', **{\n        'split_sents': False,\n        'keep_tok_func': doctable.Comp('keep_tok'),\n        'parse_tok_func': doctable.Comp('parse_tok'),\n    })\n])\n\nparser.components\n</code></pre> <pre><code>[&lt;spacy.lang.en.English at 0x7f2cd9334400&gt;,\n functools.partial(&lt;function tokenize at 0x7f2d86167160&gt;, split_sents=False, keep_tok_func=functools.partial(&lt;function keep_tok at 0x7f2d86167280&gt;), parse_tok_func=functools.partial(&lt;function parse_tok at 0x7f2d861671f0&gt;))]\n</code></pre> <p>Now we loop through rows in the doctable and for each iteration parse the text and insert it back into the table using <code>.update()</code>. We use the <code>ParsePipeline</code> method <code>.parsemany()</code> to parse paragraphs from each document in parallel. This is much faster.</p> <pre><code>docs = db.select()\nfor doc in tqdm(docs):\n    doc.parsed = parser.parsemany(doc.text[:1000].split('\\n\\n'), workers=8) # parse paragraphs in parallel\n    db.update_dataclass(doc, verbose=False)\n</code></pre> <pre><code>  0%|          | 0/51 [00:00&lt;?, ?it/s]\n\nDocTable: SELECT _documents_.id, _documents_.year, _documents_.president, _documents_.party, _documents_.text, _documents_.parsed \nFROM _documents_\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:04&lt;00:00, 11.42it/s]\n</code></pre> <p>See the 'parsed' column in the dataframe below to view the paragraphs.</p> <pre><code>db.select_df(limit=3)\n</code></pre> <pre><code>DocTable: SELECT _documents_.id, _documents_.year, _documents_.president, _documents_.party, _documents_.text, _documents_.parsed \nFROM _documents_\n LIMIT ? OFFSET ?\n</code></pre> id year president party text parsed 0 1 1987 Reagan R I. An American Perspective \\n\\nIn the early da... [[I., An, American, Perspective], [in, the, ea... 1 2 1993 H.W. Bush R Preface \\n\\nAmerican Leadership for Peaceful C... [[preface], [American, leadership, for, peacef... 2 3 2002 W. Bush R The great struggles of the twentieth century b... [[the, great, struggles, of, the, twentieth, c... <p>And here we show a few tokenized paragraphs.</p> <pre><code>paragraphs = db.select_first('parsed')\nfor par in paragraphs[:3]:\n    print(par, '\\n')\n</code></pre> <pre><code>DocTable: SELECT _documents_.parsed \nFROM _documents_\n LIMIT ? OFFSET ?\n['I.', 'An', 'American', 'Perspective']\n\n['in', 'the', 'early', 'days', 'of', 'this', 'administration', 'we', 'laid', 'the', 'foundation', 'for', 'a', 'more', 'constructive', 'and', 'positive', 'American', 'role', 'in', 'world', 'affairs', 'by', 'clarifying', 'the', 'essential', 'elements', 'of', 'U.S.', 'foreign', 'and', 'defense', 'policy', '.']\n\n['over', 'the', 'intervening', 'years', ',', 'we', 'have', 'looked', 'objectively', 'at', 'our', 'policies', 'and', 'performance', 'on', 'the', 'world', 'scene', 'to', 'ensure', 'they', 'reflect', 'the', 'dynamics', 'of', 'a', 'complex', 'and', 'ever', '-', 'changing', 'world', '.', 'where', 'course', 'adjustments', 'have', 'been', 'required', ',', 'i', 'have', 'directed', 'changes', '.', 'but', 'we', 'have', 'not', 'veered', 'and', 'will', 'not', 'veer', 'from', 'the', 'broad', 'aims', 'that', 'guide', 'America', \"'s\", 'leadership', 'role', 'in', 'today', \"'s\", 'world', ':']\n</code></pre>"},{"location":"legacy_documentation/legacy_adv/","title":"DocTable (slightly more) Advanced Example","text":"<p>In this notebook, I show how to define a DocTable with blob data types, add new rows, and then iterate through rows to populate previously empty fields.</p> <pre><code>import email\nfrom .legacy_helper import get_sklearn_newsgroups # for this example\n\nimport sys\nsys.path.append('..')\nimport doctable as dt # this will be the table object we use to interact with our database.\n\ntempfolder = dt.TempFolder('tmp')\n</code></pre>"},{"location":"legacy_documentation/legacy_adv/#get-news-data-from-sklearndatasets","title":"Get News Data From sklearn.datasets","text":"<p>Then parses into a dataframe.</p> <pre><code>ddf = get_sklearn_newsgroups()\nprint(ddf.shape)\nddf.head(3)\n</code></pre> <pre><code>(11314, 3)\n</code></pre> filename target text 0 21379 soc.religion.christian From: kbanner@philae.sas.upenn.edu (Ken Banner... 1 20874 soc.religion.christian From: simon@monu6.cc.monash.edu.au\\nSubject: S... 2 58936 sci.med From: jeffp@vetmed.wsu.edu (Jeff Parke)\\nSubje..."},{"location":"legacy_documentation/legacy_adv/#define-newsgroups-doctable","title":"Define NewsGroups DocTable","text":"<p>This definition includes fields file_id, category, raw_text, subject, author, and tokenized_text. The extra columns compared to example_simple.ipynb are for storing extracted metadata.</p> <pre><code>class NewsGroups(dt.DocTableLegacy):\n    def __init__(self, fname):\n        '''\n            DocTable class.\n            Inputs:\n                fname: fname is the name of the new sqlite database that will be used for instances of class.\n        '''\n        tabname = 'newsgroups'\n        super().__init__(\n            fname=fname, \n            tabname=tabname, \n            colschema=(\n                'id integer primary key autoincrement',\n                'file_id int', \n                'category string',\n                'raw_text string',\n                'subject string', \n                'author string', \n                'tokenized_text blob', \n            ),\n            constraints=('UNIQUE(file_id)',)\n        )\n\n        # create indices on file_id and category\n        self.query(\"create index if not exists idx1 on \"+tabname+\"(file_id)\")\n        self.query(\"create index if not exists idx2 on \"+tabname+\"(category)\")\n</code></pre> <pre><code>sng = NewsGroups(f'{tmp}/news_groupssss.db')\nprint(sng)\n</code></pre> <pre><code>&lt;Documents ct: 0&gt;\n</code></pre> <pre><code># add in raw data\ncol_order = ('file_id','category','raw_text')\ndata = [(dat['filename'],dat['target'],dat['text']) for ind,dat in ddf.iterrows()]\nsng.addmany(data,keys=col_order, ifnotunique='ignore')\nsng.getdf(limit=2)\n</code></pre> id file_id category raw_text subject author tokenized_text 0 1 21379 soc.religion.christian From: kbanner@philae.sas.upenn.edu (Ken Banner... None None None 1 2 20874 soc.religion.christian From: simon@monu6.cc.monash.edu.au\\nSubject: S... None None None"},{"location":"legacy_documentation/legacy_adv/#update-tokenized_text-column","title":"Update \"tokenized_text\" Column","text":"<p>Use .get() to loop through rows in the database, and .update() to add in the newly extracted data. In this case, we simply tokenize the text using the python builtin split() function.</p> <pre><code>query = sng.get(sel=('file_id','raw_text',))\nfor row in query:\n\n    dat = {'tokenized_text':row['raw_text'].split(),}\n    sng.update(dat, 'file_id == {}'.format(row['file_id']))\nsng.getdf(limit=2)\n</code></pre> id file_id category raw_text subject author tokenized_text 0 1 21379 soc.religion.christian From: kbanner@philae.sas.upenn.edu (Ken Banner... None None [From:, kbanner@philae.sas.upenn.edu, (Ken, Ba... 1 2 20874 soc.religion.christian From: simon@monu6.cc.monash.edu.au\\nSubject: S... None None [From:, simon@monu6.cc.monash.edu.au, Subject:..."},{"location":"legacy_documentation/legacy_adv/#extract-email-metadata","title":"Extract Email Metadata","text":"<p>This example takes it even further by using the \"email\" package to parse apart the blog files. It then uses the extracted information to populate the corresponding fields in the DocTable.</p> <pre><code>query = sng.get(sel=('file_id','raw_text',), asdict=False)\nfor fid,text in query:\n    e = email.message_from_string(text)\n    auth = e['From'] if 'From' in e.keys() else ''\n    subj = e['Subject'] if 'Subject' in e.keys() else ''\n    tok = e.get_payload().split()\n    dat = {\n        'tokenized_text':tok,\n        'author':auth,\n        'subject':subj,\n    }\n\n    sng.update(dat, 'file_id == {}'.format(fid))\nsng.getdf(limit=2)\n</code></pre> id file_id category raw_text subject author tokenized_text 0 1 21379 soc.religion.christian From: kbanner@philae.sas.upenn.edu (Ken Banner... Re: SATANIC TOUNGES kbanner@philae.sas.upenn.edu (Ken Banner) [In, article, &lt;May.5.02.53.10.1993.28880@athos... 1 2 20874 soc.religion.christian From: simon@monu6.cc.monash.edu.au\\nSubject: S... Saint Story St. Aloysius Gonzaga simon@monu6.cc.monash.edu.au [Heres, a, story, of, a, Saint, that, people, ... <pre><code>sng.getdf().to_csv('newsgroup20.csv')\n</code></pre>"},{"location":"legacy_documentation/legacy_simple/","title":"DocTable Simple Example","text":"<p>In this notebook, I show how to define a DocTable as a python class, populate the DocTable using the .add() and .addmany() commands, query data through generators and pandas dataframes, and finally update DocTable entries.</p> <pre><code>from pprint import pprint\nfrom timeit import default_timer as timer\nfrom .legacy_helper import get_sklearn_newsgroups # for this example\n\nimport sys\nsys.path.append('..')\nimport doctable as dt # this will be the table object we use to interact with our database.\n</code></pre>"},{"location":"legacy_documentation/legacy_simple/#get-news-data-from-sklearndatasets","title":"Get News Data From sklearn.datasets","text":"<p>Then parses into a dataframe.</p> <pre><code>ddf = get_sklearn_newsgroups()\nprint(ddf.info())\nddf.head(3)\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 11314 entries, 0 to 11313\nData columns (total 3 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   filename  11314 non-null  object\n 1   target    11314 non-null  object\n 2   text      11314 non-null  object\ndtypes: object(3)\nmemory usage: 265.3+ KB\nNone\n</code></pre> filename target text 0 21379 soc.religion.christian From: kbanner@philae.sas.upenn.edu (Ken Banner... 1 20874 soc.religion.christian From: simon@monu6.cc.monash.edu.au\\nSubject: S... 2 58936 sci.med From: jeffp@vetmed.wsu.edu (Jeff Parke)\\nSubje..."},{"location":"legacy_documentation/legacy_simple/#define-doctable-class","title":"Define DocTable Class","text":"<p>This class definition will contain the columns, datatypes, unique constraints, and index commands needed for your DocTable. In moving your data from DataFrame to DocTable, you should consider column data types and custom indices carefully.</p> <pre><code># this class will represent the doctable. It inherits from DocTable a number of add/query/remove functions.\n# of course, you can add any additional methods to this class definition as you find useful.\nclass SimpleNewsGroups(dt.DocTableLegacy):\n    def __init__(self, fname):\n        '''\n            This includes examples of init variables. See DocTable class for complete list of options.\n            Inputs:\n                fname: fname is the name of the new sqlite database that will be used for this class.\n        '''\n        tabname = 'simplenewsgroups'\n        super().__init__(\n            fname=fname, \n            tabname=tabname, \n            colschema=(\n                'id integer primary key autoincrement',\n                'file_id int',\n                'category string',\n                'raw_text string',\n            )\n        )\n\n        # this section defines any other commands that should be executed upon init\n        # NOTICE: references tabname defined in the above __init__ function\n        # extra commands to create index tables for fast lookup\n        self.query(\"create index if not exists idx1 on \"+tabname+\"(file_id)\")\n        self.query(\"create index if not exists idx2 on \"+tabname+\"(category)\")\n</code></pre> <p>Create a connection to the database by constructing an instance of the class. If this is the first time you've run this code, it will create a new sqlite database file with no entries.</p> <pre><code>sng = SimpleNewsGroups('simple_news_group.db')\nprint(sng)\n</code></pre> <pre><code>&lt;Documents ct: 0&gt;\n</code></pre>"},{"location":"legacy_documentation/legacy_simple/#adding-data","title":"Adding Data","text":"<p>There are two common ways to add data to your DocTable.</p> <p>(1) Add in rows individually</p> <p>(2) Add in bulk with or without specifying column names</p> <pre><code># adds data one row at a time. Takes longer than bulk version\nstart = timer()\n\nfor ind,dat in ddf.iterrows():\n    row = {'file_id':int(dat['filename']), 'category':dat['target'], 'raw_text':dat['text']}\n    sng.add(row, ifnotunique='replace')\n\nprint((timer() - start)*1000, 'mil sec.')\nprint(sng)\n</code></pre> <pre><code>1629.2546929325908 mil sec.\n&lt;Documents ct: 11314&gt;\n</code></pre> <pre><code># adds tuple data in bulk by specifying columns we are adding\nstart = timer()\n\ncol_order = ('file_id','category','raw_text')\ndata = [(dat['filename'],dat['target'],dat['text']) for ind,dat in ddf.iterrows()]\nsng.addmany(data,keys=col_order, ifnotunique='replace')\n\nprint((timer() - start)*1000, 'mil sec.')\nprint(sng)\n</code></pre> <pre><code>1269.7400120086968 mil sec.\n&lt;Documents ct: 22628&gt;\n</code></pre>"},{"location":"legacy_documentation/legacy_simple/#querying-data","title":"Querying Data","text":"<p>There are two primary ways of querying data from a DocTable:</p> <p>(1) retrieve one-by-one from generator using \".get()\" function. (2) retrieve all data in Pandas DataFrame suing \".getdf()\" function.</p> <pre><code>result = sng.get(\n    sel=('file_id','raw_text'), \n    where='category == \"rec.motorcycles\"', \n    orderby='file_id ASC', \n    limit=3,\n)\nfor row in result:\n    print(str(row['file_id'])+':', row['raw_text'][:50])\n</code></pre> <pre><code>72052: From: ivan@erich.triumf.ca (Ivan D. Reid)\nSubject:\n72052: From: ivan@erich.triumf.ca (Ivan D. Reid)\nSubject:\n101725: Subject: Re: Lexan Polish?\nFrom: jeff@mri.com (Jon\n</code></pre> <pre><code>result_df = sng.getdf(\n    sel=('file_id','raw_text'), \n    where='category == \"rec.motorcycles\"', \n    orderby='file_id ASC', \n    limit=5,\n)\nresult_df\n</code></pre> file_id raw_text 0 72052 From: ivan@erich.triumf.ca (Ivan D. Reid)\\nSub... 1 72052 From: ivan@erich.triumf.ca (Ivan D. Reid)\\nSub... 2 101725 Subject: Re: Lexan Polish?\\nFrom: jeff@mri.com... 3 101725 Subject: Re: Lexan Polish?\\nFrom: jeff@mri.com... 4 102616 From: blgardne@javelin.sim.es.com (Dances With..."},{"location":"legacy_documentation/legacy_simple/#updating-data-in-doctable","title":"Updating Data in DocTable","text":"<p>The \".update()\" function will change entries in the DocTable.</p> <pre><code>sng.update({'category':'nevermind',},where='file_id == \"103121\"')\nsng.getdf(where='file_id == \"103121\"') # to see update, look at \"category\" column entry\n</code></pre> id file_id category raw_text 0 395 103121 nevermind From: MJMUISE@1302.watstar.uwaterloo.ca (Mike ... 1 11709 103121 nevermind From: MJMUISE@1302.watstar.uwaterloo.ca (Mike ..."},{"location":"legacy_documentation/parse_parsepipeline/","title":"<code>ParsePipeline</code> Basics","text":"<p>Here I demonstrate the basics of parsing text using Spacy + doctable to tokenize text. Spacy does most of the heavy-lifting here to actually parse the document, and doctable methods handle the conversion from the Spacy Document object to a sequence of string tokens (words).</p> <pre><code>from IPython import get_ipython\nimport sys\nsys.path.append('..')\nimport doctable\n</code></pre> <pre><code>ex_texts = [\n    'I am pretty bored today. I have been stuck in quarantine for more than two months!',\n    'We are all waiting on Dr. Fauci to let us know when to return from quarantine.',\n    'On the bright side, I have more time to talk to my distant friends over video chat.',\n    'But still, I wish I could travel, go to bars, and go out to eat mrore often!',\n    'Here we show an example URL: https://devincornell.github.io/doctable/',\n    'And also one with &lt;b&gt;&lt;i&gt;xml tags&lt;/i&gt;&lt;/b&gt;.',\n]\n</code></pre>"},{"location":"legacy_documentation/parse_parsepipeline/#1-build-a-parsepipeline-for-tokenization","title":"1. Build a <code>ParsePipeline</code> for Tokenization","text":"<p><code>ParsePipeline</code> makes it easy to define a processing pipeline as a list of functions (called components) to apply sequentially to each document in your corpus. You can use the <code>.parsemany()</code> method to run the pipeline on documents in paralel, or simply use the <code>.parse()</code> method to parse a single document.</p> <p>Our most basic pipeline uses a lambda function to split each text document by whitespace.</p> <pre><code>parser_split = doctable.ParsePipeline([\n    lambda text: text.split(),\n])\n</code></pre> <p>We then use the <code>.parse()</code> method to apply the pipeline to a single document.</p> <pre><code>parsed_text = parser_split.parse(ex_texts[0])\nprint(parsed_text[:7])\n</code></pre> <pre><code>['I', 'am', 'pretty', 'bored', 'today.', 'I', 'have']\n</code></pre> <p>We can also use the <code>.parsemany()</code> method to parse all of our texts at once. Use the <code>workers</code> parameter to specify the number of processes to use if you want to use parallelization.</p> <pre><code>parsed_texts = parser_split.parsemany(ex_texts, workers=2) # processes in parallel\nfor text in parsed_texts:\n    print(text[:7])\n</code></pre> <pre><code>['I', 'am', 'pretty', 'bored', 'today.', 'I', 'have']\n['We', 'are', 'all', 'waiting', 'on', 'Dr.', 'Fauci']\n['On', 'the', 'bright', 'side,', 'I', 'have', 'more']\n['But', 'still,', 'I', 'wish', 'I', 'could', 'travel,']\n['Here', 'we', 'show', 'an', 'example', 'URL:', 'https://devincornell.github.io/doctable/']\n['And', 'also', 'one', 'with', '&lt;b&gt;&lt;i&gt;xml', 'tags&lt;/i&gt;&lt;/b&gt;.']\n</code></pre>"},{"location":"legacy_documentation/parse_parsepipeline/#2-use-doctable-parsing-components","title":"2. Use doctable Parsing Components","text":"<p>doctable has some built-in methods for pre- and post-processing Spacy documents. This list includes all functions in the doctable.parse namespace, and you can access them using the <code>doctable.Comp</code> function.</p> <pre><code>print(doctable.components)\n</code></pre> <pre><code>{'preprocess': &lt;function preprocess at 0x7fb23bc901f0&gt;, 'tokenize': &lt;function tokenize at 0x7fb23bc90310&gt;, 'parse_tok': &lt;function parse_tok at 0x7fb23bc903a0&gt;, 'keep_tok': &lt;function keep_tok at 0x7fb23bc90430&gt;, 'merge_tok_spans': &lt;function merge_tok_spans at 0x7fb23bc904c0&gt;, 'merge_tok_ngrams': &lt;function merge_tok_ngrams at 0x7fb23bc90550&gt;, 'get_parsetrees': &lt;function get_parsetrees at 0x7fb23bc90670&gt;}\n</code></pre> <pre><code>preproc = doctable.Comp('preprocess', replace_url='_URL_', replace_xml='')\nprint(ex_texts[4])\npreproc(ex_texts[4])\n</code></pre> <pre><code>Here we show an example URL: https://devincornell.github.io/doctable/\n\n\n\n\n\n'Here we show an example URL: _URL_'\n</code></pre> <p>Now we show a pipeline that uses the doctable <code>preprocess</code> method to remove xml tags and urls, the Spacy nlp model to parse the document, and the built-in <code>tokenize</code> method to convert the spacy doc object to a list of tokens. </p> <pre><code>from doctable import Comp\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\nparser_tok = doctable.ParsePipeline([\n    Comp('preprocess', replace_xml='', replace_url='XXURLXX'),\n    nlp,\n    Comp('tokenize', split_sents=False),\n])\n\ndocs = parser_tok.parsemany(ex_texts)\nfor doc in docs:\n    print(doc[:10])\n</code></pre> <pre><code>[I, am, pretty, bored, today, ., I, have, been, stuck]\n[We, are, all, waiting, on, Dr., Fauci, to, let, us]\n[On, the, bright, side, ,, I, have, more, time, to]\n[But, still, ,, I, wish, I, could, travel, ,, go]\n[Here, we, show, an, example, URL, :, XXURLXX]\n[And, also, one, with, xml, tags, .]\n</code></pre>"},{"location":"legacy_documentation/parse_parsepipeline/#3-more-complicated-pipelines","title":"3. More Complicated Pipelines","text":"<p>Now we show a more complicated mode. The function <code>tokenize</code> also takes two additional methods: <code>keep_tok_func</code> determines whether a Spacy token should be included in the final document, and the <code>parse_tok_func</code> determines how the spacy token objects should be converted to strings. We access the doctable <code>keep_tok</code> and <code>parse_tok</code> methods using the same <code>Comp</code> function to create nested parameter lists.</p> <pre><code>parser_full = doctable.ParsePipeline([\n\n    # preprocess to remove xml tags and replace URLs (doctable.parse.preprocess)\n    Comp('preprocess', replace_xml='', replace_url='XXURLXX'),\n    nlp, # spacy nlp parser object\n\n    # merge spacy multi-word named entities (doctable.parse.merge_tok_spans)\n    Comp('merge_tok_spans', merge_ents=True, merge_noun_chunks=False),\n\n    # tokenize document\n    Comp('tokenize', **{\n        'split_sents': False,\n\n        # choose tokens to keep (doctable.parse.keep_tok)\n        'keep_tok_func': Comp('keep_tok', **{\n            'keep_whitespace': False, # don't keep whitespace\n            'keep_punct': True, # keep punctuation and stopwords\n            'keep_stop': True,\n        }),\n\n        # choose how to convert Spacy token t text (doctable.parse.parse_tok)\n        'parse_tok_func': Comp('parse_tok', **{\n            'format_ents': True,\n            'lemmatize': False,\n            'num_replacement': 'NUM',\n            'ent_convert': lambda e: e.text.upper(), # function to capitalize named entities\n        })\n    })\n])\nlen(parser_full.components)\n</code></pre> <pre><code>4\n</code></pre> <pre><code>parsed_docs = parser_full.parsemany(ex_texts)\nfor tokens in parsed_docs:\n    print(tokens[:10])\n</code></pre> <pre><code>['i', 'am', 'pretty', 'bored', 'TODAY', '.', 'i', 'have', 'been', 'stuck']\n['we', 'are', 'all', 'waiting', 'on', 'dr.', 'FAUCI', 'to', 'let', 'us']\n['on', 'the', 'bright', 'side', ',', 'i', 'have', 'more', 'time', 'to']\n['but', 'still', ',', 'i', 'wish', 'i', 'could', 'travel', ',', 'go']\n['here', 'we', 'show', 'an', 'example', 'url', ':', 'xxurlxx']\n['and', 'also', 'NUM', 'with', 'xml', 'tags', '.']\n</code></pre> <p>These are the fundamentals of building <code>ParsePipeline</code>s in doctable. While these tools are totally optional, I believe they make it easier to structure your code for text analysis applications.</p>"},{"location":"legacy_documentation/parse_parsetrees/","title":"Working with doctable Parsetrees","text":"<p>Here I'll show you how to extract and use parsetrees in your doctable using Spacy + doctable. The motivation is that parsetree information in raw Spacy Document objects are very large and not suitable for storage when using large corpora. We solve this by simply converting the Spacy Document object to a tree data structure built from python lists and dictionaries, and use the <code>ParseTree</code> object to serialize, de-serialize, and interact with the tree structure.</p> <p>We use this feature using the <code>get_parsetrees</code> pipeline component after the spacy parser. Check the docs to learn more about this function. You can see more examples of creating parse pipelines in our overview examples.</p> <pre><code>import spacy\nnlp = spacy.load('en_core_web_sm')\nimport pandas as pd\nimport sys\nsys.path.append('..')\nimport doctable\n</code></pre> <p>First we define some example text docuemnts, Star Wars themed.</p> <pre><code>text = 'Help me Obi-Wan Kenobi. You\u2019re my only hope. ' \\\n    'I find your lack of faith disturbing. ' \\\n    'Do, or do not - there is no try. '\ntext\n</code></pre> <pre><code>'Help me Obi-Wan Kenobi. You\u2019re my only hope. I find your lack of faith disturbing. Do, or do not - there is no try. '\n</code></pre>"},{"location":"legacy_documentation/parse_parsetrees/#creating-parsetreedoc-objects","title":"Creating <code>ParseTreeDoc</code> Objects","text":"<p>The most direct way of creating a parsetree is to parse the desired text using the spacy language model, then use <code>ParseTreeDoc.from_spacy()</code> to construct the <code>ParseTreeDoc</code>. The <code>ParseTreeDoc</code> object is a container for parsetree objects representing each of the sentences identified with the SpaCy parser.</p> <pre><code>spacydoc = nlp(text)\ndoc = doctable.ParseTreeDoc.from_spacy(spacydoc)\nprint(f'{len(doc)} sentences of type {type(doc)}')\n</code></pre> <pre><code>4 sentences of type &lt;class 'doctable.parse.documents.parsetreedoc.ParseTreeDoc'&gt;\n</code></pre> <p>The most important arguments to <code>parse_tok_func</code> are <code>text_parse_func</code> and <code>userdata_map</code>.</p> <ol> <li> <p><code>text_parse_func</code> determines the mapping from a spacy doc object to the text representation of each token accessed through <code>token.text</code>. By default this parameter is set to <code>lambda d: d.text</code>.</p> </li> <li> <p><code>userdata_map</code> is a dictionary mapping an attribute name to a function. You can, for instance, extract info from the original spacy doc object through this method. I'll explain later how these attributes can be accessed and used.</p> </li> </ol> <pre><code>doc = doctable.ParseTreeDoc.from_spacy(spacydoc, \n    text_parse_func=lambda spacydoc: spacydoc.text,\n    userdata_map = {\n        'lower': lambda spacydoc: spacydoc.text.lower().strip(),\n        'lemma': lambda spacydoc: spacydoc.lemma_,\n        'like_num': lambda spacydoc: spacydoc.like_num,\n    }\n)\nprint(doc[0]) # show the first sentence\n</code></pre> <pre><code>ParseTree(Help me Obi - Wan Kenobi .)\n</code></pre>"},{"location":"legacy_documentation/parse_parsetrees/#working-with-parsetrees","title":"Working With <code>ParseTree</code>s","text":"<p><code>ParseTreeDoc</code> objects represent sequences of <code>ParseTree</code> objects identified by the spacy language parser. You can see we can access individual sentence parsetrees using numerical indexing or through iteration.</p> <pre><code>print(doc[0])\nfor sent in doc:\n    print(sent)\n</code></pre> <pre><code>ParseTree(Help me Obi - Wan Kenobi .)\nParseTree(Help me Obi - Wan Kenobi .)\nParseTree(You \u2019re my only hope .)\nParseTree(I find your lack of faith disturbing .)\nParseTree(Do , or do not - there is no try .)\n</code></pre> <p>Now we will show how to work with <code>ParseTree</code> objects. These objects are collections of tokens that can be accessed either as a tree (based on the structure of the dependency tree produced by spacy), or as an ordered sequence. We can use numerical indexing or iteration to interact with individual tokens.</p> <pre><code>for token in doc[0]:\n    print(token)\n</code></pre> <pre><code>Help\nme\nObi\n-\nWan\nKenobi\n.\n</code></pre> <p>We can work with the tree structure of a <code>ParseTree</code> object using the <code>root</code> property.</p> <pre><code>print(doc[0].root)\n</code></pre> <pre><code>Help\n</code></pre> <p>And access the children of a given token using the <code>childs</code> property. The following tokens are children of the root token.</p> <pre><code>for child in doc[0].root.childs:\n    print(child)\n</code></pre> <pre><code>me\nKenobi\n.\n</code></pre> <p>These objects can be serialized using the <code>.as_dict()</code> method and de-serialized using the <code>.from_dict()</code> method.</p> <pre><code>serialized = doc.as_dict()\ndeserialized = doctable.ParseTreeDoc.from_dict(serialized)\nfor sent in deserialized:\n    print(sent)\n</code></pre> <pre><code>ParseTree(Help me Obi - Wan Kenobi .)\nParseTree(You \u2019re my only hope .)\nParseTree(I find your lack of faith disturbing .)\nParseTree(Do , or do not - there is no try .)\n</code></pre>"},{"location":"legacy_documentation/parse_parsetrees/#more-about-tokens","title":"More About <code>Token</code>s","text":"<p>Each token in a <code>ParseTree</code> is represented by a <code>Token</code> object. These objects maintain the tree structure of a parsetree, and each node contains some default information as well as optional and custom information. These are the most important member variables:</p>"},{"location":"legacy_documentation/parse_parsetrees/#member-variables","title":"Member Variables","text":"<ul> <li>i: index of token in sentence</li> <li>text: text representation of token</li> <li>tag: the part-of-speech tag offered by the dependency parser (different from POS tagger)</li> <li>dep: the dependency relation to parent object. See the Spacy annotation docs for more detail.</li> <li>parent: reference to parent node</li> <li>childs: list of references to child nodes</li> </ul>"},{"location":"legacy_documentation/parse_parsetrees/#optional-member-variables","title":"Optional Member Variables","text":"<p>The following are provided if the associated spacy parser component was enabled.</p> <ul> <li>pos: part-of-speech tag created if user enablled POS 'tagger' in Spacy. See Spacy POS tag docs for more detail. Also check out docs for UPOS tags.</li> <li>ent: named entity type of token (if NER was enabled when creating parsetree). See Spacy NER docs for more detail.</li> </ul> <pre><code>for tok in doc[0][:3]:\n    print(f\"{tok.text}:\\n\\ti={tok.i}\\n\\ttag={tok.tag}\\n\\tdep={tok.dep}\\n\\tent={tok.ent}\\n\\tpos={tok.pos}\")\n</code></pre> <pre><code>Help:\n    i=0\n    tag=VB\n    dep=ROOT\n    ent=\n    pos=VERB\nme:\n    i=1\n    tag=PRP\n    dep=dobj\n    ent=\n    pos=PRON\nObi:\n    i=2\n    tag=NNP\n    dep=compound\n    ent=\n    pos=PROPN\n</code></pre> <p>We can also access the custom token properties provided to the <code>ParseTreeDoc.from_spacy()</code> method earlier.</p> <pre><code>for token in doc[0]:\n    print(f\"{token.text}: {token['lemma']}\")\n</code></pre> <pre><code>Help: help\nme: I\nObi: Obi\n-: -\nWan: Wan\nKenobi: Kenobi\n.: .\n</code></pre>"},{"location":"legacy_documentation/parse_parsetrees/#recursive-functions-on-parsetrees","title":"Recursive Functions on Parsetrees","text":"<p>We can also navigate the tree structure of parsetrees using recursive functions. Here I simply print out the trajectory of this recursive function.</p> <pre><code>def print_recursion(tok, level=0):\n    if not tok.childs:\n        print('    '*level + 'base node', tok)\n    else:\n        print('    '*level + 'entering', tok)\n        for child in tok.childs:\n            print_recursion(child, level+1)\n        print('    '*level + 'leaving', tok)\n\nprint_recursion(doc[0].root)\n</code></pre> <pre><code>entering Help\n    base node me\n    entering Kenobi\n        entering Wan\n            base node Obi\n            base node -\n        leaving Wan\n    leaving Kenobi\n    base node .\nleaving Help\n</code></pre>"},{"location":"legacy_documentation/parse_parsetrees/#create-using-parsepipelines","title":"Create Using <code>ParsePipeline</code>s","text":"<p>The most common use case, however, probably involves the creation of of a <code>ParsePipeline</code> in which the end result will be a <code>ParseTreeDoc</code>. We make this using the <code>get_parsetrees</code> pipeline component, and here we show several of the possible arguments.</p> <pre><code>parser = doctable.ParsePipeline([\n    nlp, # the spacy parser\n    doctable.Comp('get_parsetrees', **{\n        'text_parse_func': lambda spacydoc: spacydoc.text,\n        'userdata_map': {\n            'lower': lambda spacydoc: spacydoc.text.lower().strip(),\n            'lemma': lambda spacydoc: spacydoc.lemma_,\n            'like_num': lambda spacydoc: spacydoc.like_num,\n        }\n    })\n])\nparser.components\n</code></pre> <pre><code>[&lt;spacy.lang.en.English at 0x7f3d584c7dc0&gt;,\n functools.partial(&lt;function get_parsetrees at 0x7f3d27b95dc0&gt;, text_parse_func=&lt;function &lt;lambda&gt; at 0x7f3d26a17a60&gt;, userdata_map={'lower': &lt;function &lt;lambda&gt; at 0x7f3d26a17160&gt;, 'lemma': &lt;function &lt;lambda&gt; at 0x7f3d26a170d0&gt;, 'like_num': &lt;function &lt;lambda&gt; at 0x7f3d26a17040&gt;})]\n</code></pre> <p>You can see that the parser provides the same output as we got before with <code>ParseTreeDoc.from_spacy()</code>.</p> <pre><code>for sent in parser.parse(text):\n    print(sent)\n</code></pre> <pre><code>ParseTree(Help me Obi - Wan Kenobi .)\nParseTree(You \u2019re my only hope .)\nParseTree(I find your lack of faith disturbing .)\nParseTree(Do , or do not - there is no try .)\n</code></pre>"}]}