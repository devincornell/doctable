{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser Pipeline Basics\n",
    "Parsing texts is usually the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import doctable\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import doctable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I. An American Perspective ',\n",
       " 'In the early days of this Administration we laid the foundation for a more constructive and positive American role in world affairs by clarifying the essential elements of U.S. foreign and defense policy. ',\n",
       " \"Over the intervening years, we have looked objectively at our policies and performance on the world scene to ensure they reflect the dynamics of a complex and ever-changing world . Where course adjustments have been required, I have directed changes. But we have not veered and will not veer from the broad aims that guide America's leadership role in today's world: \"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download data for examples\n",
    "from util import download_nss # for downloading nss text documents from my github repo\n",
    "nss = download_nss(years=[1987])[1987].split('\\n\\n') # download nss and split paragraphs\n",
    "nss[:3] # list of paragraph strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a ParsePipeline\n",
    "A pipeline is simply a list of functions (called components) to apply sequentially on each element of your data.\n",
    "\n",
    "Pipeline components from doctable are functions that lie in `doctable.parse`, and can be accessed using `doctable.parse.<function_name>`. You can also use the `doctable.component` function as a shortcut to access those functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function doctable.pipeline.component.<locals>.<lambda>(x)>,\n",
       " <spacy.lang.en.English at 0x7f614d07aba8>,\n",
       " <function doctable.pipeline.component.<locals>.<lambda>(x)>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy # for example text processing\n",
    "nlp = spacy.load('en') # nlp is function for parsing with spacy\n",
    "\n",
    "from doctable import component # shortcut to functions in doctable.parse.<functions>\n",
    "\n",
    "pipeline = doctable.ParsePipeline([\n",
    "    component('preprocess', replace_xml=''), # preprocess to remove xml tags (doctable.parse.preprocess)\n",
    "    nlp, # spacy nlp parser object\n",
    "    component('tokenize', split_sents=False), # add tokenizer component (doctable.parse.tokenize)\n",
    "])\n",
    "pipeline.components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I., An, American, Perspective]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.parse(nss[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Components For Pipeline\n",
    "Because some of the component functions in `doctable.parse` take more functions as arguments, these components can be nested. Consider the `tokenize` function, which takes an argument `keep_tok_func` for deciding whether to keep a spacy token in the final output and an argument `parse_tok_func` to convert a Spacy token object into a string. The doctable functions `keep_tok` and `parse_tok` have some useful settings that we'll set in this next pipeline exmaple. We'll also use the doctable function `merge_tok_spans` to combine multi-word entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function doctable.pipeline.component.<locals>.<lambda>(x)>,\n",
       " <spacy.lang.en.English at 0x7f614d07aba8>,\n",
       " <function doctable.pipeline.component.<locals>.<lambda>(x)>,\n",
       " <function doctable.pipeline.component.<locals>.<lambda>(x)>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from doctable import component # shortcut to functions in doctable.parse.<functions>\n",
    "\n",
    "pipeline = doctable.ParsePipeline([\n",
    "    component('preprocess', replace_xml=''), # preprocess to remove xml tags (doctable.parse.preprocess)\n",
    "    nlp, # spacy nlp parser object\n",
    "    component('merge_tok_spans', merge_ents=True, merge_noun_chunks=False),\n",
    "    component('tokenize', **{\n",
    "        'split_sents': False,\n",
    "        'keep_tok_func': component('keep_tok', **{\n",
    "            'keep_whitespace': False, # don't keep whitespace\n",
    "            'keep_punct': True, # keep punctuation and stopwords\n",
    "            'keep_stop': True,\n",
    "        }),\n",
    "        'parse_tok_func': component('parse_tok', **{\n",
    "            'format_ents': True,\n",
    "            'lemmatize': False,\n",
    "            'num_replacement': 'NUM',\n",
    "            'ent_convert': lambda e: e.text.upper(), # function to capitalize named entities\n",
    "        })\n",
    "    })\n",
    "])\n",
    "pipeline.components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over the intervening years, we have looked objectively at our policies and performance on the world scene to ensure they reflect the dynamics of a complex and ever-changing world . Where course adjustments have been required, I have directed changes. But we have not veered and will not veer from the broad aims that guide America's leadership role in today's world: \n",
      "\n",
      "['over', 'THE INTERVENING YEARS', ',', 'we', 'have', 'looked', 'objectively', 'at', 'our', 'policies', 'and', 'performance', 'on', 'the', 'world', 'scene', 'to', 'ensure', 'they', 'reflect', 'the', 'dynamics', 'of', 'a', 'complex', 'and', 'ever', '-', 'changing', 'world', '.', 'where', 'course', 'adjustments', 'have', 'been', 'required', ',', 'i', 'have', 'directed', 'changes', '.', 'but', 'we', 'have', 'not', 'veered', 'and', 'will', 'not', 'veer', 'from', 'the', 'broad', 'aims', 'that', 'guide', 'AMERICA', \"'s\", 'leadership', 'role', 'in', 'TODAY', \"'s\", 'world', ':']\n"
     ]
    }
   ],
   "source": [
    "print(nss[2])\n",
    "print()\n",
    "print(pipeline.parse(nss[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Using Multiprocessing\n",
    "The `ParsePipeline` class also allows you to parse multiple texts at once in parallel using the `doctable.Distribute()` module. Simply use the `.parsemany()` method to access this feature. It can give huge performance gains when used in the right places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.41 s, sys: 2.49 ms, total: 2.41 s\n",
      "Wall time: 2.41 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I. AN AMERICAN', 'perspective']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time parsed = pipeline.parsemany(nss[:100], workers=1)\n",
    "parsed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.71 ms, sys: 33.4 ms, total: 40.1 ms\n",
      "Wall time: 556 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I. AN AMERICAN', 'perspective']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UNDER CERTAIN CONDITIONS THIS FAILS - IT FAILS IN THE FIRST EXAMPLE HERE, but not the last\n",
    "%time parsed = pipeline.parsemany(nss[:100], workers=5)\n",
    "parsed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Functions in Pipeline\n",
    "Because a pipeline is just a list of function components, it's easy to add components after creating a pipeline or simply "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IN', 'THE', 'EARLY', 'DAYS', 'OF', 'THIS', 'ADMINISTRATION', 'WE', 'LAID', 'THE', 'FOUNDATION', 'FOR', 'A', 'MORE', 'CONSTRUCTIVE', 'AND', 'POSITIVE', 'AMERICAN', 'ROLE', 'IN', 'WORLD', 'AFFAIRS', 'BY', 'CLARIFYING', 'THE', 'ESSENTIAL', 'ELEMENTS', 'OF', 'U.S.', 'FOREIGN', 'AND', 'DEFENSE', 'POLICY.']\n"
     ]
    }
   ],
   "source": [
    "pipeline = doctable.ParsePipeline([\n",
    "    lambda x: x.upper(),\n",
    "    lambda x: x.split(),\n",
    "])\n",
    "print(pipeline.parse(nss[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
