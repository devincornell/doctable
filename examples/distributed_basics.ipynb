{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Distribute` Parallel Processing Basics\n",
    "Due to a number of limitations involving data passed to processes using `multiprocessing.Pool()`, I've implemented a similar class called `Distribute()`. The primary difference is that Distribute is meant to distribute chunks of data for parallel processing, so your map function should parse multiple values. There are currently two functions in Distribute:\n",
    "\n",
    "* `.map_chunk()` simply applies a function to a list of elements and returns a list of parsed elements.\n",
    "* `.map()` applies a function to a single element. Same as `multiprocessing.Pool().map()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython import get_ipython\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import doctable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.map()` Method\n",
    "Unsurprisingly, this method works exactly like the `multiprocessing.Pool().map()`. Simply provide a sequence of elements and a function to apply to them, and this method will parse all the elements in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8]\n",
      "[0, 5, 10, 15, 20]\n"
     ]
    }
   ],
   "source": [
    "def multiply(x, y=2):\n",
    "    return x * y\n",
    "\n",
    "nums = list(range(5))\n",
    "with doctable.Distribute(3) as d:\n",
    "    res = d.map(multiply, nums)\n",
    "    print(res)\n",
    "    \n",
    "    # pass any argument to your function here.\n",
    "    # we try multiplying by 5 instead of 2.\n",
    "    res = d.map(multiply, nums, 5)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.map_chunk()` Method\n",
    "Allows you to write map functions that processes a chunk of your data at a time. This is the lowest-level method for distributed processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.92 ms, sys: 13.3 ms, total: 19.2 ms\n",
      "Wall time: 22.7 ms\n",
      "CPU times: user 63 µs, sys: 85 µs, total: 148 µs\n",
      "Wall time: 154 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 1.275, 2.55]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map function to multiply 1.275 by each num and return a list\n",
    "def multiply_nums(nums):\n",
    "    return [num*1.275 for num in nums]\n",
    "\n",
    "# use Distribute(3) to create three separate processes\n",
    "nums = list(range(1000))\n",
    "with doctable.Distribute(3) as d:\n",
    "    %time res = d.map_chunk(multiply_nums, nums)\n",
    "\n",
    "# won't create new process at all. good for testing\n",
    "with doctable.Distribute(1) as d:\n",
    "    %time res = d.map_chunk(multiply_nums, nums)\n",
    "res[:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
