{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Parsing\n",
    "These examples will show how to use the DocParser class to make a parallelized parsing pipeline. Most of these features are in two methods:\n",
    "\n",
    "\n",
    "* **`.distribute_parse()`**: A text processing-specific method that will distribute texts among processors, apply a supplied preprocssing function, process using spacy's nlp.pipe(), and then parse to a non-spacy object using a supplied function. If a dt_inst keyword argument is defined, your supplied function may additionally insert the parsed result into the DocTable directly from the parsing process.\n",
    "\n",
    "* **`.distribute_process()`**: A more general function that allows you to distribute parsing of any element list and then if supplied with dt_inst can insert into the DocTable. This is ideal for cases where you do not want to store all texts into memory at once (as `distribute_parse()` requires, and could instead supply a list of filenames that then could be read, preprocessed, postprocessed, and inserted into the database all within the distributed processes.\n",
    "\n",
    "* **`.distribute_chunks()`**: A function that creates processes and allows you to provide a function which operates on a chunk of the provided elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "from spacy.matcher import Matcher\n",
    "from pprint import pprint\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import doctable as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_small = ['The hat is red. And so are you.\\n\\nWhatever, they said. Whatever indeed.', \n",
    "               'But why is the hat blue?\\n\\nAre you colorblind? See the answer here: http://google.com']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.distribute_parse()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'hat', 'is', 'red', '.', 'and', 'so', 'are', 'you', '.', 'whatever', ',', 'they', 'said', '.', 'whatever', 'indeed', '.'], ['but', 'why', 'is', 'the', 'hat', 'blue', '?', 'are', 'you', 'colorblind', '?', 'see', 'the', 'answer', 'here', ':', 'http://google.com']]\n"
     ]
    }
   ],
   "source": [
    "# this is the straightforward mode where it tokenizes each doc in parallel\n",
    "# using default document preprocessing and parsing\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['the', 'hat', 'is', 'red', '.', 'and', 'so', 'are', 'you', '.'], ['whatever', ',', 'they', 'said', '.', 'whatever', 'indeed', '.']], [['but', 'why', 'is', 'the', 'hat', 'blue', '?'], ['are', 'you', 'colorblind', '?', 'see', 'the', 'answer', 'here', ':', 'http://google.com']]]\n"
     ]
    }
   ],
   "source": [
    "# use paragraph_sep to maintain paragraph information\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, paragraph_sep='\\n\\n')\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['the', 'hat', 'is', 'red', '.'],\n",
      "  ['and', 'so', 'are', 'you', '.'],\n",
      "  ['whatever', ',', 'they', 'said', '.'],\n",
      "  ['whatever', 'indeed', '.']],\n",
      " [['but', 'why', 'is', 'the', 'hat', 'blue', '?'],\n",
      "  ['are'],\n",
      "  ['you', 'colorblind', '?'],\n",
      "  ['see', 'the', 'answer', 'here', ':']]]\n"
     ]
    }
   ],
   "source": [
    "# this shows an exmple where it will customize every element of the process\n",
    "def preprocess(text): return dt.DocParser.preprocess(text, replace_url='')\n",
    "def use_token_overload(tok): return dt.DocParser.use_tok(tok, filter_stop=False, filter_punct=False)\n",
    "def parse_token_overload(tok): return dt.DocParser.parse_tok(tok, lemmatize=False)\n",
    "def parsefunc(doc): return dt.DocParser.tokenize_doc(doc, use_tok_func=use_token_overload, parse_tok_func=parse_token_overload, split_sents=True)\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, parsefunc=parsefunc, preprocessfunc=preprocess)\n",
    "pprint(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse and insert into database\n",
    "The best part about all of these methods is that you can place them into a doctable directly, rather than returning them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              tokens\n",
      "0  [the, hat, is, red, ., and, so, are, you, ., w...\n",
      "1  [but, why, is, the, hat, blue, ?, are, you, co...\n"
     ]
    }
   ],
   "source": [
    "def parse_insert(doc, db):\n",
    "    toks = dt.DocParser.tokenize_doc(doc)\n",
    "    db.insert({'tokens':toks})\n",
    "db = dt.DocTable(schema=[('pickle','tokens')], fname='t12.db')\n",
    "db.delete() # empty if it had some rows\n",
    "dt.DocParser.distribute_parse(texts_small, nlp, dt_inst=db, parsefunc=parse_insert)\n",
    "print(db.select_df())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With parsetrees or arbitrary objects\n",
    "The fact that you can pass custom parsers means you can also use parsetrees or any other custom document representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ParseTree(['the', 'hat', 'is', 'red', '.']),\n",
      "  ParseTree(['and', 'so', 'are', 'you', '.', '']),\n",
      "  ParseTree(['whatever', ',', 'they', 'said', '.']),\n",
      "  ParseTree(['whatever', 'indeed', '.'])],\n",
      " [ParseTree(['but', 'why', 'is', 'the', 'hat', 'blue', '?', '']),\n",
      "  ParseTree(['are']),\n",
      "  ParseTree(['you', 'colorblind', '?']),\n",
      "  ParseTree(['see', 'the', 'answer', 'here', ':'])]]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text): return dt.DocParser.preprocess(text, replace_url='')\n",
    "def parse_token_overload(tok): return dt.DocParser.parse_tok(tok, lemmatize=False)\n",
    "def parsefunc(doc): return dt.DocParser.get_parsetrees(doc, parse_tok_func=parse_token_overload)\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, parsefunc=parsefunc, preprocessfunc=preprocess)\n",
    "pprint(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 18]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pf(doc):\n",
    "    return len(doc)\n",
    "dt.DocParser.distribute_parse(texts_small, nlp, parsefunc=pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 8], [7, 10]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pf(doc):\n",
    "    return len(doc)\n",
    "dt.DocParser.distribute_parse(texts_small, nlp, parsefunc=pf, paragraph_sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.distribute_process()`\n",
    "This method has fewer features than `.distribute_parse()` because you cannot control the outer `nlp.pipe()` call that wraps chunk processing, but using this method you can concurrently process lists of any elements, and similarily add result objects into a DocTable directly. This is a good option for when you are training large models or would like more control over your text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this shows the general case where you can parse any element\n",
    "def multiply(num):\n",
    "    return num*2\n",
    "elements = list(range(10))\n",
    "dt.DocParser.distribute_process(multiply, elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num\n",
       "0    0\n",
       "1    2\n",
       "2    4\n",
       "3    6\n",
       "4    8\n",
       "5   10\n",
       "6   12\n",
       "7   14\n",
       "8   16\n",
       "9   18"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also store into a db\n",
    "def multiply_sp(num, db):\n",
    "    db.insert({'num':num*2})\n",
    "elements = list(range(10))\n",
    "db = dt.DocTable(schema=[('integer','num')], fname='t1.db')\n",
    "db.delete()\n",
    "dt.DocParser.distribute_process(multiply_sp, elements, dt_inst=db)\n",
    "db.select_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 8, 12, 16, 20, 24, 28, 32, 36]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can also add additional ordered args to pass to the function\n",
    "def multiply_sp(num, mult):\n",
    "    return num*mult\n",
    "elements = list(range(10))\n",
    "dt.DocParser.distribute_process(multiply_sp, elements, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[The, hat, is, red, ., And, so, are, you, ., \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[But, why, is, the, hat, blue, ?, \\n\\n, Are, y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens\n",
       "0  [The, hat, is, red, ., And, so, are, you, ., \\...\n",
       "1  [But, why, is, the, hat, blue, ?, \\n\\n, Are, y..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now additional args plus the database\n",
    "def cust_tokenize(text, db, nlp):\n",
    "    doc = nlp(text)\n",
    "    db.insert({'tokens':[t.text for t in doc]})\n",
    "\n",
    "db = dt.DocTable(schema=[('pickle','tokens')], fname='tcustok.db')\n",
    "db.delete() # just in case had prev elements\n",
    "dt.DocParser.distribute_process(cust_tokenize, texts_small, nlp, dt_inst=db)\n",
    "db.select_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.distribute_chunks()`\n",
    "This function provides the least functionality and the most flexibility. It is used by both the other two functions. The method passed to this function will operate on a chunk of elements rather than a single element. If it is desirable to enter data into a DocTable, this must be handeled manually. The provided function must return a list of the same size as the chunk size it was given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.23 ms, sys: 7.57 ms, total: 11.8 ms\n",
      "Wall time: 32.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 1.275, 2.55]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now additional args plus the database\n",
    "def muli_multi(nums):\n",
    "    return [num*1.275 for num in nums]\n",
    "\n",
    "nums = list(range(1000))\n",
    "%time res = dt.DocParser.distribute_chunks(muli_multi, nums, workers=1)\n",
    "res[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.44 ms, sys: 13.6 ms, total: 15.1 ms\n",
      "Wall time: 64.6 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[the, hat, is, red, ., and, so, are, you, ., w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[but, why, is, the, hat, blue, ?, are, you, co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens\n",
       "0  [the, hat, is, red, ., and, so, are, you, ., w...\n",
       "1  [but, why, is, the, hat, blue, ?, are, you, co..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can see that passing the database instance to the worker\n",
    "def parse_texts_chunk(texts, db, nlp):\n",
    "    for doc in nlp.pipe(texts):\n",
    "        toks = dt.DocParser.tokenize_doc(doc)\n",
    "        db.insert({'tokens':toks})\n",
    "\n",
    "nums = list(range(1000))\n",
    "db = dt.DocTable(schema=[('pickle','tokens')], fname='parse_text_chunk.db')\n",
    "db.delete()\n",
    "%time res = dt.DocParser.distribute_chunks(parse_texts_chunk, texts_small, db, nlp, workers=2)\n",
    "res[:3]\n",
    "db.select_df()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
