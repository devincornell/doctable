{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Parsing\n",
    "These examples will show how to use the DocParser class to make a parallelized pipeline. The `.distribute_parse()` method is the most important aspect of these features, and it is used to parallelize parsing tasks across many CPUs. Generally the workflow goes that you define your functions for tokenization, often times simply overloading parameters on the DocParser functions of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "from spacy.matcher import Matcher\n",
    "from pprint import pprint\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import doctable as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_small = ['The hat is red. And so are you.\\n\\nWhatever, they said. Whatever indeed.', \n",
    "               'But why is the hat blue?\\n\\nAre you colorblind? See the answer here: http://google.com']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.distribute_parse()`\n",
    "This method will automatically distribute parsing jobs across multiple CPUs for maximum concurrency. It can also break up paragraphs to maintain maximum ammount of information from the original texts. It also allows you to pass custom preprocessors and parser functions that wil be passed to each process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'hat', 'is', 'red', '.', 'and', 'so', 'are', 'you', '.', 'whatever', ',', 'they', 'said', '.', 'whatever', 'indeed', '.'], ['but', 'why', 'is', 'the', 'hat', 'blue', '?', 'are', 'you', 'colorblind', '?', 'see', 'the', 'answer', 'here', ':', 'http://google.com']]\n"
     ]
    }
   ],
   "source": [
    "# this is the straightforward mode where it tokenizes each doc in parallel\n",
    "# using default document preprocessing and parsing\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['the', 'hat', 'is', 'red', '.', 'and', 'so', 'are', 'you', '.'], ['whatever', ',', 'they', 'said', '.', 'whatever', 'indeed', '.']], [['but', 'why', 'is', 'the', 'hat', 'blue', '?'], ['are', 'you', 'colorblind', '?', 'see', 'the', 'answer', 'here', ':', 'http://google.com']]]\n"
     ]
    }
   ],
   "source": [
    "# use paragraph_sep to maintain paragraph information\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, paragraph_sep='\\n\\n')\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['the', 'hat', 'is', 'red', '.'],\n",
      "  ['and', 'so', 'are', 'you', '.'],\n",
      "  ['whatever', ',', 'they', 'said', '.'],\n",
      "  ['whatever', 'indeed', '.']],\n",
      " [['but', 'why', 'is', 'the', 'hat', 'blue', '?'],\n",
      "  ['are'],\n",
      "  ['you', 'colorblind', '?'],\n",
      "  ['see', 'the', 'answer', 'here', ':']]]\n"
     ]
    }
   ],
   "source": [
    "# this shows an exmple where it will customize every element of the process\n",
    "def preprocess(text): return dt.DocParser.preprocess(text, replace_url='')\n",
    "def use_token_overload(tok): return dt.DocParser.use_tok(tok, filter_stop=False, filter_punct=False)\n",
    "def parse_token_overload(tok): return dt.DocParser.parse_tok(tok, lemmatize=False)\n",
    "def parsefunc(doc): return dt.DocParser.tokenize_doc(doc, use_tok_func=use_token_overload, parse_tok_func=parse_token_overload, split_sents=True)\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, parsefunc=parsefunc, preprocessfunc=preprocess, verbose=False)\n",
    "pprint(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With parsetrees\n",
    "The fact that you can pass custom parsers means you can also use parsetrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ParseTree(['the', 'hat', 'is', 'red', '.']),\n",
      "  ParseTree(['and', 'so', 'are', 'you', '.', '']),\n",
      "  ParseTree(['whatever', ',', 'they', 'said', '.']),\n",
      "  ParseTree(['whatever', 'indeed', '.'])],\n",
      " [ParseTree(['but', 'why', 'is', 'the', 'hat', 'blue', '?', '']),\n",
      "  ParseTree(['are']),\n",
      "  ParseTree(['you', 'colorblind', '?']),\n",
      "  ParseTree(['see', 'the', 'answer', 'here', ':'])]]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text): return dt.DocParser.preprocess(text, replace_url='')\n",
    "def parse_token_overload(tok): return dt.DocParser.parse_tok(tok, lemmatize=False)\n",
    "def parsefunc(doc): return dt.DocParser.get_parsetrees(doc, parse_tok_func=parse_token_overload)\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, parsefunc=parsefunc, preprocessfunc=preprocess, verbose=False)\n",
    "pprint(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Parsing\n",
    "Use the `.distribute_parse()` method to process many documents in parallel. All of the same tokenization function settings can be passed, as we will show.\n",
    "\n",
    "By default, this method will use the built-in DocParser methods for tokenization, etc. But these can be overloaded and overridden to provide more robust functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing 2 docs\n",
      "processing chunks of size 1 with 2 processes.\n",
      "returned 2 parsed docs or paragraphs\n",
      "[['the', 'hat', 'is', 'red', '.', 'and', 'so', 'are', 'you', '.', 'whatever', ',', 'they', 'said', '.', 'whatever', 'indeed', '.'], ['but', 'why', 'is', 'the', 'hat', 'blue', '?', 'are', 'you', 'colorblind', '?', 'see', 'the', 'answer', 'here', ':', 'http://google.com']]\n"
     ]
    }
   ],
   "source": [
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, verbose=True)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing 2 docs\n",
      "processing chunks of size 1 with 4 processes.\n",
      "returned 4 parsed docs or paragraphs\n",
      "[[['the', 'hat', 'is', 'red', '.', 'and', 'so', 'are', 'you', '.'], ['whatever', ',', 'they', 'said', '.', 'whatever', 'indeed', '.']], [['but', 'why', 'is', 'the', 'hat', 'blue', '?'], ['are', 'you', 'colorblind', '?', 'see', 'the', 'answer', 'here', ':', 'http://google.com']]]\n"
     ]
    }
   ],
   "source": [
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, verbose=True, paragraph_sep='\\n\\n')\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'hat', 'is', 'red', '.', 'wutwutwut', 'so', 'are', 'you', '.', 'whatever', ',', 'they', 'said', '.', 'whatever', 'indeed', '.'], ['but', 'why', 'is', 'the', 'hat', 'blue', '?', 'are', 'you', 'colorblind', '?', 'see', 'the', 'answer', 'here', ':', 'http://google.com']]\n"
     ]
    }
   ],
   "source": [
    "# provide a custom preprocess function.\n",
    "def preprocess(text):\n",
    "    return text.replace('And','wutwutwut') \n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, preprocessfunc=preprocess)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 18]\n"
     ]
    }
   ],
   "source": [
    "# now provide a totally custom parser fucntion. Can return literally anything.\n",
    "def parsefunc(doc): return len(doc)\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, parsefunc=parsefunc)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-20:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/spacy/lookups.py\", line 182, in __init__\n",
      "    def __init__(self, name=None, data=None):\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-21:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-23:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "Process ForkPoolWorker-24:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# broken into paragraphs and then sentences\n",
    "def parsefunc(doc): return len(doc)\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, parsefunc=parsefunc, paragraph_sep='\\n\\n')\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribute Parse With DocParser Methods\n",
    "For most applications, you can simply re-use many of the DocParser methods. We will show some examples. To see their full documentation refer to the docs for those functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we overload many parts of the DocParser methods for parsing documents. In this way, we can still use DocParser features but by customizing a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DocParser With Parsetrees\n",
    "One obvious implication of being able to overload the parser function is that we can also use the DocParser parsetree functionality. This is pretty straightforward and builds on the other examples used here and in the parsetree examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example we create the most basic parsetree\n",
    "def preproc(text): return dt.DocParser.preprocess(text, remove_url=True)\n",
    "def parsetree(doc): return dt.DocParser.get_parsetrees(doc)\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, parsefunc=parsetree, preprocessfunc=preproc)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
