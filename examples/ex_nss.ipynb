{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doctable Demo: US National Security Strategy Documents\n",
    "This example shows a full example of a doctable workflow designed to parse texts end-to-end, using the NSS documents for demonstation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import doctable as dt\n",
    "import spacy\n",
    "import os\n",
    "from pprint import pprint\n",
    "import urllib.request # used for downloading nss docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DocTable-based Class\n",
    "This class inherits from DocTable and will typically store schema and other static inforamtion about the database. This is the most common way to work with DocTable. You can see we keep two class member variables to store the database table name and the schema. See the [schema guide](examples/doctable_schema.html) for more schema examples.\n",
    "\n",
    "We also create a `.insert_nssdoc()` method which wraps the `DocTable.insert()` method to make insertion easier by counting paragraphs, sentences, and tokens to insert. A `.print_doctable()` static method is created so we can print the contents of a NSSDocs database later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSSDocs(dt.DocTable):\n",
    "    tabname = 'nssdocs'\n",
    "    schema = (\n",
    "        ('integer', 'id', dict(primary_key=True, autoincrement=True)),\n",
    "        ('integer', 'year', dict(unique=True, nullable=False)),\n",
    "        ('integer','num_pars'),\n",
    "        ('integer','num_sents'),\n",
    "        ('integer', 'num_toks'),\n",
    "        ('pickle','par_sents'), # nested tokens within sentences within paragraphs\n",
    "        ('index', 'ind_yr', ['year'], dict(unique=True)),        \n",
    "    )\n",
    "    def __init__(self, **kwargs):\n",
    "        dt.DocTable.__init__(self, schema=self.schema, tabname=self.tabname, **kwargs)\n",
    "        \n",
    "    def insert_nssdoc(self, year, par_sents, **kwargs):\n",
    "        self.insert({\n",
    "            'year': year,\n",
    "            'num_pars': len(par_sents),\n",
    "            'num_sents': len([s for par in par_sents for s in par]),\n",
    "            'num_toks': len([t for par in par_sents for s in par for t in s]),\n",
    "            'par_sents': par_sents,\n",
    "        }, **kwargs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_doctable(fname):\n",
    "        '''Simple method for printing contents of a doctable.'''\n",
    "        db = NSSDocs(fname=fname)\n",
    "        print(db)\n",
    "        print(db.select_df(limit=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a connection to a database by instantiating. Since the fname parameter was not provided, this doctable exists only in memory using sqlite. Our other examples will use files, but instantiating in memory first is a good way to check that the schema is valid. We can check the sqlite table schema using the `.schemainfo` property. You can see that the 'pickle' datatype we chose above is represented as a BLOB column. This is because DocTable, using SQLAlchemy core, creates an interface on top of sqlite to handle the data conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'id',\n",
       "  'type': INTEGER(),\n",
       "  'nullable': False,\n",
       "  'default': None,\n",
       "  'autoincrement': 'auto',\n",
       "  'primary_key': 1},\n",
       " {'name': 'year',\n",
       "  'type': INTEGER(),\n",
       "  'nullable': False,\n",
       "  'default': None,\n",
       "  'autoincrement': 'auto',\n",
       "  'primary_key': 0},\n",
       " {'name': 'num_pars',\n",
       "  'type': INTEGER(),\n",
       "  'nullable': True,\n",
       "  'default': None,\n",
       "  'autoincrement': 'auto',\n",
       "  'primary_key': 0},\n",
       " {'name': 'num_sents',\n",
       "  'type': INTEGER(),\n",
       "  'nullable': True,\n",
       "  'default': None,\n",
       "  'autoincrement': 'auto',\n",
       "  'primary_key': 0},\n",
       " {'name': 'num_toks',\n",
       "  'type': INTEGER(),\n",
       "  'nullable': True,\n",
       "  'default': None,\n",
       "  'autoincrement': 'auto',\n",
       "  'primary_key': 0},\n",
       " {'name': 'par_sents',\n",
       "  'type': BLOB(),\n",
       "  'nullable': True,\n",
       "  'default': None,\n",
       "  'autoincrement': 'auto',\n",
       "  'primary_key': 0}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = NSSDocs()\n",
    "db.schemainfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Parser Class\n",
    "Now we create a parser class NSSParser which inherits from DocParser. This class will handle parsing the text data and inserting it into the DocTable. This is the most common way to use the DocParser class; it allows our NSSParser to have access to the flexible functions. You can see more of these methods in the [DocParser reference](ref/doctable.DocParser.html) or the [overview examples](examples/docparser_basics.html).\n",
    "\n",
    "The `parse_nss_docs()` method we created here shows the use of the `DocParser.distribute_chunks()` method which can take an input sequence, in our case a list of years as integers, and break it up into chunks to send to a user-provided function, in this case our `.parse_nss_chunk()`. This function handles a batch of years and for each year will download the texts (from my [nssdocs github repo](https://github.com/devincornell/nssdocs)), split them into paragraphs, parse them using spacy, and tokenize or convert to parsetrees using the `DocParser.tokenize_doc()` and `DocParser.get_parsetree()` methods respectively. Note that we can see the `DocParser.preprocess()` method used for preprocessing and the `DocParser.use_tok()` and `DocParser.parse_tok()` methods, wrapped in lambda functions, passed to `DocParser.tokenize_doc()` method for extra control over the parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSSParser(dt.DocParser):\n",
    "    ''''''\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.nlp = spacy.load('en')\n",
    "        \n",
    "    def parse_nss_docs(self, years, dbfname, as_parsetree=False, workers=None, verbose=False):\n",
    "        '''Parse and store nss docs into a doctable.\n",
    "        Args:\n",
    "            years (list): years to request from the nss corpus\n",
    "            dbfname (str): fname for DocTable to initialize in each process.\n",
    "            as_parsetree (bool): store parsetrees (True) or tokens (False)\n",
    "            workers (int or None): number of processes to create for parsing.\n",
    "        '''\n",
    "        self.distribute_chunks(self.parse_nss_chunk, years, self.nlp, dbfname, as_parsetree, verbose, workers=workers)\n",
    "    \n",
    "    @classmethod\n",
    "    def parse_nss_chunk(cls, years, nlp, dbfname, as_parsetree, verbose):\n",
    "        '''Runs in separate process for each chunk of nss docs.\n",
    "        Description: each \n",
    "        Args:\n",
    "            years (list<int>): years of nss to download and parse\n",
    "            nlp (spacy parser object): process documents using nlp.pipe()\n",
    "            dbfname (str): filename of NSSDocs database to open\n",
    "            as_parsetree (bool): parse into parsetree or just tokens.\n",
    "                storing parsetrees is much more (~6x) expensive than\n",
    "                just storing tokens.\n",
    "        '''\n",
    "        \n",
    "        # create a new database connection\n",
    "        db = NSSDocs(fname=dbfname)\n",
    "        \n",
    "        # download, preprocess, and break texts into paragraphs\n",
    "        preprocess = lambda text: cls.preprocess(text, replace_xml='')\n",
    "        texts = list(map(preprocess, list(map(cls.download_nss, years))))\n",
    "        pars = [(i,par.strip()) for i,text in enumerate(texts) \n",
    "                      for par in text.split('\\n\\n') if len(par.strip()) > 0]\n",
    "        ind, pars = list(zip(*pars))\n",
    "        \n",
    "        use_tok = lambda tok: cls.use_tok(tok, filter_whitespace=True)\n",
    "        parse_tok = lambda tok: cls.parse_tok(tok, replace_num=True, format_ents=True)\n",
    "        \n",
    "        # choose to create either token sequences or parsetrees\n",
    "        if not as_parsetree:\n",
    "            tokenize = lambda doc: cls.tokenize_doc(doc, merge_ents=True, split_sents=True, parse_tok_func=parse_tok, use_tok_func=use_tok)\n",
    "        else:\n",
    "            tokenize = lambda doc: cls.get_parsetrees(doc, merge_ents=True, parse_tok_func=parse_tok)\n",
    "        \n",
    "        if verbose: print('starting', years)\n",
    "        # process documents\n",
    "        pp = list()\n",
    "        for doc in nlp.pipe(pars):\n",
    "            toks = tokenize(doc)\n",
    "            pp.append(toks)\n",
    "        if verbose: print('about to insert', years)\n",
    "        # merge paragraphs back into docs and insert into db\n",
    "        doc_pars = [[p for idx,p in zip(ind,pp) if idx==i] for i in range(max(ind)+1)]\n",
    "        for yr,dp in zip(years,doc_pars):\n",
    "            db.insert_nssdoc(yr, dp, ifnotunique='replace')\n",
    "        if verbose: print('inserted', years)\n",
    "\n",
    "            \n",
    "    @staticmethod\n",
    "    def download_nss(year):\n",
    "        '''Simple helper function for downloading texts from my nssdocs repo.'''\n",
    "        baseurl = 'https://raw.githubusercontent.com/devincornell/nssdocs/master/docs/{}.txt'\n",
    "        url = baseurl.format(year)\n",
    "        text = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Parser\n",
    "Now we run the parsing algorithm by instantiating NSSParser (which simply loads a spacy module) and parse the documents using the method we created `.parse_nss_docs()`. From looking at the `.parse_nss_chunk()` method above, you can see that each process is passed only a year and a doctable filename, and each process will download a copy of the given document, process the document, and insert the document into its own DocTable connection.\n",
    "\n",
    "In this first example you can see the print output from each of the processes as they act simultaneously and then insert their results into their doctable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting (2015, 2017)\n",
      "starting (1987, 1988, 1990, 1991, 1993)\n",
      "starting (1999, 2000, 2002, 2006, 2010)\n",
      "starting (1994, 1995, 1996, 1997, 1998)\n",
      "about to insert (2015, 2017)\n",
      "inserted (2015, 2017)\n",
      "about to insert (1987, 1988, 1990, 1991, 1993)\n",
      "inserted (1987, 1988, 1990, 1991, 1993)\n",
      "about to insert (1994, 1995, 1996, 1997, 1998)\n",
      "inserted (1994, 1995, 1996, 1997, 1998)\n",
      "about to insert (1999, 2000, 2002, 2006, 2010)\n",
      "inserted (1999, 2000, 2002, 2006, 2010)\n",
      "CPU times: user 22 ms, sys: 24.8 ms, total: 46.8 ms\n",
      "Wall time: 24.7 s\n",
      "<DocTable::nssdocs ct: 17>\n",
      "    id  year  num_pars  num_sents  num_toks  \\\n",
      "0  342  2015       150        659     16108   \n",
      "1  343  2017       400       1170     23587   \n",
      "\n",
      "                                           par_sents  \n",
      "0  [[[Today, ,, The United States, is, stronger, ...  \n",
      "1  [[[an, America, that, is, safe, ,, prosperous,...  \n"
     ]
    }
   ],
   "source": [
    "# download and parse these years\n",
    "years = (1987, 1988, 1990, 1991, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2002, 2006, 2010, 2015, 2017)\n",
    "    \n",
    "# instantiate parser (loads spacy model) and call .parse_nss_docs() to parse and store the docs\n",
    "fname_tokens = 'exdb/ex_workflow_tokens.db'\n",
    "parser = NSSParser()\n",
    "%time parser.parse_nss_docs(years, fname_tokens, as_parsetree=False, workers=4, verbose=True)\n",
    "NSSDocs.print_doctable(fname_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.6 ms, sys: 93.1 ms, total: 111 ms\n",
      "Wall time: 8.59 s\n",
      "<DocTable::nssdocs ct: 17>\n",
      "    id  year  num_pars  num_sents  num_toks  \\\n",
      "0  359  1993       125        578     13091   \n",
      "1  360  2002       199        652     13854   \n",
      "\n",
      "                                           par_sents  \n",
      "0  [[[preface]], [[American, leadership, for, pea...  \n",
      "1  [[[the, great, struggles, of, The Twentieth Ce...  \n"
     ]
    }
   ],
   "source": [
    "# by omitting the \"workers\" parameter, DocParser will use all the cores the machine has\n",
    "%time parser.parse_nss_docs(years, fname_tokens, as_parsetree=False)\n",
    "NSSDocs.print_doctable(fname_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.4 ms, sys: 88 ms, total: 109 ms\n",
      "Wall time: 9.55 s\n",
      "<DocTable::nssdocs ct: 17>\n",
      "    id  year  num_pars  num_sents  num_toks  \\\n",
      "0  186  1993       125        578     13134   \n",
      "1  187  2002       199        652     13917   \n",
      "\n",
      "                                           par_sents  \n",
      "0  [[(ParseNode(preface))], [(ParseNode(American)...  \n",
      "1  [[(ParseNode(the), ParseNode(great), ParseNode...  \n"
     ]
    }
   ],
   "source": [
    "# now we set \"as_parsetree\" to true so it will store the docs as parstrees instead of tokens.\n",
    "fname_parsetrees = 'exdb/ex_workflow_parsetrees.db'\n",
    "%time parser.parse_nss_docs(years, fname_parsetrees, as_parsetree=True)\n",
    "NSSDocs.print_doctable(fname_parsetrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filesize Comparison\n",
    "While the timed performance of generating parsetrees vs tokens is relatively insignificant, we see a huge difference in the resulting database file sizes. Wheras the tokens database took about 6 MB, the parsetree database took about 40 MB. A significant difference worth consideration in your analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.651904, 40.660992)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.getsize(fname_tokens)/1e6, os.path.getsize(fname_parsetrees)/1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Database\n",
    "Now we can use DocTable to view and manipulate the stored documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DocTable::nssdocs ct: 17>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = NSSDocs(fname=fname_tokens)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This corpus consists of 17 documents, 5028 paragraphs, 17448 sentences, and 418921 tokens.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>num_pars</th>\n",
       "      <th>num_sents</th>\n",
       "      <th>num_toks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1993</td>\n",
       "      <td>125</td>\n",
       "      <td>578</td>\n",
       "      <td>13091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2002</td>\n",
       "      <td>199</td>\n",
       "      <td>652</td>\n",
       "      <td>13854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "      <td>150</td>\n",
       "      <td>659</td>\n",
       "      <td>16108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1994</td>\n",
       "      <td>223</td>\n",
       "      <td>789</td>\n",
       "      <td>18731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1991</td>\n",
       "      <td>290</td>\n",
       "      <td>826</td>\n",
       "      <td>19289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1990</td>\n",
       "      <td>294</td>\n",
       "      <td>787</td>\n",
       "      <td>16979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2006</td>\n",
       "      <td>468</td>\n",
       "      <td>1034</td>\n",
       "      <td>20755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1997</td>\n",
       "      <td>212</td>\n",
       "      <td>811</td>\n",
       "      <td>21635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1987</td>\n",
       "      <td>263</td>\n",
       "      <td>1091</td>\n",
       "      <td>25349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1988</td>\n",
       "      <td>156</td>\n",
       "      <td>1142</td>\n",
       "      <td>26521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>400</td>\n",
       "      <td>1170</td>\n",
       "      <td>23587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1995</td>\n",
       "      <td>294</td>\n",
       "      <td>989</td>\n",
       "      <td>22051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "      <td>317</td>\n",
       "      <td>1149</td>\n",
       "      <td>31104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1996</td>\n",
       "      <td>315</td>\n",
       "      <td>1238</td>\n",
       "      <td>33587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1999</td>\n",
       "      <td>338</td>\n",
       "      <td>1244</td>\n",
       "      <td>32096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1998</td>\n",
       "      <td>418</td>\n",
       "      <td>1503</td>\n",
       "      <td>36540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2000</td>\n",
       "      <td>566</td>\n",
       "      <td>1786</td>\n",
       "      <td>47644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  num_pars  num_sents  num_toks\n",
       "0   1993       125        578     13091\n",
       "1   2002       199        652     13854\n",
       "2   2015       150        659     16108\n",
       "3   1994       223        789     18731\n",
       "4   1991       290        826     19289\n",
       "5   1990       294        787     16979\n",
       "6   2006       468       1034     20755\n",
       "7   1997       212        811     21635\n",
       "8   1987       263       1091     25349\n",
       "9   1988       156       1142     26521\n",
       "10  2017       400       1170     23587\n",
       "11  1995       294        989     22051\n",
       "12  2010       317       1149     31104\n",
       "13  1996       315       1238     33587\n",
       "14  1999       338       1244     32096\n",
       "15  1998       418       1503     36540\n",
       "16  2000       566       1786     47644"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we show some metatadat from the new corpus\n",
    "df = db.select_df(['year','num_pars', 'num_sents', 'num_toks'])\n",
    "print('This corpus consists of {} documents, {} paragraphs, {} sentences, and {} tokens.'\n",
    "      ''.format(df.shape[0], df['num_pars'].sum(), df['num_sents'].sum(), df['num_toks'].sum()))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['preface'], ['American', 'leadership', 'for', 'peaceful', 'change'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sents(db):\n",
    "    for doc in db.select('par_sents'):\n",
    "        for par in doc:\n",
    "            for sent in par:\n",
    "                yield sent\n",
    "sents = list(get_sents(db))\n",
    "sents[0][:5], sents[1][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "This example shows a very common way of working with the doctable package. Wheras the DocTable class provides a simple interface for storing and accessing databases, DocParser provides convenient methods for processing texts in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models in Parallel\n",
    "Now we want to train several ml models on the data, one for each parmeter configuration. We can train multiple models in the same way that we parse text by using the `.DocParser.distribute_process()` method, where we provide a function that takes the input data and trains a model and then stores the result in a database. This will allow you to take full advantage of your computing machinery.\n",
    "\n",
    "Our example will be in measuring classification accuracy for predicting the document from which a given NSS paragraph was drawn. To do this, we extract all paragraphs from the documents, train a model pipeline including TF-IDF, SVD, and an SVM classifier to predict the nss document given a set of paragraph tokens, and report cross-validation results.\n",
    "\n",
    "First we create a new ModelDB class which inherits from DocTable, then we create a `train_model()` function which takes the number of features as a parameter. The function trains a model and saves the number of features used as well as the result metrics to the database. Because we use `DocTable.distribute_process()`, these models are all trained in parallel and saved to the db when finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts a list of paragraph tokens from all docs, records year from which each par came\n",
    "pars = [(yr,par) for yr,doc in db.select(['year','par_sents']) for par in doc]\n",
    "years, paragraphs = list(zip(*pars))\n",
    "paragraphs = [[tok for sent in par for tok in sent if isinstance(tok,str)] for par in paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x7f2138285e80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new database to store the models\n",
    "class ModelDB(dt.DocTable):\n",
    "    tabname = 'modeldb'\n",
    "    schema = (\n",
    "        ('integer', 'id', dict(primary_key=True, autoincrement=True)),\n",
    "        ('integer', 'num_feat', dict(unique=True)),\n",
    "        ('float', 'train_time'),\n",
    "        ('float', 'av_train'),\n",
    "        ('float', 'av_test'),\n",
    "    )\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(schema=self.schema, tabname=self.tabname, **kwargs)\n",
    "fname_models = 'exdb/nss_models.db'\n",
    "modeldb = ModelDB(fname=fname_models)\n",
    "modeldb.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn imports\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.decomposition import PCA, SparsePCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_feat, pars, years, fname):\n",
    "    '''Function to train a single model.'''\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, min_df=10)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('svd', TruncatedSVD(n_components=num_feat)),\n",
    "        ('clf', SGDClassifier()),\n",
    "    ])\n",
    "    \n",
    "    scores = cross_validate(pipeline, pars, years, cv=5, return_train_score=True)\n",
    "    db = ModelDB(fname=fname)\n",
    "    db.insert({\n",
    "        'num_feat': num_feat,\n",
    "        'train_time': scores['fit_time'].mean(),\n",
    "        'av_train': scores['train_score'].mean(),\n",
    "        'av_test': scores['test_score'].mean(),\n",
    "    }, ifnotunique='replace')\n",
    "    return scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.28110600108012773,\n",
       " 0.2916861180251956,\n",
       " 0.3004187547102196,\n",
       " 0.30603284510838974]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_feat_list = (500, 1000, 1500, 2000)\n",
    "\n",
    "dt.DocParser.distribute_process(train_model, num_feat_list, paragraphs, years, fname_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>num_feat</th>\n",
       "      <th>train_time</th>\n",
       "      <th>av_train</th>\n",
       "      <th>av_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>33.807687</td>\n",
       "      <td>0.632242</td>\n",
       "      <td>0.281106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>46.300928</td>\n",
       "      <td>0.788089</td>\n",
       "      <td>0.291686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1500</td>\n",
       "      <td>61.322850</td>\n",
       "      <td>0.840742</td>\n",
       "      <td>0.300419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2000</td>\n",
       "      <td>66.627453</td>\n",
       "      <td>0.876095</td>\n",
       "      <td>0.306033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  num_feat  train_time  av_train   av_test\n",
       "0   1       500   33.807687  0.632242  0.281106\n",
       "1   2      1000   46.300928  0.788089  0.291686\n",
       "2   3      1500   61.322850  0.840742  0.300419\n",
       "3   4      2000   66.627453  0.876095  0.306033"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeldb.select_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "This example shows how we can use DocTable for parsing texts and for training models in parallel. Databases make it easy to parallelize tasks across processes because the results can be stored in a table that is thread-safe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
