{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Workflow Example\n",
    "This example shows a full example of a doctable workflow designed to parse texts end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import doctable as dt\n",
    "import spacy\n",
    "from pprint import pprint\n",
    "import urllib.request # used for downloading nss docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Parser Class\n",
    "This class will be used to parse your entire corpus. It inherits from DocParser to use a number of built-in features to make parsing convenient. For maximum efficiency, it works by providing only a number of years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting (2015, 2017)\n",
      "starting (1987, 1988, 1990, 1991, 1993)\n",
      "starting (1999, 2000, 2002, 2006, 2010)\n",
      "starting (1994, 1995, 1996, 1997, 1998)\n",
      "about to insert (2015, 2017)\n",
      "inserted (2015, 2017)\n",
      "about to insert (1987, 1988, 1990, 1991, 1993)\n",
      "inserted (1987, 1988, 1990, 1991, 1993)\n",
      "about to insert (1999, 2000, 2002, 2006, 2010)\n",
      "inserted (1999, 2000, 2002, 2006, 2010)\n",
      "about to insert (1994, 1995, 1996, 1997, 1998)\n",
      "inserted (1994, 1995, 1996, 1997, 1998)\n",
      "<DocTable::nssdocs ct: 17>\n",
      "    id  year  num_pars  num_sents  num_toks  \\\n",
      "0  155  2015       150        659     16108   \n",
      "1  156  2017       400       1170     23587   \n",
      "\n",
      "                                           par_sents  \n",
      "0  [[[Today, ,, The United States, is, stronger, ...  \n",
      "1  [[[an, America, that, is, safe, ,, prosperous,...  \n",
      "starting (2015, 2017)\n",
      "starting (1987, 1988, 1990, 1991, 1993)\n",
      "starting (1994, 1995, 1996, 1997, 1998)\n",
      "starting (1999, 2000, 2002, 2006, 2010)\n",
      "about to insert (2015, 2017)\n",
      "inserted (2015, 2017)\n",
      "about to insert (1987, 1988, 1990, 1991, 1993)\n",
      "inserted (1987, 1988, 1990, 1991, 1993)\n",
      "about to insert (1994, 1995, 1996, 1997, 1998)\n",
      "inserted (1994, 1995, 1996, 1997, 1998)\n",
      "about to insert (1999, 2000, 2002, 2006, 2010)\n",
      "inserted (1999, 2000, 2002, 2006, 2010)\n",
      "<DocTable::nssdocs ct: 17>\n",
      "   id  year  num_pars  num_sents  num_toks  \\\n",
      "0  84  2015       150        659     16108   \n",
      "1  85  2017       400       1170     23625   \n",
      "\n",
      "                                           par_sents  \n",
      "0  [[(ParseNode(Today), ParseNode(,), ParseNode(T...  \n",
      "1  [[(ParseNode(an), ParseNode(America), ParseNod...  \n"
     ]
    }
   ],
   "source": [
    "class NSSDocs(dt.DocTable):\n",
    "    tabname = 'nssdocs'\n",
    "    schema = (\n",
    "        ('integer', 'id', dict(primary_key=True, autoincrement=True)),\n",
    "        ('integer', 'year', dict(unique=True)),\n",
    "        ('integer','num_pars'),\n",
    "        ('integer','num_sents'),\n",
    "        ('integer', 'num_toks'),\n",
    "        ('pickle','par_sents'), # nested tokens within sentences within paragraphs\n",
    "        ('index', 'ind_yr', ['year'], dict(unique=True)),        \n",
    "    )\n",
    "    def __init__(self, **kwargs):\n",
    "        dt.DocTable.__init__(self, schema=self.schema, tabname=self.tabname, **kwargs)\n",
    "        \n",
    "    def insert_nssdoc(self, year, par_sents, **kwargs):\n",
    "        self.insert({\n",
    "            'year': year,\n",
    "            'num_pars': len(par_sents),\n",
    "            'num_sents': len([s for par in par_sents for s in par]),\n",
    "            'num_toks': len([t for par in par_sents for s in par for t in s]),\n",
    "            'par_sents': par_sents,\n",
    "        }, **kwargs)\n",
    "\n",
    "class NSSParser(dt.DocParser):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.nlp = spacy.load('en')\n",
    "        \n",
    "    def parse_nss_docs(self, years, dbfname, as_parsetree=False, workers=None):\n",
    "        '''Parse and store nss docs into a doctable.\n",
    "        Args:\n",
    "            years (list): years to request from the nss corpus\n",
    "        '''\n",
    "        self.distribute_chunks(self.parse_nss_chunk, years, self.nlp, dbfname, as_parsetree, workers=workers)\n",
    "    \n",
    "    @classmethod\n",
    "    def parse_nss_chunk(cls, years, nlp, dbfname, as_parsetree):\n",
    "        '''Run in separate process for each chunk of nss years.'''\n",
    "        \n",
    "        # create a new database connection\n",
    "        db = NSSDocs(fname=dbfname)\n",
    "        \n",
    "        # download, preprocess, and break texts into paragraphs\n",
    "        preprocess = lambda text: cls.preprocess(text, replace_xml='')\n",
    "        texts = list(map(preprocess, list(map(cls.download_nss, years))))\n",
    "        pars = [(i,par.strip()) for i,text in enumerate(texts) \n",
    "                      for par in text.split('\\n\\n') if len(par.strip()) > 0]\n",
    "        ind, pars = list(zip(*pars))\n",
    "        \n",
    "        use_tok = lambda tok: cls.use_tok(tok, filter_whitespace=True)\n",
    "        parse_tok = lambda tok: cls.parse_tok(tok, replace_num=True, format_ents=True)\n",
    "        \n",
    "        # choose to create either token sequences or parsetrees\n",
    "        if not as_parsetree:\n",
    "            tokenize = lambda doc: cls.tokenize_doc(doc, merge_ents=True, split_sents=True, parse_tok_func=parse_tok, use_tok_func=use_tok)\n",
    "        else:\n",
    "            tokenize = lambda doc: cls.get_parsetrees(doc, merge_ents=True, parse_tok_func=parse_tok)\n",
    "        \n",
    "        print('starting', years)\n",
    "        # process documents\n",
    "        pp = list()\n",
    "        for doc in nlp.pipe(pars):\n",
    "            toks = tokenize(doc)\n",
    "            pp.append(toks)\n",
    "        print('about to insert', years)\n",
    "        # merge paragraphs back into docs and insert into db\n",
    "        doc_pars = [[p for idx,p in zip(ind,pp) if idx==i] for i in range(max(ind)+1)]\n",
    "        for yr,dp in zip(years,doc_pars):\n",
    "            db.insert_nssdoc(yr, dp, ifnotunique='replace')\n",
    "        print('inserted', years)\n",
    "\n",
    "            \n",
    "    @staticmethod\n",
    "    def download_nss(year):\n",
    "        baseurl = 'https://raw.githubusercontent.com/devincornell/nssdocs/master/docs/{}.txt'\n",
    "        url = baseurl.format(year)\n",
    "        text = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "        return text\n",
    "    \n",
    "\n",
    "# download and parse these years\n",
    "years = (1987, 1988, 1990, 1991, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2002, 2006, 2010, 2015, 2017)\n",
    "    \n",
    "# instantiate parser (primarily loads spacy parser)\n",
    "parser = NSSParser()\n",
    "\n",
    "fname = 'exdb/ex_workflow_tokens.db'\n",
    "parser.parse_nss_docs(years, fname, as_parsetree=False, workers=4)\n",
    "db = NSSDocs(fname=fname)\n",
    "print(db)\n",
    "print(db.select_df(limit=2))\n",
    "\n",
    "fname = 'exdb/ex_workflow_parsetrees.db'\n",
    "parser.parse_nss_docs(years, fname, as_parsetree=True, workers=4)\n",
    "db = NSSDocs(fname=fname)\n",
    "\n",
    "print(db)\n",
    "print(db.select_df(limit=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
