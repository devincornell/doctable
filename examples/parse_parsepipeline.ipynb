{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# `ParsePipeline` Basics\n",
    "Here I demonstrate the basics of parsing text using [Spacy](https://spacy.io/) + doctable to tokenize text. Spacy does most of the heavy-lifting here to actually parse the document, and doctable methods handle the conversion from the Spacy Document object to a sequence of string tokens (words)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from IPython import get_ipython\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import doctable"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "ex_texts = [\n",
    "    'I am pretty bored today. I have been stuck in quarantine for more than two months!',\n",
    "    'We are all waiting on Dr. Fauci to let us know when to return from quarantine.',\n",
    "    'On the bright side, I have more time to talk to my distant friends over video chat.',\n",
    "    'But still, I wish I could travel, go to bars, and go out to eat mrore often!',\n",
    "    'Here we show an example URL: https://devincornell.github.io/doctable/',\n",
    "    'And also one with <b><i>xml tags</i></b>.',\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Build a `ParsePipeline` for Tokenization\n",
    "\n",
    "`ParsePipeline` makes it easy to define a processing pipeline as a list of functions (called components) to apply sequentially to each document in your corpus. You can use the `.parsemany()` method to run the pipeline on documents in paralel, or simply use the `.parse()` method to parse a single document.\n",
    "\n",
    "Our most basic pipeline uses a lambda function to split each text document by whitespace."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "parser_split = doctable.ParsePipeline([\n",
    "    lambda text: text.split(),\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then use the `.parse()` method to apply the pipeline to a single document."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "parsed_text = parser_split.parse(ex_texts[0])\n",
    "print(parsed_text[:7])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['I', 'am', 'pretty', 'bored', 'today.', 'I', 'have']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also use the `.parsemany()` method to parse all of our texts at once. Use the `workers` parameter to specify the number of processes to use if you want to use parallelization."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "parsed_texts = parser_split.parsemany(ex_texts, workers=2) # processes in parallel\n",
    "for text in parsed_texts:\n",
    "    print(text[:7])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['I', 'am', 'pretty', 'bored', 'today.', 'I', 'have']\n",
      "['We', 'are', 'all', 'waiting', 'on', 'Dr.', 'Fauci']\n",
      "['On', 'the', 'bright', 'side,', 'I', 'have', 'more']\n",
      "['But', 'still,', 'I', 'wish', 'I', 'could', 'travel,']\n",
      "['Here', 'we', 'show', 'an', 'example', 'URL:', 'https://devincornell.github.io/doctable/']\n",
      "['And', 'also', 'one', 'with', '<b><i>xml', 'tags</i></b>.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Use doctable Parsing Components\n",
    "doctable has some built-in methods for pre- and post-processing Spacy documents. This list includes all functions in the [doctable.parse](ref/doctable.parse.html) namespace, and you can access them using the `doctable.Comp` function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(doctable.components)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'preprocess': <function preprocess at 0x7fb23bc901f0>, 'tokenize': <function tokenize at 0x7fb23bc90310>, 'parse_tok': <function parse_tok at 0x7fb23bc903a0>, 'keep_tok': <function keep_tok at 0x7fb23bc90430>, 'merge_tok_spans': <function merge_tok_spans at 0x7fb23bc904c0>, 'merge_tok_ngrams': <function merge_tok_ngrams at 0x7fb23bc90550>, 'get_parsetrees': <function get_parsetrees at 0x7fb23bc90670>}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "preproc = doctable.Comp('preprocess', replace_url='_URL_', replace_xml='')\n",
    "print(ex_texts[4])\n",
    "preproc(ex_texts[4])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Here we show an example URL: https://devincornell.github.io/doctable/\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Here we show an example URL: _URL_'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we show a pipeline that uses the doctable `preprocess` method to remove xml tags and urls, the [Spacy nlp model](https://spacy.io/usage/spacy-101) to parse the document, and the built-in `tokenize` method to convert the spacy doc object to a list of tokens. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from doctable import Comp\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "parser_tok = doctable.ParsePipeline([\n",
    "    Comp('preprocess', replace_xml='', replace_url='XXURLXX'),\n",
    "    nlp,\n",
    "    Comp('tokenize', split_sents=False),\n",
    "])\n",
    "\n",
    "docs = parser_tok.parsemany(ex_texts)\n",
    "for doc in docs:\n",
    "    print(doc[:10])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[I, am, pretty, bored, today, ., I, have, been, stuck]\n",
      "[We, are, all, waiting, on, Dr., Fauci, to, let, us]\n",
      "[On, the, bright, side, ,, I, have, more, time, to]\n",
      "[But, still, ,, I, wish, I, could, travel, ,, go]\n",
      "[Here, we, show, an, example, URL, :, XXURLXX]\n",
      "[And, also, one, with, xml, tags, .]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. More Complicated Pipelines\n",
    "\n",
    "Now we show a more complicated mode. The function `tokenize` also takes two additional methods: `keep_tok_func` determines whether a Spacy token should be included in the final document, and the `parse_tok_func` determines how the spacy token objects should be converted to strings. We access the doctable `keep_tok` and `parse_tok` methods using the same `Comp` function to create nested parameter lists."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "parser_full = doctable.ParsePipeline([\n",
    "    \n",
    "    # preprocess to remove xml tags and replace URLs (doctable.parse.preprocess)\n",
    "    Comp('preprocess', replace_xml='', replace_url='XXURLXX'),\n",
    "    nlp, # spacy nlp parser object\n",
    "    \n",
    "    # merge spacy multi-word named entities (doctable.parse.merge_tok_spans)\n",
    "    Comp('merge_tok_spans', merge_ents=True, merge_noun_chunks=False),\n",
    "    \n",
    "    # tokenize document\n",
    "    Comp('tokenize', **{\n",
    "        'split_sents': False,\n",
    "        \n",
    "        # choose tokens to keep (doctable.parse.keep_tok)\n",
    "        'keep_tok_func': Comp('keep_tok', **{\n",
    "            'keep_whitespace': False, # don't keep whitespace\n",
    "            'keep_punct': True, # keep punctuation and stopwords\n",
    "            'keep_stop': True,\n",
    "        }),\n",
    "        \n",
    "        # choose how to convert Spacy token t text (doctable.parse.parse_tok)\n",
    "        'parse_tok_func': Comp('parse_tok', **{\n",
    "            'format_ents': True,\n",
    "            'lemmatize': False,\n",
    "            'num_replacement': 'NUM',\n",
    "            'ent_convert': lambda e: e.text.upper(), # function to capitalize named entities\n",
    "        })\n",
    "    })\n",
    "])\n",
    "len(parser_full.components)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "parsed_docs = parser_full.parsemany(ex_texts)\n",
    "for tokens in parsed_docs:\n",
    "    print(tokens[:10])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['i', 'am', 'pretty', 'bored', 'TODAY', '.', 'i', 'have', 'been', 'stuck']\n",
      "['we', 'are', 'all', 'waiting', 'on', 'dr.', 'FAUCI', 'to', 'let', 'us']\n",
      "['on', 'the', 'bright', 'side', ',', 'i', 'have', 'more', 'time', 'to']\n",
      "['but', 'still', ',', 'i', 'wish', 'i', 'could', 'travel', ',', 'go']\n",
      "['here', 'we', 'show', 'an', 'example', 'url', ':', 'xxurlxx']\n",
      "['and', 'also', 'NUM', 'with', 'xml', 'tags', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "These are the fundamentals of building `ParsePipeline`s in doctable. While these tools are totally optional, I believe they make it easier to structure your code for text analysis applications."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "80712910726593b564a07c7ac6087ce3072c1b43af7fa58c28aea85a2c346dd3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}