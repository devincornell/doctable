{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doctable Demo: US National Security Strategy Documents\n",
    "This example shows a full example of a doctable workflow designed to parse texts end-to-end, using the NSS documents for demonstation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import doctable as dt\n",
    "import spacy\n",
    "import os\n",
    "from pprint import pprint\n",
    "import urllib.request # used for downloading nss docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Dataset\n",
    "This dataset is the plain text version of the US National Security Strategy documents. During the parsing process, all plain text files will be downloaded from my [github project hosting the nss docs](https://github.com/devincornell/nssdocs). I compiled the metadata you see below from [a page hosted by the historical dept of the secretary's office](https://history.defense.gov/Historical-Sources/National-Security-Strategy/). In short, each US President must release at least one NSS per term, with some (namely Clinton) producing more than one per term.\n",
    "\n",
    "In this example, I'll show how to create a database for document + metadata storage using the `DocTable` class, and a parser class using `DocParser`. We will store the metadata you see below in addition to the formatted document text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nss_metadata = {1993: {'party': 'R', 'president': 'H.W. Bush'}, \n",
    "            2002: {'party': 'R', 'president': 'W. Bush'}, \n",
    "            2015: {'party': 'D', 'president': 'Obama'}, \n",
    "            1994: {'party': 'D', 'president': 'Clinton'}, \n",
    "            1990: {'party': 'R', 'president': 'H.W. Bush'}, \n",
    "            1991: {'party': 'R', 'president': 'H.W. Bush'}, \n",
    "            2006: {'party': 'R', 'president': 'W. Bush'}, \n",
    "            1997: {'party': 'D', 'president': 'Clinton'}, \n",
    "            1995: {'party': 'D', 'president': 'Clinton'}, \n",
    "            1987: {'party': 'R', 'president': 'Reagan'}, \n",
    "            1988: {'party': 'R', 'president': 'Reagan'}, \n",
    "            2017: {'party': 'R', 'president': 'Trump'}, \n",
    "            1996: {'party': 'D', 'president': 'Clinton'}, \n",
    "            2010: {'party': 'D', 'president': 'Obama'}, \n",
    "            1999: {'party': 'D', 'president': 'Clinton'}, \n",
    "            1998: {'party': 'D', 'president': 'Clinton'}, \n",
    "            2000: {'party': 'D', 'president': 'Clinton'}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DocTable-based Class\n",
    "This class inherits from DocTable and will typically store schema and other static inforamtion about the database. This is the most common way to work with DocTable. You can see we keep two class member variables to store the database table name and the schema. See the [schema guide](examples/doctable_schema.html) for more schema examples.\n",
    "\n",
    "We also create a `.insert_nssdoc()` method which wraps the `DocTable.insert()` method to make insertion easier by counting paragraphs, sentences, and tokens to insert. A `.print_doctable()` static method is created so we can print the contents of a NSSDocs database later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSSDocs(dt.DocTable):\n",
    "    tabname = 'nssdocs'\n",
    "    schema = (\n",
    "        ('integer', 'id', dict(primary_key=True, autoincrement=True)),\n",
    "        ('integer', 'year', dict(unique=True, nullable=False)),\n",
    "        ('string','president'),\n",
    "        ('string','party'), ('check_constraint', 'party in (\"R\",\"D\")'),\n",
    "        ('integer','num_pars'),\n",
    "        ('integer','num_sents'),\n",
    "        ('integer', 'num_toks'),\n",
    "        ('pickle','par_sents'), # nested tokens within sentences within paragraphs\n",
    "        ('index', 'ind_yr', ['year'], dict(unique=True)),        \n",
    "    )\n",
    "    def __init__(self, **kwargs):\n",
    "        dt.DocTable.__init__(self, schema=self.schema, tabname=self.tabname, **kwargs)\n",
    "        \n",
    "    def insert_nssdoc(self, year, par_sents, prez, party, **kwargs):\n",
    "        self.insert({\n",
    "            'year': year,\n",
    "            'president': prez,\n",
    "            'party': party,\n",
    "            'num_pars': len(par_sents),\n",
    "            'num_sents': len([s for par in par_sents for s in par]),\n",
    "            'num_toks': len([t for par in par_sents for s in par for t in s]),\n",
    "            'par_sents': par_sents,\n",
    "        }, **kwargs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_doctable(fname):\n",
    "        '''Simple method for printing contents of a doctable.'''\n",
    "        db = NSSDocs(fname=fname)\n",
    "        print(db)\n",
    "        print(db.select_df(limit=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a connection to a database by instantiating. Since the fname parameter was not provided, this doctable exists only in memory using sqlite. Our other examples will use files, but instantiating in memory first is a good way to check that the schema is valid. We can check the sqlite table schema using the `.schemainfo` property. You can see that the 'pickle' datatype we chose above is represented as a BLOB column. This is because DocTable, using SQLAlchemy core, creates an interface on top of sqlite to handle the data conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'id',\n",
       "  'type': INTEGER(),\n",
       "  'nullable': False,\n",
       "  'default': None,\n",
       "  'autoincrement': 'auto',\n",
       "  'primary_key': 1},\n",
       " {'name': 'year',\n",
       "  'type': INTEGER(),\n",
       "  'nullable': False,\n",
       "  'default': None,\n",
       "  'autoincrement': 'auto',\n",
       "  'primary_key': 0},\n",
       " {'name': 'president',\n",
       "  'type': VARCHAR(),\n",
       "  'nullable': True,\n",
       "  'default': None,\n",
       "  'autoincrement': 'auto',\n",
       "  'primary_key': 0},\n",
       " {'name': 'party',\n",
       "  'type': VARCHAR(),\n",
       "  'nullable': True,\n",
       "  'default': None,\n",
       "  'autoincrement': 'auto',\n",
       "  'primary_key': 0},\n",
       " {'name': 'num_pars',\n",
       "  'type': INTEGER(),\n",
       "  'nullable': True,\n",
       "  'default': None,\n",
       "  'autoincrement': 'auto',\n",
       "  'primary_key': 0},\n",
       " {'name': 'num_sents',\n",
       "  'type': INTEGER(),\n",
       "  'nullable': True,\n",
       "  'default': None,\n",
       "  'autoincrement': 'auto',\n",
       "  'primary_key': 0},\n",
       " {'name': 'num_toks',\n",
       "  'type': INTEGER(),\n",
       "  'nullable': True,\n",
       "  'default': None,\n",
       "  'autoincrement': 'auto',\n",
       "  'primary_key': 0},\n",
       " {'name': 'par_sents',\n",
       "  'type': BLOB(),\n",
       "  'nullable': True,\n",
       "  'default': None,\n",
       "  'autoincrement': 'auto',\n",
       "  'primary_key': 0}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = NSSDocs()\n",
    "db.schemainfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Parser Class\n",
    "Now we create a parser class NSSParser which inherits from DocParser. This class will handle parsing the text data and inserting it into the DocTable. This is the most common way to use the DocParser class; it allows our NSSParser to have access to the flexible functions. You can see more of these methods in the [DocParser reference](ref/doctable.DocParser.html) or the [overview examples](examples/docparser_basics.html).\n",
    "\n",
    "The `parse_nss_docs()` method we created here shows the use of the `DocParser.distribute_chunks()` method which can take an input sequence, in our case a list of years as integers, and break it up into chunks to send to a user-provided function, in this case our `.parse_nss_chunk()`. This function handles a batch of years and for each year will download the texts (from my [nssdocs github repo](https://github.com/devincornell/nssdocs)), split them into paragraphs, parse them using spacy, and tokenize or convert to parsetrees using the `DocParser.tokenize_doc()` and `DocParser.get_parsetree()` methods respectively. Note that we can see the `DocParser.preprocess()` method used for preprocessing and the `DocParser.use_tok()` and `DocParser.parse_tok()` methods, wrapped in lambda functions, passed to `DocParser.tokenize_doc()` method for extra control over the parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSSParser(dt.DocParser):\n",
    "    ''''''\n",
    "    years_default = (1987, 1988, 1990, 1991, 1993, 1994, 1995, \n",
    "                     1996, 1997, 1998, 1999, 2000, 2002, 2006, \n",
    "                     2010, 2015, 2017)\n",
    "    \n",
    "    def __init__(self, dbfname, metadata, *args, **kwargs):\n",
    "        self.nlp = spacy.load('en')\n",
    "        self.dbfname = dbfname\n",
    "        self.metadata = metadata\n",
    "        \n",
    "    def parse_nss_docs(self, years=None, as_parsetree=False, workers=None, verbose=False):\n",
    "        '''Parse and store nss docs into a doctable.\n",
    "        Args:\n",
    "            years (list): years to request from the nss corpus\n",
    "            dbfname (str): fname for DocTable to initialize in each process.\n",
    "            as_parsetree (bool): store parsetrees (True) or tokens (False)\n",
    "            workers (int or None): number of processes to create for parsing.\n",
    "        '''\n",
    "        if years is None:\n",
    "            years = self.years_default\n",
    "        self.distribute_chunks(self.parse_nss_chunk, years, self.nlp, self.dbfname, \n",
    "                               as_parsetree, verbose, self.metadata, workers=workers)\n",
    "    \n",
    "    @classmethod\n",
    "    def parse_nss_chunk(cls, years, nlp, dbfname, as_parsetree, verbose, metadata):\n",
    "        '''Runs in separate process for each chunk of nss docs.\n",
    "        Description: each \n",
    "        Args:\n",
    "            years (list<int>): years of nss to download and parse\n",
    "            nlp (spacy parser object): process documents using nlp.pipe()\n",
    "            dbfname (str): filename of NSSDocs database to open\n",
    "            as_parsetree (bool): parse into parsetree or just tokens.\n",
    "                storing parsetrees is much more (~6x) expensive than\n",
    "                just storing tokens.\n",
    "        '''\n",
    "        \n",
    "        # create a new database connection\n",
    "        db = NSSDocs(fname=dbfname)\n",
    "        \n",
    "        # download, preprocess, and break texts into paragraphs\n",
    "        preprocess = lambda text: cls.preprocess(text, replace_xml='')\n",
    "        texts = list(map(preprocess, list(map(cls.download_nss, years))))\n",
    "        pars = [(i,par.strip()) for i,text in enumerate(texts) \n",
    "                      for par in text.split('\\n\\n') if len(par.strip()) > 0]\n",
    "        ind, pars = list(zip(*pars))\n",
    "        \n",
    "        use_tok = lambda tok: cls.use_tok(tok, filter_whitespace=True)\n",
    "        parse_tok = lambda tok: cls.parse_tok(tok, replace_num=True, format_ents=True)\n",
    "        \n",
    "        # choose to create either token sequences or parsetrees\n",
    "        if not as_parsetree:\n",
    "            tokenize = lambda doc: cls.tokenize_doc(doc, merge_ents=True, split_sents=True, parse_tok_func=parse_tok, use_tok_func=use_tok)\n",
    "        else:\n",
    "            tokenize = lambda doc: cls.get_parsetrees(doc, merge_ents=True, parse_tok_func=parse_tok)\n",
    "        \n",
    "        if verbose: print('starting', years)\n",
    "        # process documents\n",
    "        pp = list()\n",
    "        for doc in nlp.pipe(pars):\n",
    "            toks = tokenize(doc)\n",
    "            pp.append(toks)\n",
    "        if verbose: print('about to insert', years)\n",
    "        # merge paragraphs back into docs and insert into db\n",
    "        doc_pars = [[p for idx,p in zip(ind,pp) if idx==i] for i in range(max(ind)+1)]\n",
    "        for yr,dp in zip(years,doc_pars):\n",
    "            prez = metadata[yr]['president']\n",
    "            party = metadata[yr]['party']\n",
    "            db.insert_nssdoc(yr, dp, prez, party, ifnotunique='replace')\n",
    "        if verbose: print('inserted', years)\n",
    "\n",
    "            \n",
    "    @staticmethod\n",
    "    def download_nss(year):\n",
    "        '''Simple helper function for downloading texts from my nssdocs repo.'''\n",
    "        baseurl = 'https://raw.githubusercontent.com/devincornell/nssdocs/master/docs/{}.txt'\n",
    "        url = baseurl.format(year)\n",
    "        text = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Parser\n",
    "Now we run the parsing algorithm by instantiating NSSParser (which simply loads a spacy module) and parse the documents using the method we created `.parse_nss_docs()`. From looking at the `.parse_nss_chunk()` method above, you can see that each process is passed only a year and a doctable filename, and each process will download a copy of the given document, process the document, and insert the document into its own DocTable connection.\n",
    "\n",
    "In this first example you can see the print output from each of the processes as they act simultaneously and then insert their results into their doctable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting (2015, 2017)\n",
      "starting (1999, 2000, 2002, 2006, 2010)\n",
      "starting (1994, 1995, 1996, 1997, 1998)\n",
      "starting (1987, 1988, 1990, 1991, 1993)\n",
      "about to insert (2015, 2017)\n",
      "inserted (2015, 2017)\n",
      "about to insert (1987, 1988, 1990, 1991, 1993)\n",
      "inserted (1987, 1988, 1990, 1991, 1993)\n",
      "about to insert (1994, 1995, 1996, 1997, 1998)\n",
      "inserted (1994, 1995, 1996, 1997, 1998)\n",
      "about to insert (1999, 2000, 2002, 2006, 2010)\n",
      "inserted (1999, 2000, 2002, 2006, 2010)\n",
      "CPU times: user 21.2 ms, sys: 50.2 ms, total: 71.4 ms\n",
      "Wall time: 42.2 s\n",
      "<DocTable::nssdocs ct: 17>\n",
      "   id  year president party  num_pars  num_sents  num_toks  \\\n",
      "0   1  2015     Obama     D       150        659     16108   \n",
      "1   2  2017     Trump     R       400       1170     23587   \n",
      "\n",
      "                                           par_sents  \n",
      "0  [[[Today, ,, The United States, is, stronger, ...  \n",
      "1  [[[an, America, that, is, safe, ,, prosperous,...  \n"
     ]
    }
   ],
   "source": [
    "# instantiate parser (loads spacy model) and call .parse_nss_docs() to parse and store the docs\n",
    "fname_tokens = 'exdb/ex_workflow_tokens.db'\n",
    "parser = NSSParser(fname_tokens, nss_metadata)\n",
    "%time parser.parse_nss_docs(as_parsetree=False, workers=4, verbose=True)\n",
    "NSSDocs.print_doctable(fname_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.4 ms, sys: 254 ms, total: 285 ms\n",
      "Wall time: 20.6 s\n",
      "<DocTable::nssdocs ct: 17>\n",
      "   id  year president party  num_pars  num_sents  num_toks  \\\n",
      "0   1  2015     Obama     D       150        659     16108   \n",
      "1   2  2017     Trump     R       400       1170     23587   \n",
      "\n",
      "                                           par_sents  \n",
      "0  [[[Today, ,, The United States, is, stronger, ...  \n",
      "1  [[[an, America, that, is, safe, ,, prosperous,...  \n"
     ]
    }
   ],
   "source": [
    "# by omitting the \"workers\" parameter, DocParser will use all the cores the machine has\n",
    "%time parser.parse_nss_docs(as_parsetree=False)\n",
    "NSSDocs.print_doctable(fname_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.5 ms, sys: 264 ms, total: 290 ms\n",
      "Wall time: 22.7 s\n",
      "<DocTable::nssdocs ct: 17>\n",
      "   id  year  president party  num_pars  num_sents  num_toks  \\\n",
      "0  34  2002    W. Bush     R       199        652     13917   \n",
      "1  35  1993  H.W. Bush     R       125        578     13134   \n",
      "\n",
      "                                           par_sents  \n",
      "0  [[(ParseNode(the), ParseNode(great), ParseNode...  \n",
      "1  [[(ParseNode(preface))], [(ParseNode(American)...  \n"
     ]
    }
   ],
   "source": [
    "# now we set \"as_parsetree\" to true so it will store the docs as parstrees instead of tokens.\n",
    "fname_parsetrees = 'exdb/ex_workflow_parsetrees.db'\n",
    "parser = NSSParser(fname_parsetrees, nss_metadata)\n",
    "%time parser.parse_nss_docs(as_parsetree=True)\n",
    "NSSDocs.print_doctable(fname_parsetrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filesize Comparison\n",
    "While the timed performance of generating parsetrees vs tokens is relatively insignificant, we see a huge difference in the resulting database file sizes. Wheras the tokens database took about 6 MB, the parsetree database took about 40 MB. A significant difference worth consideration in your analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.643712, 40.656896)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.getsize(fname_tokens)/1e6, os.path.getsize(fname_parsetrees)/1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Database\n",
    "Now we can use DocTable to view and manipulate the stored documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DocTable::nssdocs ct: 17>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = NSSDocs(fname=fname_tokens)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This corpus consists of 17 documents, 5028 paragraphs, 17448 sentences, and 418921 tokens.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>president</th>\n",
       "      <th>party</th>\n",
       "      <th>num_pars</th>\n",
       "      <th>num_sents</th>\n",
       "      <th>num_toks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>Obama</td>\n",
       "      <td>D</td>\n",
       "      <td>150</td>\n",
       "      <td>659</td>\n",
       "      <td>16108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>Trump</td>\n",
       "      <td>R</td>\n",
       "      <td>400</td>\n",
       "      <td>1170</td>\n",
       "      <td>23587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1987</td>\n",
       "      <td>Reagan</td>\n",
       "      <td>R</td>\n",
       "      <td>263</td>\n",
       "      <td>1091</td>\n",
       "      <td>25349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1988</td>\n",
       "      <td>Reagan</td>\n",
       "      <td>R</td>\n",
       "      <td>156</td>\n",
       "      <td>1142</td>\n",
       "      <td>26521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1990</td>\n",
       "      <td>H.W. Bush</td>\n",
       "      <td>R</td>\n",
       "      <td>294</td>\n",
       "      <td>787</td>\n",
       "      <td>16979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1991</td>\n",
       "      <td>H.W. Bush</td>\n",
       "      <td>R</td>\n",
       "      <td>290</td>\n",
       "      <td>826</td>\n",
       "      <td>19289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1993</td>\n",
       "      <td>H.W. Bush</td>\n",
       "      <td>R</td>\n",
       "      <td>125</td>\n",
       "      <td>578</td>\n",
       "      <td>13091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1994</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>D</td>\n",
       "      <td>223</td>\n",
       "      <td>789</td>\n",
       "      <td>18731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1995</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>D</td>\n",
       "      <td>294</td>\n",
       "      <td>989</td>\n",
       "      <td>22051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1996</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>D</td>\n",
       "      <td>315</td>\n",
       "      <td>1238</td>\n",
       "      <td>33587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1997</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>D</td>\n",
       "      <td>212</td>\n",
       "      <td>811</td>\n",
       "      <td>21635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1998</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>D</td>\n",
       "      <td>418</td>\n",
       "      <td>1503</td>\n",
       "      <td>36540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1999</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>D</td>\n",
       "      <td>338</td>\n",
       "      <td>1244</td>\n",
       "      <td>32096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2000</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>D</td>\n",
       "      <td>566</td>\n",
       "      <td>1786</td>\n",
       "      <td>47644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2002</td>\n",
       "      <td>W. Bush</td>\n",
       "      <td>R</td>\n",
       "      <td>199</td>\n",
       "      <td>652</td>\n",
       "      <td>13854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2006</td>\n",
       "      <td>W. Bush</td>\n",
       "      <td>R</td>\n",
       "      <td>468</td>\n",
       "      <td>1034</td>\n",
       "      <td>20755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2010</td>\n",
       "      <td>Obama</td>\n",
       "      <td>D</td>\n",
       "      <td>317</td>\n",
       "      <td>1149</td>\n",
       "      <td>31104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  president party  num_pars  num_sents  num_toks\n",
       "0   2015      Obama     D       150        659     16108\n",
       "1   2017      Trump     R       400       1170     23587\n",
       "2   1987     Reagan     R       263       1091     25349\n",
       "3   1988     Reagan     R       156       1142     26521\n",
       "4   1990  H.W. Bush     R       294        787     16979\n",
       "5   1991  H.W. Bush     R       290        826     19289\n",
       "6   1993  H.W. Bush     R       125        578     13091\n",
       "7   1994    Clinton     D       223        789     18731\n",
       "8   1995    Clinton     D       294        989     22051\n",
       "9   1996    Clinton     D       315       1238     33587\n",
       "10  1997    Clinton     D       212        811     21635\n",
       "11  1998    Clinton     D       418       1503     36540\n",
       "12  1999    Clinton     D       338       1244     32096\n",
       "13  2000    Clinton     D       566       1786     47644\n",
       "14  2002    W. Bush     R       199        652     13854\n",
       "15  2006    W. Bush     R       468       1034     20755\n",
       "16  2010      Obama     D       317       1149     31104"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we show some metatadat from the new corpus\n",
    "df = db.select_df(['year','president', 'party', 'num_pars', 'num_sents', 'num_toks'])\n",
    "print('This corpus consists of {} documents, {} paragraphs, {} sentences, and {} tokens.'\n",
    "      ''.format(df.shape[0], df['num_pars'].sum(), df['num_sents'].sum(), df['num_toks'].sum()))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Today', ',', 'The United States', 'is', 'stronger'],\n",
       " ['americas', 'growing', 'economic', 'strength', 'is'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sents(db):\n",
    "    for doc in db.select('par_sents'):\n",
    "        for par in doc:\n",
    "            for sent in par:\n",
    "                yield sent\n",
    "sents = list(get_sents(db))\n",
    "sents[0][:5], sents[1][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "This example shows a very common way of working with the doctable package. Wheras the DocTable class provides a simple interface for storing and accessing databases, DocParser provides convenient methods for processing texts in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models in Parallel\n",
    "Now we want to train several ml models on the data, one for each parmeter configuration. We can train multiple models in the same way that we parse text by using the `.DocParser.distribute_process()` method, where we provide a function that takes the input data and trains a model and then stores the result in a database. This will allow you to take full advantage of your computing machinery.\n",
    "\n",
    "Our example will be in measuring classification accuracy for predicting the document from which a given NSS paragraph was drawn. To do this, we extract all paragraphs from the documents, train a model pipeline including TF-IDF, SVD, and an SVM classifier to predict the nss document given a set of paragraph tokens, and report cross-validation results.\n",
    "\n",
    "First we create a new ModelDB class which inherits from DocTable, then we create a `train_model()` function which takes the number of features as a parameter. The function trains a model and saves the number of features used as well as the result metrics to the database. Because we use `DocTable.distribute_process()`, these models are all trained in parallel and saved to the db when finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts a list of paragraph tokens from all docs, records year from which each par came\n",
    "pars = [(pty,par) for pty,doc in db.select(['party','par_sents']) for par in doc]\n",
    "party, paragraphs = list(zip(*pars))\n",
    "paragraphs = [[tok for sent in par for tok in sent if isinstance(tok,str)] for par in paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DocTable::modeldb ct: 0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new database to store the models\n",
    "class ModelDB(dt.DocTable):\n",
    "    tabname = 'modeldb'\n",
    "    schema = (\n",
    "        ('integer', 'id', dict(primary_key=True, autoincrement=True)),\n",
    "        ('integer', 'num_feat', dict(unique=True)),\n",
    "        ('float', 'train_time'),\n",
    "        ('float', 'av_train'),\n",
    "        ('float', 'av_test'),\n",
    "    )\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(schema=self.schema, tabname=self.tabname, **kwargs)\n",
    "fname_models = 'exdb/nss_models.db'\n",
    "modeldb = ModelDB(fname=fname_models)\n",
    "modeldb.delete()\n",
    "modeldb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn imports\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.decomposition import PCA, SparsePCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_feat, pars, party, fname):\n",
    "    '''Function to train a single model.'''\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=lambda x:x, preprocessor=lambda x:x, min_df=10)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('svd', TruncatedSVD(n_components=num_feat)),\n",
    "        ('clf', SGDClassifier()),\n",
    "    ])\n",
    "    \n",
    "    scores = cross_validate(pipeline, pars, party, cv=5, return_train_score=True)\n",
    "    db = ModelDB(fname=fname)\n",
    "    db.insert({\n",
    "        'num_feat': num_feat,\n",
    "        'train_time': scores['fit_time'].mean(),\n",
    "        'av_train': scores['train_score'].mean(),\n",
    "        'av_test': scores['test_score'].mean(),\n",
    "    }, ifnotunique='replace')\n",
    "    return scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6901276915620704,\n",
       " 0.7062340385547412,\n",
       " 0.7133970307508185,\n",
       " 0.7078270674460698]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_feat_list = (500, 1000, 1500, 2000)\n",
    "\n",
    "dt.DocParser.distribute_process(train_model, num_feat_list, paragraphs, party, fname_models)\n",
    "modeldb.select_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "This example shows how we can use DocTable for parsing texts and for training models in parallel. Databases make it easy to parallelize tasks across processes because the results can be stored in a table that is thread-safe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
