{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser Pipeline\n",
    "These examples will show how to use the DocParser class to make a parallelized pipeline. The `.distribute_parse()` method is the most important aspect of these features, and it is used to parallelize parsing tasks across many CPUs. Generally the workflow goes that you define your functions for tokenization, often times simply overloading parameters on the DocParser functions of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import doctable as dt\n",
    "from spacy import displacy\n",
    "import urllib.request\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trump: An America that is safe, prosperous, and free at home is an America with the str\n",
      "obama: Today, the United States is stronger and better positioned to seize the opportun\n"
     ]
    }
   ],
   "source": [
    "base = 'https://raw.githubusercontent.com/devincornell/intro_to_text_analysis/master/duke_workshop/nss/'\n",
    "urls = (\n",
    "    base+'trump_nss.txt',\n",
    "    base+'obama_nss.txt',\n",
    ")\n",
    "texts = [urllib.request.urlopen(url).read().decode('utf-8') for url in urls]\n",
    "print('trump:', texts[0][:80])\n",
    "print('obama:', texts[1][:80])\n",
    "texts_small = ['The hat is red. And so are you.\\n\\nWhatever, they said. Whatever indeed.', \n",
    "               'But why is the hat blue?\\n\\nAre you colorblind? See the answer here: http://google.com']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Parsing\n",
    "Use the `.distribute_parse()` method to process many documents in parallel. All of the same tokenization function settings can be passed, as we will show.\n",
    "\n",
    "By default, this method will use the built-in DocParser methods for tokenization, etc. But these can be overloaded and overridden to provide more robust functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing 2 docs\n",
      "processing chunks of size 1 with 2 processes.\n",
      "returned 2 parsed docs or paragraphs\n",
      "[['the', 'hat', 'is', 'red', '.', 'and', 'so', 'are', 'you', '.', 'whatever', ',', 'they', 'said', '.', 'whatever', 'indeed', '.'], ['but', 'why', 'is', 'the', 'hat', 'blue', '?', 'are', 'you', 'colorblind', '?', 'see', 'the', 'answer', 'here', ':', 'http://google.com']]\n"
     ]
    }
   ],
   "source": [
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, verbose=True)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing 2 docs\n",
      "split into 4 paragraphs\n",
      "processing chunks of size 1 with 4 processes.\n",
      "returned 4 parsed docs or paragraphs\n",
      "[[['the', 'hat', 'is', 'red', '.', 'and', 'so', 'are', 'you', '.'], ['whatever', ',', 'they', 'said', '.', 'whatever', 'indeed', '.']], [['but', 'why', 'is', 'the', 'hat', 'blue', '?']]]\n"
     ]
    }
   ],
   "source": [
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, verbose=True, paragraph_sep='\\n\\n')\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'hat', 'is', 'red', '.', 'wutwutwut', 'so', 'are', 'you', '.', 'whatever', ',', 'they', 'said', '.', 'whatever', 'indeed', '.'], ['but', 'why', 'is', 'the', 'hat', 'blue', '?', 'are', 'you', 'colorblind', '?', 'see', 'the', 'answer', 'here', ':', 'http://google.com']]\n"
     ]
    }
   ],
   "source": [
    "# provide a custom preprocess function.\n",
    "def preprocess(text):\n",
    "    return text.replace('And','wutwutwut') \n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, preprocessfunc=preprocess)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 18]\n"
     ]
    }
   ],
   "source": [
    "# now provide a totally custom parser fucntion. Can return literally anything.\n",
    "def parsefunc(doc): return len(doc)\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, parsefunc=parsefunc)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 8], [7]]\n"
     ]
    }
   ],
   "source": [
    "# works with paragraphs too\n",
    "def parsefunc(doc): return len(doc)\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, parsefunc=parsefunc, paragraph_sep='\\n\\n')\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribute Parse With DocParser Methods\n",
    "For most applications, you can simply re-use many of the DocParser methods. We will show some examples. To see their full documentation refer to the docs for those functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'hat', 'is', 'red', '.', 'and', 'so', 'are', 'you', '.', 'whatever', ',', 'they', 'said', '.', 'whatever', 'indeed', '.'], ['but', 'why', 'is', 'the', 'hat', 'blue', '?', 'are', 'you', 'colorblind', '?', 'see', 'the', 'answer', 'here', ':']]\n"
     ]
    }
   ],
   "source": [
    "# example overloading the remove_url of the built-in DocParser.preprocess() method\n",
    "def preprocess(text): return dt.DocParser.preprocess(text, remove_url=True)\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, preprocessfunc=preprocess)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we overload many parts of the DocParser methods for parsing documents. In this way, we can still use DocParser features but by customizing a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['hat', 'red'], [], ['say'], []], [['hat', 'blue'], [], ['colorblind'], ['answer']]]\n"
     ]
    }
   ],
   "source": [
    "def use_token_overload(tok):\n",
    "    # decide to include token or not\n",
    "    # this example excludes stopwords and punctuation\n",
    "    return dt.DocParser.use_tok(tok, filter_stop=True, filter_punct=True)\n",
    "\n",
    "def parse_token_overload(tok):\n",
    "    # decide how to parse a spacy token\n",
    "    # this example lemmatizes each word\n",
    "    return dt.DocParser.parse_tok(tok, lemmatize=True)\n",
    "\n",
    "def parsefunc(doc):\n",
    "    # decide how to parse\n",
    "    # in this example, we simply overload some of the supplied functions and also have it split sentences\n",
    "    return dt.DocParser.tokenize_doc(doc, use_tok_func=use_token_overload, parse_tok_func=parse_token_overload, split_sents=True)\n",
    "\n",
    "def preprocess(text):\n",
    "    # decide to preprocess\n",
    "    # overloaded to remove url\n",
    "    return dt.DocParser.preprocess(text, remove_url=True)\n",
    "\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, parsefunc=parsefunc, preprocessfunc=preprocess)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DocParser With Parsetrees\n",
    "One obvious implication of being able to overload the parser function is that we can also use the DocParser parsetree functionality. This is pretty straightforward and builds on the other examples used here and in the parsetree examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ParseTree(5), ParseTree(6), ParseTree(5), ParseTree(3)], [ParseTree(8), ParseTree(1), ParseTree(3), ParseTree(5)]]\n"
     ]
    }
   ],
   "source": [
    "# in this example we create the most basic parsetree\n",
    "def preproc(text): return dt.DocParser.preprocess(text, remove_url=True)\n",
    "def parsetree(doc): return dt.DocParser.get_parsetrees(doc)\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, parsefunc=parsetree, preprocessfunc=preproc)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
