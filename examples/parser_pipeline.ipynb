{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser Pipeline\n",
    "These examples will show how to use the DocParser class for an integrated pipeline. The `.distribute_parse()` method is the most important aspect of these features, and it is used to parallelize parsing tasks across many CPUs. Generally the workflow goes that you define your functions for tokenization, often times simply overloading parameters on the DocParser functions of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import doctable as dt\n",
    "from spacy import displacy\n",
    "import urllib.request\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trump: An America that is safe, prosperous, and free at home is an America with the str\n",
      "obama: Today, the United States is stronger and better positioned to seize the opportun\n"
     ]
    }
   ],
   "source": [
    "base = 'https://raw.githubusercontent.com/devincornell/intro_to_text_analysis/master/duke_workshop/nss/'\n",
    "urls = (\n",
    "    base+'trump_nss.txt',\n",
    "    base+'obama_nss.txt',\n",
    ")\n",
    "texts = [urllib.request.urlopen(url).read().decode('utf-8') for url in urls]\n",
    "print('trump:', texts[0][:80])\n",
    "print('obama:', texts[1][:80])\n",
    "texts_small = ['The hat is red. And so are you.\\n\\nWhatever, they said. Whatever indeed.', 'But why is the hat blue?\\n\\nAre you colorblind?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Parsing\n",
    "Use the `.distribute_parse()` method to process many documents in parallel. All of the same tokenization function settings can be passed, as we will show.\n",
    "\n",
    "By default, this method will use the built-in DocParser methods for tokenization, etc. But these can be overloaded and overridden to provide more robust functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing 2 docs\n",
      "processing chunks of size 1 with 2 processes.\n",
      "returned 2 parsed docs or paragraphs\n",
      "[['the', 'hat', 'is', 'red', '.', 'and', 'so', 'are', 'you', '.', 'whatever', ',', 'they', 'said', '.', 'whatever', 'indeed', '.'], ['but', 'why', 'is', 'the', 'hat', 'blue', '?', 'are', 'you', 'colorblind', '?']]\n"
     ]
    }
   ],
   "source": [
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, verbose=True)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing 2 docs\n",
      "split into 4 paragraphs\n",
      "processing chunks of size 1 with 4 processes.\n",
      "returned 4 parsed docs or paragraphs\n",
      "[[['the', 'hat', 'is', 'red', '.', 'and', 'so', 'are', 'you', '.'], ['whatever', ',', 'they', 'said', '.', 'whatever', 'indeed', '.']], [['but', 'why', 'is', 'the', 'hat', 'blue', '?']]]\n"
     ]
    }
   ],
   "source": [
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, verbose=True, paragraph_sep='\\n\\n')\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing chunks of size 1 with 2 processes.\n",
      "returned 2 parsed docs or paragraphs\n",
      "[['the', 'hat', 'is', 'red', '.', 'wutwutwut', 'so', 'are', 'you', '.', 'whatever', ',', 'they', 'said', '.', 'whatever', 'indeed', '.'], ['but', 'why', 'is', 'the', 'hat', 'blue', '?', 'are', 'you', 'colorblind', '?']]\n"
     ]
    }
   ],
   "source": [
    "# provide a custom preprocess function.\n",
    "def preprocess(text):\n",
    "    return text.replace('And','wutwutwut') \n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, preprocessfunc=preprocess)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing chunks of size 2 with 1 processes.\n",
      "returned 2 parsed docs or paragraphs\n",
      "[19, 12]\n"
     ]
    }
   ],
   "source": [
    "# now provide a totally custom parser fucntion. Can return literally anything.\n",
    "def parsefunc(doc): return len(doc)\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, parsefunc=parsefunc, n_cores=1)\n",
    "print(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing chunks of size 1 with 4 processes.\n",
      "returned 4 parsed docs or paragraphs\n",
      "[[10, 8], [7]]\n"
     ]
    }
   ],
   "source": [
    "# works with paragraphs too\n",
    "def parsefunc(doc): return len(doc)\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, parsefunc=parsefunc, paragraph_sep='\\n\\n')\n",
    "print(parsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
