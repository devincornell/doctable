{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from spacy import displacy\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# now import doctable\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import doctable as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trump: An America that is safe, prosperous, and free at home is an America with the str\n",
      "obama: Today, the United States is stronger and better positioned to seize the opportun\n"
     ]
    }
   ],
   "source": [
    "# download some example documents which are national security strategy documents\n",
    "base = 'https://raw.githubusercontent.com/devincornell/intro_to_text_analysis/master/duke_workshop/nss/'\n",
    "urls = (\n",
    "    base+'trump_nss.txt',\n",
    "    base+'obama_nss.txt',\n",
    ")\n",
    "texts = [urllib.request.urlopen(url).read().decode('utf-8') for url in urls]\n",
    "print('trump:', texts[0][:80])\n",
    "print('obama:', texts[1][:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception during reset or similar\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/sqlalchemy/pool/base.py\", line 680, in _finalize_fairy\n",
      "    fairy._reset(pool)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/sqlalchemy/pool/base.py\", line 867, in _reset\n",
      "    pool._dialect.do_rollback(self)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/default.py\", line 530, in do_rollback\n",
      "    dbapi_connection.rollback()\n",
      "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 140366570628928 and this is thread id 140364765624064.\n",
      "Exception closing connection <sqlite3.Connection object at 0x7fa9272b1490>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/sqlalchemy/pool/base.py\", line 680, in _finalize_fairy\n",
      "    fairy._reset(pool)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/sqlalchemy/pool/base.py\", line 867, in _reset\n",
      "    pool._dialect.do_rollback(self)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/default.py\", line 530, in do_rollback\n",
      "    dbapi_connection.rollback()\n",
      "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 140366570628928 and this is thread id 140364765624064.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/sqlalchemy/pool/base.py\", line 270, in _close_connection\n",
      "    self._dialect.do_close(connection)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/default.py\", line 536, in do_close\n",
      "    dbapi_connection.close()\n",
      "sqlite3.ProgrammingError: SQLite objects created in a thread can only be used in that same thread. The object was created in thread id 140366570628928 and this is thread id 140364765624064.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'NSSParsParser' has no attribute 'matcher'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"../doctable/docparser.py\", line 82, in _distribute_parse_thread\n    parsed_docs.append(parsefunc(doc))\n  File \"<ipython-input-12-f86e65ab4616>\", line 54, in parsetree_tokenize\n    spacy_ngram_matcher=cls.matcher, merge_noun_chunks=False)\nAttributeError: type object 'NSSParsParser' has no attribute 'matcher'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f86e65ab4616>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mtrump\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobama\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNSSParsParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trump'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrump\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'obama'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobama\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-f86e65ab4616>\u001b[0m in \u001b[0;36minsert_document\u001b[0;34m(self, sourcename, text)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#doc = self.distribute_parse([text], self.nlp, paragraph_sep='\\n\\n')[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         paragraphs = self.distribute_parse(texts, self.nlp, parsefunc=self.parsetree_tokenize, preprocessfunc=None, \n\u001b[0;32m---> 37\u001b[0;31m             paragraph_sep='\\n\\n', n_cores=4, verbose=False)[0]\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mvalid_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/dc326/code/doctable/doctable/docparser.py\u001b[0m in \u001b[0;36mdistribute_parse\u001b[0;34m(cls, texts, spacynlp, parsefunc, preprocessfunc, paragraph_sep, n_cores, verbose)\u001b[0m\n\u001b[1;32m     57\u001b[0m                            for i in range(p._processes)]\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             parsed = [d for docs in p.map(cls._distribute_parse_thread, chunks) \n\u001b[0m\u001b[1;32m     60\u001b[0m                     for d in docs]\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'returned {} parsed docs or paragraphs'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'NSSParsParser' has no attribute 'matcher'"
     ]
    }
   ],
   "source": [
    "# make a new doctable object\n",
    "class NSSPars(dt.DocTable2):\n",
    "    schema = (\n",
    "        ('id','integer',dict(primary_key=True, autoincrement=True)),\n",
    "        ('source', 'string'), # trump or obama - whichever nss document\n",
    "        ('parid', 'integer'), # paragraph id\n",
    "        ('sents','pickle'), # contains token parsetrees\n",
    "        ('ind_src_par', 'index', ('source','parid'),dict(unique=True)),\n",
    "    )\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(schema=self.schema, tabname='nsspars', **kwargs)\n",
    "        \n",
    "class NSSParsParser(NSSPars, dt.DocParser):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \n",
    "        # create parser and matcher for parsing\n",
    "        self.nlp = spacy.load('en')\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        \n",
    "        # matches hyphens\n",
    "        pattern = [{'IS_SPACE':False},{'TEXT':'-'},{'IS_SPACE':False}]\n",
    "        self.matcher.add('hyphens', None, pattern)\n",
    "        \n",
    "        # matches hyphens\n",
    "        pattern = [{'TEXT':'@'},{'IS_ALPHA':True}]\n",
    "        self.matcher.add('handles', None, pattern)\n",
    "        \n",
    "        # doctable init\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def insert_document(self, sourcename, text):\n",
    "        # because distribute_parse takes a list of texts, we wrap in a list but\n",
    "        # provide a paragraph separator so that it will process paragraphs in\n",
    "        # parallel\n",
    "        #doc = self.distribute_parse([text], self.nlp, paragraph_sep='\\n\\n')[0]\n",
    "        paragraphs = self.distribute_parse(texts, self.nlp, parsefunc=self.parsetree_tokenize, preprocessfunc=None, \n",
    "            paragraph_sep='\\n\\n', n_cores=4, verbose=False)[0]\n",
    "        \n",
    "        valid_sents = list()\n",
    "        i = 0\n",
    "        for par in paragraphs:\n",
    "            if len(par) > 0:\n",
    "                # filter out sentences with no tokens\n",
    "                sents = [s for s in par if len(s) > 0]\n",
    "        \n",
    "                # insert into table\n",
    "                self.insert({'source': source, 'sents':sents, 'parid':i})\n",
    "                i += 1\n",
    "        \n",
    "    @classmethod\n",
    "    def parsetree_tokenize(cls,doc):\n",
    "        info = {'lemma':cls.lemmainfo, 'prefix':cls.prefixinfo}\n",
    "        return cls.get_parsetrees(doc, tok_parse_func=None, info_func_map=info, merge_ents=True, \n",
    "            spacy_ngram_matcher=cls.matcher, merge_noun_chunks=False)\n",
    "        \n",
    "    @staticmethod\n",
    "    def lemmainfo(tok):\n",
    "        return tok.lemma_\n",
    "    @staticmethod\n",
    "    def prefixinfo(tok):\n",
    "        return tok.prefix_\n",
    "    \n",
    "    @classmethod\n",
    "    def tokparser(cls,tok):\n",
    "        return cls.parse_tok(tok, replace_num=True, replace_digit=None, lemmatize=False, normal_convert=None, \n",
    "            format_ents=True, ent_convert=None)\n",
    "    \n",
    "    @classmethod\n",
    "    def include_tok(cls,tok):\n",
    "         return cls.use_tok(tok, filter_whitespace=True, filter_punct=False, filter_stop=False, \n",
    "                      filter_digit=False, filter_num=False, filter_all_ents=False, \n",
    "                      filter_ent_types=tuple())\n",
    "    \n",
    "\n",
    "trump, obama = texts    \n",
    "db = NSSParsParser()\n",
    "db.insert_document('trump', trump)\n",
    "db.insert_document('obama', obama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
