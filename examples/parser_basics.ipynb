{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import doctable as dt\n",
    "from spacy import displacy\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "James said he will paint the house red for $20 (twenty dollars). He is such a rule-breaker."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exstr = 'James said he will paint the house red for $20 (twenty dollars). He is such a rule-breaker.'\n",
    "doc = nlp(exstr)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Parsing\n",
    "\n",
    "Header: `def tokenize_doc(cls, doc, split_sents=False, merge_ents=False, merge_noun_chunks=False, ngrams=list(), spacy_ngram_matcher=None, ngram_sep=' ', use_tok_args=dict(), parse_tok_args=dict())`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'said', 'he', 'will', 'paint', 'the', 'house', 'red', 'for', '$', '20', '(', 'Twenty', 'Dollars', ')', '.', 'he', 'is', 'such', 'a', 'rule', '-', 'breaker', '.']\n"
     ]
    }
   ],
   "source": [
    "# the most basic version performs tokenizing with all default settings\n",
    "print([tok for tok in dt.DocParser.tokenize_doc(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['James', 'said', 'he', 'will', 'paint', 'the', 'house', 'red', 'for', '$', '20', '(', 'Twenty', 'Dollars', ')', '.'], ['he', 'is', 'such', 'a', 'rule', '-', 'breaker', '.']]\n"
     ]
    }
   ],
   "source": [
    "# split into sentences (list of lists)\n",
    "print([tok for tok in dt.DocParser.tokenize_doc(doc, split_sents=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'said', 'he', 'will', 'paint', 'the', 'house', 'red', 'for', '$', '20', '(', 'Twenty Dollars', ')', '.', 'he', 'is', 'such', 'a', 'rule', '-', 'breaker', '.']\n"
     ]
    }
   ],
   "source": [
    "print([tok for tok in dt.DocParser.tokenize_doc(doc, merge_ents=True)])\n",
    "doc = nlp(exstr) # reverts doc back to original because adding the match (called in .tokenize_doc()) modified it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'said', 'he', 'will', 'paint', 'the house', 'red', 'for', '$', '20', '(', 'Twenty Dollars', ')', '.', 'he', 'is', 'such a rule-breaker', '.']\n"
     ]
    }
   ],
   "source": [
    "print([tok for tok in dt.DocParser.tokenize_doc(doc, merge_noun_chunks=True)])\n",
    "doc = nlp(exstr) # reverts doc back to original because adding the match (called in .tokenize_doc()) modified it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'said', 'he', 'will', 'paint', 'the', 'house', 'red', 'for', '$', '20', '(', 'Twenty', 'Dollars', ')', '.', 'he', 'is', 'such', 'a', 'rule', '-', 'breaker', '.']\n"
     ]
    }
   ],
   "source": [
    "print([tok for tok in dt.DocParser.tokenize_doc(doc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging N-Grams\n",
    "DocParser offers two convenient ways to work with n-grams: (1) using the spacy matcher and (2) using the post-processed multi-token matcher. The first method is applied after normal spacy processing is finished. It involves passing a tuple of ngrams as tuples to apply after all parsing has completed. The good thing about this approach is that it doesn't require much code. The unfortunate thing is that it can only access the tokens after normal parsing. If you would like to merge tokens with hyphens between them or currency symbols to their numbers, you should use the pre-processing method.\n",
    "\n",
    "The pre-processing spacy.Matcher functionality is used to create ngrams which access certain underlying spacy components like IS_DIGIT etc. See [Spacy Matcher documention for more details for that](https://spacy.io/usage/rule-based-matching). The basic workflow is to create a matcher object, add patterns, and then pass matcher to .tokenize_doc(). Note that since the doc object itself is modified, python must be restarted to revert back to other tokenization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'said', 'he', 'will', 'paint', 'the house', 'red', 'for', '$', '20', '(', 'Twenty', 'Dollars', ')', '.', 'he is such', 'a', 'rule - breaker', '.']\n",
      "\n",
      "['James', 'said', 'he', 'will', 'paint', 'the_house', 'red', 'for', '$', '20', '(', 'Twenty', 'Dollars', ')', '.', 'he_is_such', 'a', 'rule_-_breaker', '.']\n"
     ]
    }
   ],
   "source": [
    "# post-parsing ngram merging\n",
    "ngrams = (\n",
    "    ('the', 'house'),\n",
    "    ('rule', '-', 'breaker'),\n",
    "    ('he', 'is', 'such'),\n",
    ")\n",
    "# first using previously tokenized doc (was modified by )\n",
    "print([tok for tok in dt.DocParser.tokenize_doc(doc, ngrams=ngrams)])\n",
    "print()\n",
    "print([tok for tok in dt.DocParser.tokenize_doc(doc, ngrams=ngrams, ngram_sep='_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'said', 'he will', 'paint', 'the', 'house', 'red', 'for', '$20', '(', 'Twenty', 'Dollars', ')', '.', 'he', 'is', 'such', 'a', 'rule-breaker', '.']\n"
     ]
    }
   ],
   "source": [
    "# spacy matcher object (will be passed to docparser)\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# matches currency numbers\n",
    "pattern = [{'TEXT':'$'},{'IS_DIGIT':True}]\n",
    "matcher.add('currency', None, pattern)\n",
    "\n",
    "# matches the phrase \"he will\" or \"He Will\" or \"HE WILL\"\n",
    "pattern2 = [{'LOWER':'he'},{'LOWER':'will'}]\n",
    "matcher.add('he_will', None, pattern2)\n",
    "\n",
    "# matches hyphens\n",
    "pattern3 = [{'IS_SPACE':False},{'TEXT':'-'},{'IS_SPACE':False}]\n",
    "matcher.add('he_will', None, pattern3)\n",
    "\n",
    "print([tok for tok in dt.DocParser.tokenize_doc(doc, spacy_ngram_matcher=matcher)])\n",
    "doc = nlp(exstr) # reverts doc back to original because adding the match (called in .tokenize_doc()) modified it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Parsing\n",
    "An essential element of any spacy tokenization process is to convert Spacy token objects into strings.\n",
    "\n",
    "Full function header: `def parse_tok(tok, replace_num=None, replace_digit=None, lemmatize=False, normal_convert=None, format_ents=True, ent_convert=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'said', 'he', 'will', 'paint', 'the', 'house', 'red', 'for', '$', '20', '(', 'Twenty', 'Dollars', ')', '.', 'he', 'is', 'such', 'a', 'rule', '-', 'breaker', '.']\n"
     ]
    }
   ],
   "source": [
    "# by default this method includes everything\n",
    "print([dt.DocParser.parse_tok(tok) for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'said', 'he', 'will', 'paint', 'the', 'house', 'red', 'for', '$', '__NUM__', '(', '__NUM__', 'Dollars', ')', '.', 'he', 'is', 'such', 'a', 'rule', '-', 'breaker', '.']\n"
     ]
    }
   ],
   "source": [
    "# numbers include anything that looks like a number\n",
    "print([dt.DocParser.parse_tok(tok, replace_num='__NUM__') for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'said', 'he', 'will', 'paint', 'the', 'house', 'red', 'for', '$', '__DIGIT__', '(', 'Twenty', 'Dollars', ')', '.', 'he', 'is', 'such', 'a', 'rule', '-', 'breaker', '.']\n"
     ]
    }
   ],
   "source": [
    "# digits include 5 but not five\n",
    "print([dt.DocParser.parse_tok(tok, replace_digit='__DIGIT__') for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'say', '-pron-', 'will', 'paint', 'the', 'house', 'red', 'for', '$', '20', '(', 'Twenty', 'Dollars', ')', '.', '-pron-', 'be', 'such', 'a', 'rule', '-', 'breaker', '.']\n"
     ]
    }
   ],
   "source": [
    "# returns spacy lemmas\n",
    "print([dt.DocParser.parse_tok(tok, lemmatize=True) for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'said', 'he', 'will', 'paint', 'the', 'house', 'red', 'for', '$', '20', '(', 'Twenty', 'Dollars', ')', '.', 'he', 'is', 'such', 'a', 'rule', '-', 'breaker', '.']\n"
     ]
    }
   ],
   "source": [
    "# additionally you can choose to format ents. This is often used so that named entities \n",
    "# with slightly different whitespaces (newlines or just spaces) can be considered as the same.\n",
    "# note that both \"Twenty\" and \"Dollars\" start with capital letters even though original sentence\n",
    "# does not have them capitalized. Essentially this process involves splitting and joining back with spaces,\n",
    "# then converting the strings to lower case and calling .capitalize() to convert the first letter of each \n",
    "# word to upper-case. Note that you can merge entities before parsing as well.\n",
    "print([dt.DocParser.parse_tok(tok, format_ents=True) for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', '_normtok_', '_normtok_', '_normtok_', '_normtok_', '_normtok_', '_normtok_', '_normtok_', '_normtok_', '_normtok_', '20', '_normtok_', 'Twenty', 'Dollars', '_normtok_', '_normtok_', '_normtok_', '_normtok_', '_normtok_', '_normtok_', '_normtok_', '_normtok_', '_normtok_', '_normtok_']\n",
      "\n",
      "['James', 'said ', 'he ', 'will ', 'paint ', 'the ', 'house ', 'red ', 'for ', '$', '20', '(', 'Twenty', 'Dollars', ')', '. ', 'He ', 'is ', 'such ', 'a ', 'rule', '-', 'breaker', '.']\n",
      "\n",
      "['James', ('said', 'VERB'), ('he', 'PRON'), ('will', 'AUX'), ('paint', 'VERB'), ('the', 'DET'), ('house', 'NOUN'), ('red', 'NOUN'), ('for', 'ADP'), ('$', 'SYM'), '20', ('(', 'PUNCT'), 'Twenty', 'Dollars', (')', 'PUNCT'), ('.', 'PUNCT'), ('he', 'PRON'), ('is', 'AUX'), ('such', 'DET'), ('a', 'DET'), ('rule', 'NOUN'), ('-', 'PUNCT'), ('breaker', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "# can provide additional post-processing that applies to only entities or normal tokens\n",
    "# (often they should be treated separately)\n",
    "print([dt.DocParser.parse_tok(tok, normal_convert=lambda t: '_normtok_') for tok in doc])\n",
    "print()\n",
    "print([dt.DocParser.parse_tok(tok, normal_convert=lambda t: t.text_with_ws) for tok in doc]) # includes whitespace\n",
    "print()\n",
    "print([dt.DocParser.parse_tok(tok, normal_convert=lambda t: (t.lower_.strip(), t.pos_)) for tok in doc]) # includes part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ORG', 'said', 'he', 'will', 'paint', 'the', 'house', 'red', 'for', '$', 'MONEY', '(', 'MONEY', 'MONEY', ')', '.', 'he', 'is', 'such', 'a', 'rule', '-', 'breaker', '.']\n",
      "\n",
      "[('james', 'ORG', 'PROPN'), 'said', 'he', 'will', 'paint', 'the', 'house', 'red', 'for', '$', ('20', 'MONEY', 'NUM'), '(', ('twenty', 'MONEY', 'NUM'), ('dollars', 'MONEY', 'NOUN'), ')', '.', 'he', 'is', 'such', 'a', 'rule', '-', 'breaker', '.']\n"
     ]
    }
   ],
   "source": [
    "# and can provide basically the same thing with named entities (while keeping normal token processing the same)\n",
    "print([dt.DocParser.parse_tok(tok, ent_convert=lambda t: t.ent_type_) for tok in doc])\n",
    "print()\n",
    "print([dt.DocParser.parse_tok(tok, ent_convert=lambda t: (t.lower_.strip(), t.ent_type_, t.pos_)) for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROPN', 'VERB', 'PRON', 'AUX', 'VERB', 'DET', 'NOUN', 'NOUN', 'ADP', 'SYM', 'NUM', 'PUNCT', 'NUM', 'NOUN', 'PUNCT', 'PUNCT', 'PRON', 'AUX', 'DET', 'DET', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "# can use both to similar effect\n",
    "print([dt.DocParser.parse_tok(tok, normal_convert=lambda t: t.pos_, ent_convert=lambda t: t.pos_) for tok in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Filtering\n",
    "As part of the DocParser pipeline, it is often useful to filter out tokens that are not useful. There are several useful options in `.use_tok()` for this. Note that this method can be overridden for use in other DocParser methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "# by default, all tokens except whitespace are included\n",
    "print([dt.DocParser.use_tok(tok) for tok in doc])\n",
    "# to turn whitespace filtering off, use filter_whitespace=False (no whitespace is in this example so can't show example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[James, said, he, will, paint, the, house, red, for, $, 20, twenty, dollars, He, is, such, a, rule, breaker]\n"
     ]
    }
   ],
   "source": [
    "# filter \n",
    "print([tok for tok in doc if dt.DocParser.use_tok(tok, filter_punct=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[James, said, paint, house, red, $, 20, (, dollars, ), ., rule, -, breaker, .]\n"
     ]
    }
   ],
   "source": [
    "# filter stopwords (according to spacy). To modify stopwords: https://stackoverflow.com/questions/41170726/add-remove-stop-words-with-spacy \n",
    "print([tok for tok in doc if dt.DocParser.use_tok(tok, filter_stop=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[James, said, paint, house, red, $, 20, dollars, rule, breaker]\n"
     ]
    }
   ],
   "source": [
    "# filter both\n",
    "print([tok for tok in doc if dt.DocParser.use_tok(tok, filter_stop=True, filter_punct=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[James, said, he, will, paint, the, house, red, for, $, (, twenty, dollars, ), ., He, is, such, a, rule, -, breaker, .]\n",
      "[James, said, he, will, paint, the, house, red, for, $, (, dollars, ), ., He, is, such, a, rule, -, breaker, .]\n"
     ]
    }
   ],
   "source": [
    "# filter_number would filter \"twenty\" wheras filter_digit would not\n",
    "print([tok for tok in doc if dt.DocParser.use_tok(tok, filter_digit=True)])\n",
    "print([tok for tok in doc if dt.DocParser.use_tok(tok, filter_num=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[said, he, will, paint, the, house, red, for, $, (, ), ., He, is, such, a, rule, -, breaker, .]\n"
     ]
    }
   ],
   "source": [
    "# filter named entities\n",
    "print([tok for tok in doc if dt.DocParser.use_tok(tok, filter_all_ents=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[James, said, he, will, paint, the, house, red, for, $, (, ), ., He, is, such, a, rule, -, breaker, .]\n"
     ]
    }
   ],
   "source": [
    "# filter only numbers. See this page for details on named entity types: https://spacy.io/api/annotation#named-entities\n",
    "print([tok for tok in doc if dt.DocParser.use_tok(tok, filter_ent_types=['MONEY',])])\n",
    "# filters \"twenty dollars\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Preprocessing\n",
    "A common step taken prior to using the spacy parser is to remove objects you don't want to be tokenized. So far this function is quite simple but more features may be added later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To search the web, see '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exstr = 'To search the web, see http://google.com'\n",
    "dt.DocParser.preprocess(exstr, remove_url=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
