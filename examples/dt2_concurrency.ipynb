{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concurrency\n",
    "Databases are able to handle concurrency easily. The best way to use concurrent connections is to set the timeout parameter passed to the sqlalchemy.engine function as an extra argument to the constructure (captured by `**engine_args`). Note that while the ``persistent_conn`` constructor argument may have other uses, it does not have much of an effect on concurrent operations in this version of DocTable2.\n",
    "\n",
    "Here I'll set up a simple test case for concurrent operations, where I run two threads simultaneously inserting many large data rows. We will see that when timeouts are set to be large, the threads take turns in inserting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from multiprocessing import Process\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import doctable as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define database schema to be sent to processes\n",
    "fname = 'tmp_connections2.db'\n",
    "schema = (\n",
    "    ('id', 'integer', dict(primary_key=True, autoincrement=True)),\n",
    "    ('data','pickle'),\n",
    "    ('procname','string')\n",
    ")\n",
    "big_data = [i for i in range(10000000)] # create big data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a thread that takes a schema and inserts three big rows\n",
    "def thread_writer(timeout, schema, fname, data, procname):\n",
    "    db = dt.DocTable2(schema, persistent_conn=True, fname=fname, connect_args={'timeout': timeout})\n",
    "    for i in range(3):\n",
    "        db.insert({'data':data,'procname':procname})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to run the processes defined by thread_writer\n",
    "def run_processes(timeout_sec, schema, fname, big_data):\n",
    "    \n",
    "    if os.path.exists(fname):\n",
    "        os.remove(fname)\n",
    "    \n",
    "    baseargs = (timeout_sec,schema,fname,big_data)\n",
    "    p1 = Process(target=thread_writer, args=(*baseargs,'p1'))\n",
    "    p2 = Process(target=thread_writer, args=(*baseargs,'p2'))\n",
    "    \n",
    "    p1.start(), p2.start() # start the processes\n",
    "    p1.join(), p2.join() # wait for processes to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct Example\n",
    "First we show a correctly working version with a sufficiently large timeout. Each thread will attempt to insert three large pickled objects (big lists) into the database. Because the timeout is sufficiently long, the processes will take turns in inserting data. You can see that effect by looking at the select statement below. The two threads have two different names that they inserted in the database, \"p1\" and \"p2\". The rows alternate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeout = 10 # seconds\n",
    "run_processes(timeout, schema, fname, big_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DocTable2::_documents_ ct: 6>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 'p1'), (2, 'p2'), (3, 'p1'), (4, 'p2'), (5, 'p1'), (6, 'p2')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = dt.DocTable2(fname=fname)\n",
    "print(db)\n",
    "db.select(['id','procname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure Example\n",
    "We can induce a failure case by setting the timeout to 0. Because the threads are inserting large python objects, one thread does not finish writing before the other attempts to write. When the insertion fails, it will give the python exception `sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) database is locked`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\", line 1249, in _execute_context\n",
      "    cursor, statement, parameters, context\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/default.py\", line 580, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "sqlite3.OperationalError: database is locked\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-23-c94c856dfbd3>\", line 5, in thread_writer\n",
      "    db.insert({'data':data,'procname':procname})\n",
      "  File \"../doctable/doctable2.py\", line 323, in insert\n",
      "    r = self.execute(q, **kwargs)\n",
      "  File \"../doctable/doctable2.py\", line 574, in execute\n",
      "    result = self._execute(query, **kwargs)\n",
      "  File \"../doctable/doctable2.py\", line 582, in _execute\n",
      "    r = self._conn.execute(query)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\", line 988, in execute\n",
      "    return meth(self, multiparams, params)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/sqlalchemy/sql/elements.py\", line 287, in _execute_on_connection\n",
      "    return connection._execute_clauseelement(self, multiparams, params)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\", line 1107, in _execute_clauseelement\n",
      "    distilled_params,\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\", line 1253, in _execute_context\n",
      "    e, statement, parameters, cursor, context\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\", line 1473, in _handle_dbapi_exception\n",
      "    util.raise_from_cause(sqlalchemy_exception, exc_info)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/sqlalchemy/util/compat.py\", line 398, in raise_from_cause\n",
      "    reraise(type(exception), exception, tb=exc_tb, cause=cause)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/sqlalchemy/util/compat.py\", line 152, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/base.py\", line 1249, in _execute_context\n",
      "    cursor, statement, parameters, context\n",
      "  File \"/home/utopia3/dc326/local/anaconda3/lib/python3.6/site-packages/sqlalchemy/engine/default.py\", line 580, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) database is locked\n",
      "[SQL: INSERT OR FAIL INTO _documents_ (data, procname) VALUES (?, ?)]\n",
      "[parameters: (<memory at 0x7fcfb12a9348>, 'p1')]\n",
      "(Background on this error at: http://sqlalche.me/e/e3q8)\n"
     ]
    }
   ],
   "source": [
    "timeout = 0 # seconds\n",
    "run_processes(timeout, schema, fname, big_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
