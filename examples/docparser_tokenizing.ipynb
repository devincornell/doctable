{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DocParser Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import doctable as dt\n",
    "from spacy import displacy\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DocParser is built specifically to convert spacy doc objects to token lists or simple parsetree objects which are convenient to store in a doctable. As such, we begin by creating a spacy doc object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "James will paint the house for $20 (twenty dollars). He is a rule-breaker"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exstr = 'James will paint the house for $20 (twenty dollars). He is a rule-breaker'\n",
    "doc = nlp(exstr)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Tokenizing\n",
    "\n",
    "Typically you will want to parse at the Spacy doc aobject. The `.tokenize_doc()` method includes common functionality for tokenizing your documents. Arguments to this function present a series of decisions that need to be made for every tokenization process. There are two "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '20', '(', 'twenty', 'dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "source": [
    "# the most basic version performs tokenizing with all default settings\n",
    "print(dt.DocParser.tokenize_doc(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['James', 'will', 'paint', 'the', 'house', 'for', '$', '20', '(', 'twenty', 'dollars', ')', '.'], ['he', 'is', 'a', 'rule', '-', 'breaker']]\n"
     ]
    }
   ],
   "source": [
    "# split into sentences (list of lists)\n",
    "print(dt.DocParser.tokenize_doc(doc, split_sents=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '20', '(', 'twenty dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "source": [
    "print(dt.DocParser.tokenize_doc(doc, merge_ents=True))\n",
    "doc = nlp(exstr) # reverts doc back to original because adding the match (called in .tokenize_doc()) modified it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'will', 'paint', 'the house', 'for', '$', '20', '(', 'twenty dollars', ')', '.', 'he', 'is', 'a rule-breaker']\n"
     ]
    }
   ],
   "source": [
    "print(dt.DocParser.tokenize_doc(doc, merge_noun_chunks=True))\n",
    "doc = nlp(exstr) # reverts doc back to original because adding the match (called in .tokenize_doc()) modified it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose to include tokens\n",
    "You may not want to include all tokens, depending on spacy token information. For this case, we use the `.use_tok()` method which includes some built-in arguments to do some boilerplate steps. Again see the [full documentaiton](https://devincornell.github.io/doctable/ref/doctable.DocParser.html) to see all arguments and defaults.\n",
    "The function simply returns a boolean True/False value given a spacy token, but can be passed to `.tokenize_doc()` for added flexibility.\n",
    "\n",
    "It is most easily used by overriding parameters through a lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '(', 'dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "source": [
    "# first try a custom function keeps only non-numbers\n",
    "use_tok_nobreaker = lambda tok: not tok.like_num\n",
    "print(dt.DocParser.tokenize_doc(doc, use_tok_func=use_tok_nobreaker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'paint', 'house', '$', '20', '(', 'dollars', ')', '.', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "source": [
    "# now, you can override the .use_tok() to take care of some simple stuff\n",
    "use_tok_nostop = lambda tok: dt.DocParser.use_tok(tok, filter_stop=True)\n",
    "print(dt.DocParser.tokenize_doc(doc, use_tok_func=use_tok_nostop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '(', 'twenty', 'dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "source": [
    "# remove digits\n",
    "use_tok_nodigit = lambda tok: dt.DocParser.use_tok(tok, filter_digit=True)\n",
    "print(dt.DocParser.tokenize_doc(doc, use_tok_func=use_tok_nodigit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '(', 'dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "source": [
    "# remove numbers (see it removed both \"20\" and \"Twenty\")\n",
    "use_tok_nonum = lambda tok: dt.DocParser.use_tok(tok, filter_num=True)\n",
    "print(dt.DocParser.tokenize_doc(doc, use_tok_func=use_tok_nonum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['will', 'paint', 'the', 'house', 'for', '$', '20', '(', 'twenty', 'dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "source": [
    "# here it thought 'James' was an organization. Use the filter_ent_types arg to remove specific ent types\n",
    "use_tok_nonames = lambda tok: dt.DocParser.use_tok(tok, filter_ent_types=['ORG'])\n",
    "print(dt.DocParser.tokenize_doc(doc, use_tok_func=use_tok_nonames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['will', 'paint', 'the', 'house', 'for', '$', '(', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "source": [
    "# remove all entities using the filter_all_ents argument\n",
    "use_tok_nonents = lambda tok: dt.DocParser.use_tok(tok, filter_all_ents=True)\n",
    "print(dt.DocParser.tokenize_doc(doc, use_tok_func=use_tok_nonents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'will', 'the', 'house', 'for', '$', '(', 'dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "source": [
    "# you can also add to the use_tok method using a custom function\n",
    "def custom_use_tok(tok):\n",
    "    use = dt.DocParser.use_tok(tok, filter_num=True)\n",
    "    return use and tok.pos_ != 'VERB' # here removes all verbs (including \"paint\")\n",
    "print(dt.DocParser.tokenize_doc(doc, use_tok_func=custom_use_tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose how to parse tokens\n",
    "Conversion from a spacy token to a string can happen a number of different ways. The `.parse_tok()` method provides a number of features for this task, or a custom function can be provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse_tok(tok, replace_num=None, replace_digit=None, lemmatize=False, normal_convert=None, format_ents=True, ent_convert=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '20', '(', 'twenty', 'dollars', ')', '.', 'He', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "source": [
    "# a custom function will simply return the original text using the tok.text property\n",
    "parse_tok = lambda tok: tok.text\n",
    "print(dt.DocParser.tokenize_doc(doc, parse_tok_func=parse_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '__NUM__', '(', '__NUM__', 'dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "source": [
    "# using .parse_tok(), first try replacing numbers with \"__NUM__\"\n",
    "parse_tok = lambda tok: dt.DocParser.parse_tok(tok, replace_num='__NUM__')\n",
    "print(dt.DocParser.tokenize_doc(doc, parse_tok_func=parse_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '20', '(', 'twenty', 'dollars', ')', '.', '-pron-', 'be', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "source": [
    "# now lemmatize\n",
    "parse_tok = lambda tok: dt.DocParser.parse_tok(tok, lemmatize=True)\n",
    "print(dt.DocParser.tokenize_doc(doc, parse_tok_func=parse_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '20', '(', 'Twenty', 'Dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "source": [
    "# format_ents is one of the most useful features. \n",
    "# It will standardize ents by converting all consecutive whitespace to \n",
    "# spaces and then capitalize the first letter. This is the default setting, but it can be turned off.\n",
    "parse_tok = lambda tok: dt.DocParser.parse_tok(tok, format_ents=True)\n",
    "print(dt.DocParser.tokenize_doc(doc, parse_tok_func=parse_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging N-Grams\n",
    "DocParser offers two convenient ways to work with n-grams: (1) using the spacy matcher and (2) using the post-processed multi-token matcher. The first method is applied after normal spacy processing is finished. It involves passing a tuple of ngrams as tuples to apply after all parsing has completed. The good thing about this approach is that it doesn't require much code. The unfortunate thing is that it can only access the tokens after normal parsing. If you would like to merge tokens with hyphens between them or currency symbols to their numbers, you should use the pre-processing method.\n",
    "\n",
    "The pre-processing spacy.Matcher functionality is used to create ngrams which access certain underlying spacy components like IS_DIGIT etc. See [Spacy Matcher documention for more details](https://spacy.io/usage/rule-based-matching). The basic workflow is to create a matcher object, add patterns, and then pass matcher to .tokenize_doc(). Note that since the doc object itself is modified, python must be restarted to revert back to other tokenization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'will', 'paint', 'the house', 'for', '$', '20', '(', 'twenty', 'dollars', ')', '.', 'he is a', 'rule - breaker']\n",
      "\n",
      "['James', 'will', 'paint', 'the_house', 'for', '$', '20', '(', 'twenty', 'dollars', ')', '.', 'he_is_a', 'rule_-_breaker']\n"
     ]
    }
   ],
   "source": [
    "# post-parsing ngram merging\n",
    "ngrams = (\n",
    "    ('the', 'house'),\n",
    "    ('rule', '-', 'breaker'),\n",
    "    ('he', 'is', 'a'),\n",
    ")\n",
    "# by default \n",
    "print(dt.DocParser.tokenize_doc(doc, ngrams=ngrams))\n",
    "print()\n",
    "print(dt.DocParser.tokenize_doc(doc, ngrams=ngrams, ngram_sep='_')) # specify ngram_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$20', '(', 'twenty', 'dollars', ')', '.', 'he', 'is', 'a', 'rule-breaker']\n"
     ]
    }
   ],
   "source": [
    "# spacy matcher object (will be passed to docparser)\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# matches currency numbers\n",
    "pattern = [{'TEXT':'$'},{'IS_DIGIT':True}]\n",
    "matcher.add('currency', None, pattern)\n",
    "\n",
    "# matches the phrase \"he will\" or \"He Will\" or \"HE WILL\"\n",
    "pattern2 = [{'LOWER':'he'},{'LOWER':'will'}]\n",
    "matcher.add('he_will', None, pattern2)\n",
    "\n",
    "# matches hyphens\n",
    "pattern3 = [{'IS_SPACE':False},{'TEXT':'-'},{'IS_SPACE':False}]\n",
    "matcher.add('he_will', None, pattern3)\n",
    "\n",
    "print([tok for tok in dt.DocParser.tokenize_doc(doc, spacy_ngram_matcher=matcher)])\n",
    "doc = nlp(exstr) # reverts doc back to original because adding the match (called in .tokenize_doc()) modified it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Parsing\n",
    "An essential element of any spacy tokenization process is to convert Spacy token objects into strings.\n",
    "\n",
    "Full function header: `def parse_tok(tok, replace_num=None, replace_digit=None, lemmatize=False, normal_convert=None, format_ents=True, ent_convert=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '20', '(', 'twenty', 'dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "source": [
    "# by default this method includes everything\n",
    "print([dt.DocParser.parse_tok(tok) for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '__NUM__', '(', '__NUM__', 'dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "source": [
    "# numbers include anything that looks like a number\n",
    "print([dt.DocParser.parse_tok(tok, replace_num='__NUM__') for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '__DIGIT__', '(', 'twenty', 'dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "source": [
    "# digits include 5 but not five\n",
    "print([dt.DocParser.parse_tok(tok, replace_digit='__DIGIT__') for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '20', '(', 'twenty', 'dollars', ')', '.', '-pron-', 'be', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "source": [
    "# returns spacy lemmas\n",
    "print([dt.DocParser.parse_tok(tok, lemmatize=True) for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '20', '(', 'Twenty', 'Dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "source": [
    "# additionally you can choose to format ents. This is often used so that named entities \n",
    "# with slightly different whitespaces (newlines or just spaces) can be considered as the same.\n",
    "# note that both \"Twenty\" and \"Dollars\" start with capital letters even though original sentence\n",
    "# does not have them capitalized. Essentially this process involves splitting and joining back with spaces,\n",
    "# then converting the strings to lower case and calling .capitalize() to convert the first letter of each \n",
    "# word to upper-case. Note that you can merge entities before parsing as well.\n",
    "print([dt.DocParser.parse_tok(tok, format_ents=True) for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', '_normtok_', '_normtok_', '_normtok_', '_normtok_', '_normtok_', '_normtok_', '20', '_normtok_', 'twenty', 'dollars', '_normtok_', '_normtok_', '_normtok_', '_normtok_', '_normtok_', '_normtok_', '_normtok_', '_normtok_']\n",
      "\n",
      "['James', 'will ', 'paint ', 'the ', 'house ', 'for ', '$', '20', '(', 'twenty', 'dollars', ')', '. ', 'He ', 'is ', 'a ', 'rule', '-', 'breaker']\n",
      "\n",
      "['James', ('will', 'AUX'), ('paint', 'VERB'), ('the', 'DET'), ('house', 'NOUN'), ('for', 'ADP'), ('$', 'SYM'), '20', ('(', 'PUNCT'), 'twenty', 'dollars', (')', 'PUNCT'), ('.', 'PUNCT'), ('he', 'PRON'), ('is', 'AUX'), ('a', 'DET'), ('rule', 'NOUN'), ('-', 'PUNCT'), ('breaker', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "# can provide additional post-processing that applies to only entities or normal tokens\n",
    "# (often they should be treated separately)\n",
    "print([dt.DocParser.parse_tok(tok, normal_convert=lambda t: '_normtok_') for tok in doc])\n",
    "print()\n",
    "print([dt.DocParser.parse_tok(tok, normal_convert=lambda t: t.text_with_ws) for tok in doc]) # includes whitespace\n",
    "print()\n",
    "print([dt.DocParser.parse_tok(tok, normal_convert=lambda t: (t.lower_.strip(), t.pos_)) for tok in doc]) # includes part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ORG', 'will', 'paint', 'the', 'house', 'for', '$', 'MONEY', '(', 'MONEY', 'MONEY', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n",
      "\n",
      "[('james', 'ORG', 'PROPN'), 'will', 'paint', 'the', 'house', 'for', '$', ('20', 'MONEY', 'NUM'), '(', ('twenty', 'MONEY', 'NUM'), ('dollars', 'MONEY', 'NOUN'), ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "source": [
    "# and can provide basically the same thing with named entities (while keeping normal token processing the same)\n",
    "print([dt.DocParser.parse_tok(tok, ent_convert=lambda t: t.ent_type_) for tok in doc])\n",
    "print()\n",
    "print([dt.DocParser.parse_tok(tok, ent_convert=lambda t: (t.lower_.strip(), t.ent_type_, t.pos_)) for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROPN', 'AUX', 'VERB', 'DET', 'NOUN', 'ADP', 'SYM', 'NUM', 'PUNCT', 'NUM', 'NOUN', 'PUNCT', 'PUNCT', 'PRON', 'AUX', 'DET', 'NOUN', 'PUNCT', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "# can use both to similar effect\n",
    "print([dt.DocParser.parse_tok(tok, normal_convert=lambda t: t.pos_, ent_convert=lambda t: t.pos_) for tok in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Filtering\n",
    "As part of the DocParser pipeline, it is often useful to filter out tokens that are not useful. There are several useful options in `.use_tok()` for this. Note that this method can be overridden for use in other DocParser methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "# by default, all tokens except whitespace are included\n",
    "print([dt.DocParser.use_tok(tok) for tok in doc])\n",
    "# to turn whitespace filtering off, use filter_whitespace=False (no whitespace is in this example so can't show example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[James, will, paint, the, house, for, $, 20, twenty, dollars, He, is, a, rule, breaker]\n"
     ]
    }
   ],
   "source": [
    "# filter \n",
    "print([tok for tok in doc if dt.DocParser.use_tok(tok, filter_punct=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[James, paint, house, $, 20, (, dollars, ), ., rule, -, breaker]\n"
     ]
    }
   ],
   "source": [
    "# filter stopwords (according to spacy). To modify stopwords: https://stackoverflow.com/questions/41170726/add-remove-stop-words-with-spacy \n",
    "print([tok for tok in doc if dt.DocParser.use_tok(tok, filter_stop=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[James, paint, house, $, 20, dollars, rule, breaker]\n"
     ]
    }
   ],
   "source": [
    "# filter both\n",
    "print([tok for tok in doc if dt.DocParser.use_tok(tok, filter_stop=True, filter_punct=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[James, will, paint, the, house, for, $, (, twenty, dollars, ), ., He, is, a, rule, -, breaker]\n",
      "[James, will, paint, the, house, for, $, (, dollars, ), ., He, is, a, rule, -, breaker]\n"
     ]
    }
   ],
   "source": [
    "# filter_number would filter \"twenty\" wheras filter_digit would not\n",
    "print([tok for tok in doc if dt.DocParser.use_tok(tok, filter_digit=True)])\n",
    "print([tok for tok in doc if dt.DocParser.use_tok(tok, filter_num=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[will, paint, the, house, for, $, (, ), ., He, is, a, rule, -, breaker]\n"
     ]
    }
   ],
   "source": [
    "# filter named entities\n",
    "print([tok for tok in doc if dt.DocParser.use_tok(tok, filter_all_ents=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[James, will, paint, the, house, for, $, (, ), ., He, is, a, rule, -, breaker]\n"
     ]
    }
   ],
   "source": [
    "# filter only numbers. See this page for details on named entity types: https://spacy.io/api/annotation#named-entities\n",
    "print([tok for tok in doc if dt.DocParser.use_tok(tok, filter_ent_types=['MONEY',])])\n",
    "# filters \"twenty dollars\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Preprocessing\n",
    "A common step taken prior to using the spacy parser is to remove objects you don't want to be tokenized. So far this function is quite simple but more features may be added later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "preprocess() got an unexpected keyword argument 'remove_url'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-d67c59e00f57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'To search the web, see http://google.com'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDocParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: preprocess() got an unexpected keyword argument 'remove_url'"
     ]
    }
   ],
   "source": [
    "exstr = 'To search the web, see http://google.com'\n",
    "dt.DocParser.preprocess(exstr, remove_url=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
