{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"doctable Python Package See the doctable website for documentation and examples. Created by Devin J. Cornell . Doctable has a new interface! The package has been updated with an entirely new API to improve on previous limitations and better match the Sqlalchemy 2.0 interface. Inspired by the attrs project , I used different names for functions and classes to make it clear that the interface has changed and open the possibility for backwards compatibility with upgraded internals in the future. For now, stick to installing from the legacy branch when using sqlalchemy <= 1.4, and the master branch for sqlalchemy >= 2.0. Installation From Python Package Index : pip install doctable For sqlalchemy >= 2.0: pip install --upgrade git+https://github.com/devincornell/doctable.git@master For sqlalchemy <= 1.4: pip install --upgrade git+https://github.com/devincornell/doctable.git@legacy Changes in Version 2.0 Create database connections using ConnectCore objects instead of ConnectEngine or DocTable objects. Database tables represented by DBTable objects instead of DocTable objects. All DBTable instances originate from a ConnectCore object. Create schemas using the doctable.table_schema decorator instead of the doctable.schema decorator. This new decorator includes constraint and index parameters as well as those for the dataclass objects. The Column function replaces Col as generic default parameter values with more fine-grained control over column properties. This function provides a clearer separation between parameters that affect the behavior of the object as a dataclass (supplied as a FieldArgs object) and those that affect the database column schema (supplied via a ColumnArgs object). New command line interface : you may execute doctable functions through the command line. Just use python -m doctable execute {args here} to see how to use it. Examples See the examples/ directory for more detailed examples. Basic steps These are the basic steps for using doctable to create a database connection, define a schema, and execute queries. For more examples, see the doctable website . 1. Create a database connection. core = doctable.ConnectCore.open( target=':memory:', # use a filename for a sqlite to write to disk dialect='sqlite', ) 2. Define a database schema from a dataclass. @doctable.table_schema class MyContainer: name: str age: int id: int = doctable.Column( column_args=doctable.ColumnArgs(order=0, primary_key=True, autoincrement=True), ) 3. Emit DDL to create a table from the schema. with core.begin_ddl() as emitter: tab0 = emitter.create_table(container_type=MyContainer) pprint.pprint(core.inspect_columns('MyContainer')) 4. Execute queries on the table. with tab1.query() as q: q.insert_single(MyContainer(name='devin', age=40)) print(q.select()) >> MyContainer(name='devin', age=40, id=1)","title":"Home"},{"location":"#doctable-python-package","text":"See the doctable website for documentation and examples. Created by Devin J. Cornell .","title":"doctable Python Package"},{"location":"#doctable-has-a-new-interface","text":"The package has been updated with an entirely new API to improve on previous limitations and better match the Sqlalchemy 2.0 interface. Inspired by the attrs project , I used different names for functions and classes to make it clear that the interface has changed and open the possibility for backwards compatibility with upgraded internals in the future. For now, stick to installing from the legacy branch when using sqlalchemy <= 1.4, and the master branch for sqlalchemy >= 2.0.","title":"Doctable has a new interface!"},{"location":"#installation","text":"From Python Package Index : pip install doctable For sqlalchemy >= 2.0: pip install --upgrade git+https://github.com/devincornell/doctable.git@master For sqlalchemy <= 1.4: pip install --upgrade git+https://github.com/devincornell/doctable.git@legacy","title":"Installation"},{"location":"#changes-in-version-20","text":"Create database connections using ConnectCore objects instead of ConnectEngine or DocTable objects. Database tables represented by DBTable objects instead of DocTable objects. All DBTable instances originate from a ConnectCore object. Create schemas using the doctable.table_schema decorator instead of the doctable.schema decorator. This new decorator includes constraint and index parameters as well as those for the dataclass objects. The Column function replaces Col as generic default parameter values with more fine-grained control over column properties. This function provides a clearer separation between parameters that affect the behavior of the object as a dataclass (supplied as a FieldArgs object) and those that affect the database column schema (supplied via a ColumnArgs object). New command line interface : you may execute doctable functions through the command line. Just use python -m doctable execute {args here} to see how to use it.","title":"Changes in Version 2.0"},{"location":"#examples","text":"See the examples/ directory for more detailed examples.","title":"Examples"},{"location":"#basic-steps","text":"These are the basic steps for using doctable to create a database connection, define a schema, and execute queries. For more examples, see the doctable website . 1. Create a database connection. core = doctable.ConnectCore.open( target=':memory:', # use a filename for a sqlite to write to disk dialect='sqlite', ) 2. Define a database schema from a dataclass. @doctable.table_schema class MyContainer: name: str age: int id: int = doctable.Column( column_args=doctable.ColumnArgs(order=0, primary_key=True, autoincrement=True), ) 3. Emit DDL to create a table from the schema. with core.begin_ddl() as emitter: tab0 = emitter.create_table(container_type=MyContainer) pprint.pprint(core.inspect_columns('MyContainer')) 4. Execute queries on the table. with tab1.query() as q: q.insert_single(MyContainer(name='devin', age=40)) print(q.select()) >> MyContainer(name='devin', age=40, id=1)","title":"Basic steps"},{"location":"documentation/ex_basics/","text":"Basics of doctable In this brief tutorial, we will cover the basics of doctable. We will cover the following topics: Connecting to the database using ConnectCore . Defining a database schema using the table_schema decorator. Creating the table using the begin_ddl() context manager. Inserting values into the database using the ConnectQuery and ConnectTable interfaces. import sys sys.path.append('../') import doctable import pprint The ConnectCore objects acts as the primary starting point for any actions performed on the database. We create a new connection to the datbase using the .open() factory method constructor. core = doctable.ConnectCore.open( target=':memory:', dialect='sqlite' ) core ConnectCore(target=':memory:', dialect='sqlite', engine=Engine(sqlite:///:memory:), metadata=MetaData()) Next we define a very basic schema using the table_schema decorator. This decorator is used to create a Container object, which contains information about the database schema and is also a dataclass that can be inserted or retrieved from the database. Read the schema definition examples for more information on creating container objects and database schemas. @doctable.table_schema class MyContainer0: id: int name: str age: int doctable.inspect_schema(MyContainer0).column_info_df() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Col Name Col Type Attr Name Hint Order Primary Key Foreign Key Index Default 0 id Integer id int (inf, 0) False False None None 1 name String name str (inf, 1) False False None None 2 age Integer age int (inf, 2) False False None None We actually connect to the database table using the context manager returned by .begin_ddl() . This design is necessary for multi-table schemas, but, because of the readability it provides, I will use it for single-table schemas as well. The method create_table_if_not_exists here returns a new instance of DBTable . Alternatively, we could reflect a database table, in which we would not be required to provide a schema container. with core.begin_ddl() as emitter: tab0 = emitter.create_table_if_not_exists(container_type=MyContainer0) for ci in core.inspect_columns('MyContainer0'): print(ci) {'name': 'id', 'type': INTEGER(), 'nullable': True, 'default': None, 'primary_key': 0} {'name': 'name', 'type': VARCHAR(), 'nullable': True, 'default': None, 'primary_key': 0} {'name': 'age', 'type': INTEGER(), 'nullable': True, 'default': None, 'primary_key': 0} We can perform queries on the database using the ConnectQuery interface returned from the ConnectCore.query() method. In this case, we insert a new row into the database using the insert_multi() method. Not that we will use an alternative interface for inserting container instances into the database. with core.query() as q: q.insert_multi(tab0, [ {'name': 'Devin J. Cornell', 'age': 50}, {'name': 'Dorothy Andrews', 'age': 49}, ]) print(q.select(tab0.all_cols()).all()) [(None, 'Devin J. Cornell', 50), (None, 'Dorothy Andrews', 49)] To insert container object instances into the table, I instead use the DBTable.query() method to generate a TableQuery instance. This behaves much like ConnectQuery except that returned data will be placed into new container instances and we may insert data from container instances directly. with tab0.query() as q: q.insert_single(MyContainer0(id=0, name='John Doe', age=30)) print(q.select()) [MyContainer0(id=None, name='Devin J. Cornell', age=50), MyContainer0(id=None, name='Dorothy Andrews', age=49), MyContainer0(id=0, name='John Doe', age=30)] Here I define a more complicated schema. The standard id column is now included. Notice that order=0 means the column will appear first in the table. The updated and added attributes have been created to automatically record the time of insertion and update. I added the birthyear method to the container type. import datetime @doctable.table_schema(table_name='mytable1') class MyContainer1: name: str age: int id: int = doctable.Column( column_args=doctable.ColumnArgs(order=0, primary_key=True, autoincrement=True), ) updated: datetime.datetime = doctable.Column( column_args=doctable.ColumnArgs(default=datetime.datetime.utcnow), ) added: datetime.datetime = doctable.Column( column_args=doctable.ColumnArgs( default=datetime.datetime.utcnow, onupdate=datetime.datetime.utcnow, ) ) def birthyear(self): '''Retrieve the birthyear of the person at the time this database entry was added.''' try: return self.added.year - self.age except AttributeError as e: raise AttributeError('Cannot calculate birthyear without the added date. ' 'Did you mean to call this on a retrieved container instance?') from e doctable.inspect_schema(MyContainer1).column_info_df() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Col Name Col Type Attr Name Hint Order Primary Key Foreign Key Index Default 0 id Integer id int (0, 2) True False None None 1 name String name str (inf, 0) False False None None 2 age Integer age int (inf, 1) False False None None 3 updated DateTime updated datetime (inf, 3) False False None utcnow 4 added DateTime added datetime (inf, 4) False False None utcnow We create this table just as we did the one before, and show the new schema using inspection. with core.begin_ddl() as emitter: tab1 = emitter.create_table_if_not_exists(container_type=MyContainer1) for ci in core.inspect_columns('mytable1'): print(ci) {'name': 'id', 'type': INTEGER(), 'nullable': True, 'default': None, 'primary_key': 1} {'name': 'name', 'type': VARCHAR(), 'nullable': True, 'default': None, 'primary_key': 0} {'name': 'age', 'type': INTEGER(), 'nullable': True, 'default': None, 'primary_key': 0} {'name': 'updated', 'type': DATETIME(), 'nullable': True, 'default': None, 'primary_key': 0} {'name': 'added', 'type': DATETIME(), 'nullable': True, 'default': None, 'primary_key': 0} We can create a containser instance just as we did before. Note that id , updated , and added are optionally now because we expect the database to create them. o = MyContainer1(name='John Doe', age=30) o MyContainer1(name='John Doe', age=30, id=MISSING, updated=MISSING, added=MISSING) As expected, calling .birthyear() raises an exception because the added entry has not been recorded - that will happen at insertion into the db. try: o.birthyear() except AttributeError as e: print('error raised:', e) error raised: Cannot calculate birthyear without the added date. Did you mean to call this on a retrieved container instance? After inserting the object into the database and retrieving it again, we can see that those previously missing fileds have been populated. with tab1.query() as q: q.insert_single(o) results = q.select() results[0] MyContainer1(name='John Doe', age=30, id=1, updated=datetime.datetime(2023, 11, 16, 17, 40, 7, 684832), added=datetime.datetime(2023, 11, 16, 17, 40, 7, 684836)) And now we can call the birthyear() method. results[0].birthyear() 1993 Conclusion For more detailed explanations of these topics, see the documentation and API reference provided on the website. Good luck!","title":"Basics of doctable"},{"location":"documentation/ex_basics/#basics-of-doctable","text":"In this brief tutorial, we will cover the basics of doctable. We will cover the following topics: Connecting to the database using ConnectCore . Defining a database schema using the table_schema decorator. Creating the table using the begin_ddl() context manager. Inserting values into the database using the ConnectQuery and ConnectTable interfaces. import sys sys.path.append('../') import doctable import pprint The ConnectCore objects acts as the primary starting point for any actions performed on the database. We create a new connection to the datbase using the .open() factory method constructor. core = doctable.ConnectCore.open( target=':memory:', dialect='sqlite' ) core ConnectCore(target=':memory:', dialect='sqlite', engine=Engine(sqlite:///:memory:), metadata=MetaData()) Next we define a very basic schema using the table_schema decorator. This decorator is used to create a Container object, which contains information about the database schema and is also a dataclass that can be inserted or retrieved from the database. Read the schema definition examples for more information on creating container objects and database schemas. @doctable.table_schema class MyContainer0: id: int name: str age: int doctable.inspect_schema(MyContainer0).column_info_df() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Col Name Col Type Attr Name Hint Order Primary Key Foreign Key Index Default 0 id Integer id int (inf, 0) False False None None 1 name String name str (inf, 1) False False None None 2 age Integer age int (inf, 2) False False None None We actually connect to the database table using the context manager returned by .begin_ddl() . This design is necessary for multi-table schemas, but, because of the readability it provides, I will use it for single-table schemas as well. The method create_table_if_not_exists here returns a new instance of DBTable . Alternatively, we could reflect a database table, in which we would not be required to provide a schema container. with core.begin_ddl() as emitter: tab0 = emitter.create_table_if_not_exists(container_type=MyContainer0) for ci in core.inspect_columns('MyContainer0'): print(ci) {'name': 'id', 'type': INTEGER(), 'nullable': True, 'default': None, 'primary_key': 0} {'name': 'name', 'type': VARCHAR(), 'nullable': True, 'default': None, 'primary_key': 0} {'name': 'age', 'type': INTEGER(), 'nullable': True, 'default': None, 'primary_key': 0} We can perform queries on the database using the ConnectQuery interface returned from the ConnectCore.query() method. In this case, we insert a new row into the database using the insert_multi() method. Not that we will use an alternative interface for inserting container instances into the database. with core.query() as q: q.insert_multi(tab0, [ {'name': 'Devin J. Cornell', 'age': 50}, {'name': 'Dorothy Andrews', 'age': 49}, ]) print(q.select(tab0.all_cols()).all()) [(None, 'Devin J. Cornell', 50), (None, 'Dorothy Andrews', 49)] To insert container object instances into the table, I instead use the DBTable.query() method to generate a TableQuery instance. This behaves much like ConnectQuery except that returned data will be placed into new container instances and we may insert data from container instances directly. with tab0.query() as q: q.insert_single(MyContainer0(id=0, name='John Doe', age=30)) print(q.select()) [MyContainer0(id=None, name='Devin J. Cornell', age=50), MyContainer0(id=None, name='Dorothy Andrews', age=49), MyContainer0(id=0, name='John Doe', age=30)] Here I define a more complicated schema. The standard id column is now included. Notice that order=0 means the column will appear first in the table. The updated and added attributes have been created to automatically record the time of insertion and update. I added the birthyear method to the container type. import datetime @doctable.table_schema(table_name='mytable1') class MyContainer1: name: str age: int id: int = doctable.Column( column_args=doctable.ColumnArgs(order=0, primary_key=True, autoincrement=True), ) updated: datetime.datetime = doctable.Column( column_args=doctable.ColumnArgs(default=datetime.datetime.utcnow), ) added: datetime.datetime = doctable.Column( column_args=doctable.ColumnArgs( default=datetime.datetime.utcnow, onupdate=datetime.datetime.utcnow, ) ) def birthyear(self): '''Retrieve the birthyear of the person at the time this database entry was added.''' try: return self.added.year - self.age except AttributeError as e: raise AttributeError('Cannot calculate birthyear without the added date. ' 'Did you mean to call this on a retrieved container instance?') from e doctable.inspect_schema(MyContainer1).column_info_df() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Col Name Col Type Attr Name Hint Order Primary Key Foreign Key Index Default 0 id Integer id int (0, 2) True False None None 1 name String name str (inf, 0) False False None None 2 age Integer age int (inf, 1) False False None None 3 updated DateTime updated datetime (inf, 3) False False None utcnow 4 added DateTime added datetime (inf, 4) False False None utcnow We create this table just as we did the one before, and show the new schema using inspection. with core.begin_ddl() as emitter: tab1 = emitter.create_table_if_not_exists(container_type=MyContainer1) for ci in core.inspect_columns('mytable1'): print(ci) {'name': 'id', 'type': INTEGER(), 'nullable': True, 'default': None, 'primary_key': 1} {'name': 'name', 'type': VARCHAR(), 'nullable': True, 'default': None, 'primary_key': 0} {'name': 'age', 'type': INTEGER(), 'nullable': True, 'default': None, 'primary_key': 0} {'name': 'updated', 'type': DATETIME(), 'nullable': True, 'default': None, 'primary_key': 0} {'name': 'added', 'type': DATETIME(), 'nullable': True, 'default': None, 'primary_key': 0} We can create a containser instance just as we did before. Note that id , updated , and added are optionally now because we expect the database to create them. o = MyContainer1(name='John Doe', age=30) o MyContainer1(name='John Doe', age=30, id=MISSING, updated=MISSING, added=MISSING) As expected, calling .birthyear() raises an exception because the added entry has not been recorded - that will happen at insertion into the db. try: o.birthyear() except AttributeError as e: print('error raised:', e) error raised: Cannot calculate birthyear without the added date. Did you mean to call this on a retrieved container instance? After inserting the object into the database and retrieving it again, we can see that those previously missing fileds have been populated. with tab1.query() as q: q.insert_single(o) results = q.select() results[0] MyContainer1(name='John Doe', age=30, id=1, updated=datetime.datetime(2023, 11, 16, 17, 40, 7, 684832), added=datetime.datetime(2023, 11, 16, 17, 40, 7, 684836)) And now we can call the birthyear() method. results[0].birthyear() 1993","title":"Basics of doctable"},{"location":"documentation/ex_basics/#conclusion","text":"For more detailed explanations of these topics, see the documentation and API reference provided on the website. Good luck!","title":"Conclusion"},{"location":"documentation/ex_insert_delete/","text":"Insert and Delete Queries with doctable In this document I will describe the interface for performing insert and delete queries with doctable. import pandas as pd import numpy as np import typing import sys sys.path.append('..') import doctable Define a demonstration schema The very first step is to define a table schema that will be appropriate for our examples. This table includes the typical id column (the first column, specified by order=0 ), as well as string, integer, and boolean attributes. The object used to specify the schema is called a container , and I will use that terminology as we go. @doctable.table_schema class Record: name: str = doctable.Column(column_args=doctable.ColumnArgs(nullable=False, unique=True)) age: int = doctable.Column() is_old: bool = doctable.Column() id: int = doctable.Column( column_args=doctable.ColumnArgs( order = 0, primary_key=True, autoincrement=True ), ) core = doctable.ConnectCore.open(target=':memory:', dialect='sqlite', echo=True) with core.begin_ddl() as ddl: rtab = ddl.create_table_if_not_exists(container_type=Record) 2023-11-13 14:46:03,418 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,418 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"Record\") 2023-11-13 14:46:03,419 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-13 14:46:03,420 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"Record\") 2023-11-13 14:46:03,421 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-13 14:46:03,422 INFO sqlalchemy.engine.Engine CREATE TABLE \"Record\" ( id INTEGER, age INTEGER, is_old INTEGER, name VARCHAR NOT NULL, PRIMARY KEY (id), UNIQUE (name) ) 2023-11-13 14:46:03,423 INFO sqlalchemy.engine.Engine [no key 0.00050s] () 2023-11-13 14:46:03,424 INFO sqlalchemy.engine.Engine COMMIT Two Interfaces: ConnectQuery and TableQuery First, a little about the doctable query interface. There are two interfaces for performing queries: ConnectQuery and TableQuery . ConnectQuery table-agnostic interface for querying any table in any result format. Create this object using the ConnectCore.query() method. TableQuery table-specific interface for querying a specific table. Insert and select from container objects used to define the schema. Create this object using the DBTable.query() method. Inserts via ConnectQuery First I will discuss the ConnectQuery interface, which is created via the ConnectCore.query() method. This object maintains a database connection, and, when used as a context manager, will commit all changes upon exit. It is fine to use the ConnectQuery object without a context manager for queries that do not require commits. There are two primary methods for insertions via the ConnectQuery interface, which you can see in this table. Both accept a single DBTable object, followed by one or multiple dictionaries of data to insert, depending on the method. Method Description insert_single() Insert a single row into a table. insert_multi() Insert multiple rows into a table. with core.query() as q: q.insert_single(rtab, { 'name': 'test_A', 'age': 10, 'is_old': False, }) q.insert_multi(rtab, data = [ { 'name': 'test_B', 'age': 10, 'is_old': False, }, { 'name': 'test_C', 'age': 10, 'is_old': False, } ]) q.select(rtab.all_cols()).df() 2023-11-13 14:46:03,478 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,480 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (age, is_old, name) VALUES (?, ?, ?) 2023-11-13 14:46:03,482 INFO sqlalchemy.engine.Engine [generated in 0.00420s] (10, False, 'test_A') 2023-11-13 14:46:03,487 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (age, is_old, name) VALUES (?, ?, ?) 2023-11-13 14:46:03,489 INFO sqlalchemy.engine.Engine [generated in 0.00216s] [(10, False, 'test_B'), (10, False, 'test_C')] 2023-11-13 14:46:03,491 INFO sqlalchemy.engine.Engine COMMIT 2023-11-13 14:46:03,494 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,495 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 14:46:03,497 INFO sqlalchemy.engine.Engine [generated in 0.00319s] () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 1 10 0 test_A 1 2 10 0 test_B 2 3 10 0 test_C Omit Attributes If some values are not provided, the database will decide which values they take. In this case, the database populates the ID column according to the schema (it acts as the primary key in this case). core.query().insert_single(rtab, { 'name': 'test_D', }) core.query().select(rtab.all_cols()).df() 2023-11-13 14:46:03,544 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,546 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (name) VALUES (?) 2023-11-13 14:46:03,548 INFO sqlalchemy.engine.Engine [generated in 0.00389s] ('test_D',) 2023-11-13 14:46:03,551 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,552 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 14:46:03,554 INFO sqlalchemy.engine.Engine [cached since 0.06015s ago] () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 1 10.0 0.0 test_A 1 2 10.0 0.0 test_B 2 3 10.0 0.0 test_C 3 4 NaN NaN test_D Note that in our schema we set nullable=False for the name column, so this must be provided in an insert otherwise there will be an error. This typically results in an sqlalchemy.exc.IntegrityError , which you may catch if needed. import sqlalchemy.exc try: core.query().insert_single(rtab, { 'is_old': True, }) except sqlalchemy.exc.IntegrityError as e: print(type(e), e) 2023-11-13 14:46:03,605 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,607 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (is_old) VALUES (?) 2023-11-13 14:46:03,609 INFO sqlalchemy.engine.Engine [generated in 0.00380s] (True,) <class 'sqlalchemy.exc.IntegrityError'> (sqlite3.IntegrityError) NOT NULL constraint failed: Record.name [SQL: INSERT OR FAIL INTO \"Record\" (is_old) VALUES (?)] [parameters: (True,)] (Background on this error at: https://sqlalche.me/e/20/gkpj) ifnotunique Parameter The ifnotunique paramter controls the behavior when a unique constraint is violated. The default value is FAIL , which will raise an error when a unique constraint is violated - it will raise an sqlalchemy.exc.IntegrityError exception in this case. The other options are IGNORE , meaning inserted rows that violate the constraints should be ignored, and REPLACE , which will replace the existing row with the new row. In the Record table we have created, there is a unique constraint on name . We will receive an integrity error if we try to insert a duplicate there when using the default ifnotunique='ERROR' . try: core.query().insert_single(rtab, { 'name': 'test_A', }) except sqlalchemy.exc.IntegrityError as e: print(type(e), e) 2023-11-13 14:46:03,665 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,668 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (name) VALUES (?) 2023-11-13 14:46:03,669 INFO sqlalchemy.engine.Engine [cached since 0.1247s ago] ('test_A',) <class 'sqlalchemy.exc.IntegrityError'> (sqlite3.IntegrityError) UNIQUE constraint failed: Record.name [SQL: INSERT OR FAIL INTO \"Record\" (name) VALUES (?)] [parameters: ('test_A',)] (Background on this error at: https://sqlalche.me/e/20/gkpj) When using ifnotunique='REPLACE' , the insert will replace the existing row with the new row. This is useful when you want to update a row if it already exists, but insert it if it does not. core.query().insert_single(rtab, { 'name': 'test_A', }, ifnotunique='REPLACE') core.query().select(rtab.all_cols()).df() 2023-11-13 14:46:03,728 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,731 INFO sqlalchemy.engine.Engine INSERT OR REPLACE INTO \"Record\" (name) VALUES (?) 2023-11-13 14:46:03,733 INFO sqlalchemy.engine.Engine [generated in 0.00453s] ('test_A',) 2023-11-13 14:46:03,738 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,739 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 14:46:03,741 INFO sqlalchemy.engine.Engine [cached since 0.2469s ago] () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 2 10.0 0.0 test_B 1 3 10.0 0.0 test_C 2 4 NaN NaN test_D 3 5 NaN NaN test_A Inserts via TableQuery The TableQuery interface is created via the DBTable.query() method. This object is table-specific, and is used to insert and select from a single table. As such, inserts ONLY accept the Record container objects used to define the schema. Following the ConnectQuery interface, there are two methods for inserting data into a table: .insert_single() and .insert_multi() . rtab.query().insert_single(Record(name='test_E', is_old=False, age=10)) rtab.query().select(rtab.all_cols()) 2023-11-13 14:46:03,806 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,809 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (age, is_old, name) VALUES (?, ?, ?) 2023-11-13 14:46:03,811 INFO sqlalchemy.engine.Engine [cached since 0.3334s ago] (10, False, 'test_E') 2023-11-13 14:46:03,814 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,815 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 14:46:03,816 INFO sqlalchemy.engine.Engine [cached since 0.3218s ago] () [Record(name='test_B', age=10, is_old=0, id=2), Record(name='test_C', age=10, is_old=0, id=3), Record(name='test_D', age=None, is_old=None, id=4), Record(name='test_A', age=None, is_old=None, id=5), Record(name='test_E', age=10, is_old=0, id=6)] rtab.query().insert_multi([ Record(name='test_F', is_old=False, age=10), Record(name='test_G', is_old=True, age=80), ]) rtab.query().select(rtab.all_cols()) 2023-11-13 14:46:03,866 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,869 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (age, is_old, name) VALUES (?, ?, ?) 2023-11-13 14:46:03,870 INFO sqlalchemy.engine.Engine [cached since 0.3834s ago] [(10, False, 'test_F'), (80, True, 'test_G')] 2023-11-13 14:46:03,874 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,875 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 14:46:03,876 INFO sqlalchemy.engine.Engine [cached since 0.3818s ago] () [Record(name='test_B', age=10, is_old=0, id=2), Record(name='test_C', age=10, is_old=0, id=3), Record(name='test_D', age=None, is_old=None, id=4), Record(name='test_A', age=None, is_old=None, id=5), Record(name='test_E', age=10, is_old=0, id=6), Record(name='test_F', age=10, is_old=0, id=7), Record(name='test_G', age=80, is_old=1, id=8)] The doctable.MISSING Sentinel Lets now take a closer look at the container object behavior. Notice that in the schema definition we gave default values of doctable.Column , which we used to specify additional attributes. This automatically sets the default value for the dataclass to be doctable.MISSING , which is special because it will be ignored when inserting - instead, it will let the database decide how to handle it. This is especially useful for columns like id , which are intended to be automatically generated by the database. We can see this when we omit attributes from the object. test_record = Record(name='test_H') test_record Record(name='test_H', age=MISSING, is_old=MISSING, id=MISSING) Those values will be omitted in the insert, filled in by the db, and be returned upon selection. rtab.query().insert_single(test_record) rtab.query().select(rtab.all_cols()) 2023-11-13 14:46:03,982 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,985 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (name) VALUES (?) 2023-11-13 14:46:03,987 INFO sqlalchemy.engine.Engine [cached since 0.4426s ago] ('test_H',) 2023-11-13 14:46:03,990 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,991 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 14:46:03,992 INFO sqlalchemy.engine.Engine [cached since 0.4979s ago] () [Record(name='test_B', age=10, is_old=0, id=2), Record(name='test_C', age=10, is_old=0, id=3), Record(name='test_D', age=None, is_old=None, id=4), Record(name='test_A', age=None, is_old=None, id=5), Record(name='test_E', age=10, is_old=0, id=6), Record(name='test_F', age=10, is_old=0, id=7), Record(name='test_G', age=80, is_old=1, id=8), Record(name='test_H', age=None, is_old=None, id=9)] If we select a subset of columns, the missing values will refer to doctable.MISSING , even though the attributes will continue to exist. rtab.query().select(rtab.cols('id', 'name')) 2023-11-13 14:46:04,041 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:04,044 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".name FROM \"Record\" 2023-11-13 14:46:04,045 INFO sqlalchemy.engine.Engine [generated in 0.00350s] () [Record(name='test_A', age=MISSING, is_old=MISSING, id=5), Record(name='test_B', age=MISSING, is_old=MISSING, id=2), Record(name='test_C', age=MISSING, is_old=MISSING, id=3), Record(name='test_D', age=MISSING, is_old=MISSING, id=4), Record(name='test_E', age=MISSING, is_old=MISSING, id=6), Record(name='test_F', age=MISSING, is_old=MISSING, id=7), Record(name='test_G', age=MISSING, is_old=MISSING, id=8), Record(name='test_H', age=MISSING, is_old=MISSING, id=9)] Note that the doctable.MISSING will never be inserted into the databse because it will be ignored. Deletion Interface Deleting rows is pretty straightforward when using either the ConnectQuery or TableQuery interfaces. In fact, it is the exact same for both. The only parameters are where and wherestr (where you can add additional conditionals as strings). print(core.query().select([rtab['id'].count()]).scalar_one()) with rtab.query() as q: q.delete(where=rtab['name']=='test_A') core.query().select([rtab['id'].count()]).scalar_one() 2023-11-13 14:46:31,486 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:31,489 INFO sqlalchemy.engine.Engine SELECT count(\"Record\".id) AS count_1 FROM \"Record\" 2023-11-13 14:46:31,490 INFO sqlalchemy.engine.Engine [cached since 27.39s ago] () 7 2023-11-13 14:46:31,493 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:31,495 INFO sqlalchemy.engine.Engine DELETE FROM \"Record\" WHERE \"Record\".name = ? 2023-11-13 14:46:31,497 INFO sqlalchemy.engine.Engine [cached since 27.39s ago] ('test_A',) 2023-11-13 14:46:31,498 INFO sqlalchemy.engine.Engine COMMIT 2023-11-13 14:46:31,500 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:31,500 INFO sqlalchemy.engine.Engine SELECT count(\"Record\".id) AS count_1 FROM \"Record\" 2023-11-13 14:46:31,501 INFO sqlalchemy.engine.Engine [cached since 27.4s ago] () 7 To delete all columns, pass the all=True flag. This prevents the user from accidentally deleting all rows. with rtab.query() as q: q.delete(all=True) core.query().select([rtab['id'].count()]).scalar_one() 2023-11-13 14:48:08,718 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:48:08,720 INFO sqlalchemy.engine.Engine DELETE FROM \"Record\" 2023-11-13 14:48:08,722 INFO sqlalchemy.engine.Engine [cached since 13.97s ago] () 2023-11-13 14:48:08,724 INFO sqlalchemy.engine.Engine COMMIT 2023-11-13 14:48:08,726 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:48:08,727 INFO sqlalchemy.engine.Engine SELECT count(\"Record\".id) AS count_1 FROM \"Record\" 2023-11-13 14:48:08,729 INFO sqlalchemy.engine.Engine [cached since 124.6s ago] () 0","title":"Insert and Delete Queries with doctable"},{"location":"documentation/ex_insert_delete/#insert-and-delete-queries-with-doctable","text":"In this document I will describe the interface for performing insert and delete queries with doctable. import pandas as pd import numpy as np import typing import sys sys.path.append('..') import doctable","title":"Insert and Delete Queries with doctable"},{"location":"documentation/ex_insert_delete/#define-a-demonstration-schema","text":"The very first step is to define a table schema that will be appropriate for our examples. This table includes the typical id column (the first column, specified by order=0 ), as well as string, integer, and boolean attributes. The object used to specify the schema is called a container , and I will use that terminology as we go. @doctable.table_schema class Record: name: str = doctable.Column(column_args=doctable.ColumnArgs(nullable=False, unique=True)) age: int = doctable.Column() is_old: bool = doctable.Column() id: int = doctable.Column( column_args=doctable.ColumnArgs( order = 0, primary_key=True, autoincrement=True ), ) core = doctable.ConnectCore.open(target=':memory:', dialect='sqlite', echo=True) with core.begin_ddl() as ddl: rtab = ddl.create_table_if_not_exists(container_type=Record) 2023-11-13 14:46:03,418 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,418 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"Record\") 2023-11-13 14:46:03,419 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-13 14:46:03,420 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"Record\") 2023-11-13 14:46:03,421 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-13 14:46:03,422 INFO sqlalchemy.engine.Engine CREATE TABLE \"Record\" ( id INTEGER, age INTEGER, is_old INTEGER, name VARCHAR NOT NULL, PRIMARY KEY (id), UNIQUE (name) ) 2023-11-13 14:46:03,423 INFO sqlalchemy.engine.Engine [no key 0.00050s] () 2023-11-13 14:46:03,424 INFO sqlalchemy.engine.Engine COMMIT","title":"Define a demonstration schema"},{"location":"documentation/ex_insert_delete/#two-interfaces-connectquery-and-tablequery","text":"First, a little about the doctable query interface. There are two interfaces for performing queries: ConnectQuery and TableQuery . ConnectQuery table-agnostic interface for querying any table in any result format. Create this object using the ConnectCore.query() method. TableQuery table-specific interface for querying a specific table. Insert and select from container objects used to define the schema. Create this object using the DBTable.query() method.","title":"Two Interfaces: ConnectQuery and TableQuery"},{"location":"documentation/ex_insert_delete/#inserts-via-connectquery","text":"First I will discuss the ConnectQuery interface, which is created via the ConnectCore.query() method. This object maintains a database connection, and, when used as a context manager, will commit all changes upon exit. It is fine to use the ConnectQuery object without a context manager for queries that do not require commits. There are two primary methods for insertions via the ConnectQuery interface, which you can see in this table. Both accept a single DBTable object, followed by one or multiple dictionaries of data to insert, depending on the method. Method Description insert_single() Insert a single row into a table. insert_multi() Insert multiple rows into a table. with core.query() as q: q.insert_single(rtab, { 'name': 'test_A', 'age': 10, 'is_old': False, }) q.insert_multi(rtab, data = [ { 'name': 'test_B', 'age': 10, 'is_old': False, }, { 'name': 'test_C', 'age': 10, 'is_old': False, } ]) q.select(rtab.all_cols()).df() 2023-11-13 14:46:03,478 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,480 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (age, is_old, name) VALUES (?, ?, ?) 2023-11-13 14:46:03,482 INFO sqlalchemy.engine.Engine [generated in 0.00420s] (10, False, 'test_A') 2023-11-13 14:46:03,487 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (age, is_old, name) VALUES (?, ?, ?) 2023-11-13 14:46:03,489 INFO sqlalchemy.engine.Engine [generated in 0.00216s] [(10, False, 'test_B'), (10, False, 'test_C')] 2023-11-13 14:46:03,491 INFO sqlalchemy.engine.Engine COMMIT 2023-11-13 14:46:03,494 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,495 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 14:46:03,497 INFO sqlalchemy.engine.Engine [generated in 0.00319s] () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 1 10 0 test_A 1 2 10 0 test_B 2 3 10 0 test_C","title":"Inserts via ConnectQuery"},{"location":"documentation/ex_insert_delete/#omit-attributes","text":"If some values are not provided, the database will decide which values they take. In this case, the database populates the ID column according to the schema (it acts as the primary key in this case). core.query().insert_single(rtab, { 'name': 'test_D', }) core.query().select(rtab.all_cols()).df() 2023-11-13 14:46:03,544 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,546 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (name) VALUES (?) 2023-11-13 14:46:03,548 INFO sqlalchemy.engine.Engine [generated in 0.00389s] ('test_D',) 2023-11-13 14:46:03,551 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,552 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 14:46:03,554 INFO sqlalchemy.engine.Engine [cached since 0.06015s ago] () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 1 10.0 0.0 test_A 1 2 10.0 0.0 test_B 2 3 10.0 0.0 test_C 3 4 NaN NaN test_D Note that in our schema we set nullable=False for the name column, so this must be provided in an insert otherwise there will be an error. This typically results in an sqlalchemy.exc.IntegrityError , which you may catch if needed. import sqlalchemy.exc try: core.query().insert_single(rtab, { 'is_old': True, }) except sqlalchemy.exc.IntegrityError as e: print(type(e), e) 2023-11-13 14:46:03,605 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,607 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (is_old) VALUES (?) 2023-11-13 14:46:03,609 INFO sqlalchemy.engine.Engine [generated in 0.00380s] (True,) <class 'sqlalchemy.exc.IntegrityError'> (sqlite3.IntegrityError) NOT NULL constraint failed: Record.name [SQL: INSERT OR FAIL INTO \"Record\" (is_old) VALUES (?)] [parameters: (True,)] (Background on this error at: https://sqlalche.me/e/20/gkpj)","title":"Omit Attributes"},{"location":"documentation/ex_insert_delete/#ifnotunique-parameter","text":"The ifnotunique paramter controls the behavior when a unique constraint is violated. The default value is FAIL , which will raise an error when a unique constraint is violated - it will raise an sqlalchemy.exc.IntegrityError exception in this case. The other options are IGNORE , meaning inserted rows that violate the constraints should be ignored, and REPLACE , which will replace the existing row with the new row. In the Record table we have created, there is a unique constraint on name . We will receive an integrity error if we try to insert a duplicate there when using the default ifnotunique='ERROR' . try: core.query().insert_single(rtab, { 'name': 'test_A', }) except sqlalchemy.exc.IntegrityError as e: print(type(e), e) 2023-11-13 14:46:03,665 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,668 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (name) VALUES (?) 2023-11-13 14:46:03,669 INFO sqlalchemy.engine.Engine [cached since 0.1247s ago] ('test_A',) <class 'sqlalchemy.exc.IntegrityError'> (sqlite3.IntegrityError) UNIQUE constraint failed: Record.name [SQL: INSERT OR FAIL INTO \"Record\" (name) VALUES (?)] [parameters: ('test_A',)] (Background on this error at: https://sqlalche.me/e/20/gkpj) When using ifnotunique='REPLACE' , the insert will replace the existing row with the new row. This is useful when you want to update a row if it already exists, but insert it if it does not. core.query().insert_single(rtab, { 'name': 'test_A', }, ifnotunique='REPLACE') core.query().select(rtab.all_cols()).df() 2023-11-13 14:46:03,728 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,731 INFO sqlalchemy.engine.Engine INSERT OR REPLACE INTO \"Record\" (name) VALUES (?) 2023-11-13 14:46:03,733 INFO sqlalchemy.engine.Engine [generated in 0.00453s] ('test_A',) 2023-11-13 14:46:03,738 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,739 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 14:46:03,741 INFO sqlalchemy.engine.Engine [cached since 0.2469s ago] () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 2 10.0 0.0 test_B 1 3 10.0 0.0 test_C 2 4 NaN NaN test_D 3 5 NaN NaN test_A","title":"ifnotunique Parameter"},{"location":"documentation/ex_insert_delete/#inserts-via-tablequery","text":"The TableQuery interface is created via the DBTable.query() method. This object is table-specific, and is used to insert and select from a single table. As such, inserts ONLY accept the Record container objects used to define the schema. Following the ConnectQuery interface, there are two methods for inserting data into a table: .insert_single() and .insert_multi() . rtab.query().insert_single(Record(name='test_E', is_old=False, age=10)) rtab.query().select(rtab.all_cols()) 2023-11-13 14:46:03,806 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,809 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (age, is_old, name) VALUES (?, ?, ?) 2023-11-13 14:46:03,811 INFO sqlalchemy.engine.Engine [cached since 0.3334s ago] (10, False, 'test_E') 2023-11-13 14:46:03,814 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,815 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 14:46:03,816 INFO sqlalchemy.engine.Engine [cached since 0.3218s ago] () [Record(name='test_B', age=10, is_old=0, id=2), Record(name='test_C', age=10, is_old=0, id=3), Record(name='test_D', age=None, is_old=None, id=4), Record(name='test_A', age=None, is_old=None, id=5), Record(name='test_E', age=10, is_old=0, id=6)] rtab.query().insert_multi([ Record(name='test_F', is_old=False, age=10), Record(name='test_G', is_old=True, age=80), ]) rtab.query().select(rtab.all_cols()) 2023-11-13 14:46:03,866 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,869 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (age, is_old, name) VALUES (?, ?, ?) 2023-11-13 14:46:03,870 INFO sqlalchemy.engine.Engine [cached since 0.3834s ago] [(10, False, 'test_F'), (80, True, 'test_G')] 2023-11-13 14:46:03,874 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,875 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 14:46:03,876 INFO sqlalchemy.engine.Engine [cached since 0.3818s ago] () [Record(name='test_B', age=10, is_old=0, id=2), Record(name='test_C', age=10, is_old=0, id=3), Record(name='test_D', age=None, is_old=None, id=4), Record(name='test_A', age=None, is_old=None, id=5), Record(name='test_E', age=10, is_old=0, id=6), Record(name='test_F', age=10, is_old=0, id=7), Record(name='test_G', age=80, is_old=1, id=8)]","title":"Inserts via TableQuery"},{"location":"documentation/ex_insert_delete/#the-doctablemissing-sentinel","text":"Lets now take a closer look at the container object behavior. Notice that in the schema definition we gave default values of doctable.Column , which we used to specify additional attributes. This automatically sets the default value for the dataclass to be doctable.MISSING , which is special because it will be ignored when inserting - instead, it will let the database decide how to handle it. This is especially useful for columns like id , which are intended to be automatically generated by the database. We can see this when we omit attributes from the object. test_record = Record(name='test_H') test_record Record(name='test_H', age=MISSING, is_old=MISSING, id=MISSING) Those values will be omitted in the insert, filled in by the db, and be returned upon selection. rtab.query().insert_single(test_record) rtab.query().select(rtab.all_cols()) 2023-11-13 14:46:03,982 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,985 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (name) VALUES (?) 2023-11-13 14:46:03,987 INFO sqlalchemy.engine.Engine [cached since 0.4426s ago] ('test_H',) 2023-11-13 14:46:03,990 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:03,991 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 14:46:03,992 INFO sqlalchemy.engine.Engine [cached since 0.4979s ago] () [Record(name='test_B', age=10, is_old=0, id=2), Record(name='test_C', age=10, is_old=0, id=3), Record(name='test_D', age=None, is_old=None, id=4), Record(name='test_A', age=None, is_old=None, id=5), Record(name='test_E', age=10, is_old=0, id=6), Record(name='test_F', age=10, is_old=0, id=7), Record(name='test_G', age=80, is_old=1, id=8), Record(name='test_H', age=None, is_old=None, id=9)] If we select a subset of columns, the missing values will refer to doctable.MISSING , even though the attributes will continue to exist. rtab.query().select(rtab.cols('id', 'name')) 2023-11-13 14:46:04,041 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:04,044 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".name FROM \"Record\" 2023-11-13 14:46:04,045 INFO sqlalchemy.engine.Engine [generated in 0.00350s] () [Record(name='test_A', age=MISSING, is_old=MISSING, id=5), Record(name='test_B', age=MISSING, is_old=MISSING, id=2), Record(name='test_C', age=MISSING, is_old=MISSING, id=3), Record(name='test_D', age=MISSING, is_old=MISSING, id=4), Record(name='test_E', age=MISSING, is_old=MISSING, id=6), Record(name='test_F', age=MISSING, is_old=MISSING, id=7), Record(name='test_G', age=MISSING, is_old=MISSING, id=8), Record(name='test_H', age=MISSING, is_old=MISSING, id=9)] Note that the doctable.MISSING will never be inserted into the databse because it will be ignored.","title":"The doctable.MISSING Sentinel"},{"location":"documentation/ex_insert_delete/#deletion-interface","text":"Deleting rows is pretty straightforward when using either the ConnectQuery or TableQuery interfaces. In fact, it is the exact same for both. The only parameters are where and wherestr (where you can add additional conditionals as strings). print(core.query().select([rtab['id'].count()]).scalar_one()) with rtab.query() as q: q.delete(where=rtab['name']=='test_A') core.query().select([rtab['id'].count()]).scalar_one() 2023-11-13 14:46:31,486 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:31,489 INFO sqlalchemy.engine.Engine SELECT count(\"Record\".id) AS count_1 FROM \"Record\" 2023-11-13 14:46:31,490 INFO sqlalchemy.engine.Engine [cached since 27.39s ago] () 7 2023-11-13 14:46:31,493 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:31,495 INFO sqlalchemy.engine.Engine DELETE FROM \"Record\" WHERE \"Record\".name = ? 2023-11-13 14:46:31,497 INFO sqlalchemy.engine.Engine [cached since 27.39s ago] ('test_A',) 2023-11-13 14:46:31,498 INFO sqlalchemy.engine.Engine COMMIT 2023-11-13 14:46:31,500 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:46:31,500 INFO sqlalchemy.engine.Engine SELECT count(\"Record\".id) AS count_1 FROM \"Record\" 2023-11-13 14:46:31,501 INFO sqlalchemy.engine.Engine [cached since 27.4s ago] () 7 To delete all columns, pass the all=True flag. This prevents the user from accidentally deleting all rows. with rtab.query() as q: q.delete(all=True) core.query().select([rtab['id'].count()]).scalar_one() 2023-11-13 14:48:08,718 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:48:08,720 INFO sqlalchemy.engine.Engine DELETE FROM \"Record\" 2023-11-13 14:48:08,722 INFO sqlalchemy.engine.Engine [cached since 13.97s ago] () 2023-11-13 14:48:08,724 INFO sqlalchemy.engine.Engine COMMIT 2023-11-13 14:48:08,726 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 14:48:08,727 INFO sqlalchemy.engine.Engine SELECT count(\"Record\".id) AS count_1 FROM \"Record\" 2023-11-13 14:48:08,729 INFO sqlalchemy.engine.Engine [cached since 124.6s ago] () 0","title":"Deletion Interface"},{"location":"documentation/ex_schemas/","text":"Table Schemas In this document, I give some examples for defining single and multi-table database schemas in Python. import sys sys.path.append('../') import doctable import pprint Containers and the table_schema decorator The first step in using doctable is to define a container object. Container objects are defined using the table_schema decorator, and are used both to define the schema of a database table and to wrap the data for insertion and selection. Container objects act very similar to normal dataclasses - in fact, they actually are dataclasses with additional information needed to create the database table attached. This informaiton is collected at the time when the decorator is used, and thus the decorator serves only to parse the database schema from the class definition, attach that information to the container class, and return the container type as a dataclass. @doctable.table_schema # equivalent to @doctable.table_schema() class Container1: name: str ins = doctable.inspect_schema(Container1) print(ins.table_name()) ins.column_info_df() Container1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Col Name Col Type Attr Name Hint Order Primary Key Foreign Key Index Default 0 name String name str (inf, 0) False False None None @doctable.table_schema(table_name='container2') class Container2: name: str age: int ins = doctable.inspect_schema(Container2) print(ins.table_name()) ins.column_info_df() container2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Col Name Col Type Attr Name Hint Order Primary Key Foreign Key Index Default 0 name String name str (inf, 0) False False None None 1 age Integer age int (inf, 1) False False None None Specifying Column Properties There are two sets of parameters you may adjust to change the behavior of a column: ColumnArgs : adjust the behavior of the generated column. This does not affect the container object, but does affect the database column. FieldArgs : adjust the behavior of container attribute by passing arguments to dataclasses.field() . This does not affect the database column, but does affect the way the container object can be used. Both are passed directly to the Column function, which, as you can see, simply returns a dataclasses.field object with column arguments passed to the metadata attribute. Note that by default, the default argument is set to doctable.MISSING , so the parameter is optional and will be populated with that value. Missing values will be ignored when inserting the object into the database. doctable.Column( column_args=doctable.ColumnArgs(), field_args=doctable.FieldArgs(), ) Field(name=None,type=None,default=MISSING,default_factory=<dataclasses._MISSING_TYPE object at 0x7fc0f9afe590>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'_column_args': ColumnArgs(order=inf, column_name=None, type_kwargs={}, use_type=None, sqlalchemy_type=None, autoincrement=False, nullable=True, unique=None, primary_key=False, index=None, foreign_key=None, default=None, onupdate=None, server_default=None, server_onupdate=None, comment=None, other_kwargs={})}),kw_only=<dataclasses._MISSING_TYPE object at 0x7fc0f9afe590>,_field_type=None) import datetime class PhoneNumber(str): pass class Address(str): pass @doctable.table_schema(table_name='container3') class Container3: name: str age: int = doctable.Column(field_args=doctable.FieldArgs(init_required=True)) address: Address = doctable.Column() phone: PhoneNumber = doctable.Column() # this column will appear first in the database, even though this attribute is later _id: int = doctable.Column( column_args=doctable.ColumnArgs( column_name='id', # name of the column in the db (might not want to have an attr called 'id') order = 0, # affects the ordering of the columns in the db primary_key=True, autoincrement=True, ), ) # doctable will define default and onupdate when inserting into database added: datetime.datetime = doctable.Column( column_args=doctable.ColumnArgs( default=datetime.datetime.now, onupdate=datetime.datetime.now ), field_args = doctable.FieldArgs( repr=False, # don't show this field when printing ) ) doctable.inspect_schema(Container3).column_info_df() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Col Name Col Type Attr Name Hint Order Primary Key Foreign Key Index Default 0 id Integer _id int (0, 4) True False None None 1 name String name str (inf, 0) False False None None 2 age Integer age int (inf, 1) False False None None 3 address String address Address (inf, 2) False False None None 4 phone String phone PhoneNumber (inf, 3) False False None None 5 added DateTime added datetime (inf, 5) False False None now Notice that the string representation does not show the added attribute, as specified via FieldAargs(repr=False) . Container3('Devin J. Cornell', 30) Container3(name='Devin J. Cornell', age=30, address=MISSING, phone=MISSING, _id=MISSING) Indices Indices may be added to a table by passing a dictionary of name, Index pairs to the indices parameter of the table_schema decorator. The arguments are the columns, and any additional keyword arguments may be passed after. @doctable.table_schema( table_name='container4', indices = { 'ind_name': doctable.Index('name'), 'ind_name_age': doctable.Index('name', 'age', unique=True), } ) class Container4: name: str age: int ins = doctable.inspect_schema(Container4) ins.index_info_df() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name columns kwargs 0 ind_name name 1 ind_name_age name, age unique: True Constraints You may pass constraints through the constraint parameter of the table_schema decorator. There are several types of constraints you may want to use in your schema. The following methods are thin wrappers over the SQLAlchemy objects of the same name . docs Constraint Description link ForeignKey(local_columns, foreign_columns, optional[onupdate], optional[ondelete]) A foreign key constraint. link CheckConstraint(text, optional[Name]) A unique constraint. link UniqueConstraint(*column_names, optional[name]) A unique constraint. link PrimaryKeyConstraint(*column_names, optional[name]) A unique constraint. @doctable.table_schema( table_name='container5', constraints = [ #doctable.ForeignKey(..), # see multi-table schemas below doctable.CheckConstraint('age >= 0', name='check_age'), doctable.UniqueConstraint('age', 'name', name='unique_age_name'), doctable.PrimaryKeyConstraint('id'), ] ) class Container5: id: int # this is the primary key now name: str age: int Column Types The column type resolution works according to the following steps: Check ColumnArgs.sqlalchemy_type and use this if it is not None . Check if column is foreign key - if it is, ask sqlalchemy to resolve the type Check ColumnArgs.use_type and use this if it is provided. Use the provided type hint to resolve the type. The valid type hints and their sqlalchemy equivalents are listed below. Type Hint SQLAlchemy Type int sqlalchemy.Integer float sqlalchemy.Float bool sqlalchemy.Boolean str sqlalchemy.String bytes sqlalchemy.LargeBinary datetime.datetime sqlalchemy.DateTime datetime.time sqlalchemy.Time datetime.date sqlalchemy.Date typing.Any sqlalchemy.PickleType 'datetime.datetime' sqlalchemy.DateTime 'datetime.time' sqlalchemy.Time 'datetime.date' sqlalchemy.Date 'Any' sqlalchemy.PickleType You can get the mappings programatically if needed as well: doctable.type_mappings() {int: sqlalchemy.sql.sqltypes.Integer, float: sqlalchemy.sql.sqltypes.Float, bool: sqlalchemy.sql.sqltypes.Boolean, str: sqlalchemy.sql.sqltypes.String, bytes: sqlalchemy.sql.sqltypes.LargeBinary, datetime.datetime: sqlalchemy.sql.sqltypes.DateTime, datetime.time: sqlalchemy.sql.sqltypes.Time, datetime.date: sqlalchemy.sql.sqltypes.Date, doctable.schema.column.column_types.PickleType: sqlalchemy.sql.sqltypes.PickleType, 'datetime.datetime': sqlalchemy.sql.sqltypes.DateTime, 'datetime.time': sqlalchemy.sql.sqltypes.Time, 'datetime.date': sqlalchemy.sql.sqltypes.Date, doctable.schema.column.column_types.JSON: sqlalchemy.sql.sqltypes.JSON} Special Column Types There are several special column types that can be used in your schemas. Type Hint SQLAlchemy Type Description doctable.JSON sqlalchemy.JSON Calls json.dumps on write, json.loads on read. doctable.PickleType sqlalchemy.PickleType Calls pickle.dumps on write, pickle.loads on read. import dataclasses import typing @dataclasses.dataclass class Address: street: str city: str state: str zip: str @doctable.table_schema class Container6: name: str # NOTE: will be serialized as a JSON string in the database # notice how we can use a more accurate type hint and still specify # the column type using use_type other_info: typing.Dict[str, typing.Union[str,int,float]] = doctable.Column( column_args=doctable.ColumnArgs( use_type=doctable.JSON, ), field_args=doctable.FieldArgs(default_factory=dict), ) # NOTE: will be pickled in the database address: Address = doctable.Column( column_args=doctable.ColumnArgs( use_type=doctable.PickleType, ) ) doctable.inspect_schema(Container6).column_info_df() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Col Name Col Type Attr Name Hint Order Primary Key Foreign Key Index Default 0 name String name str (inf, 0) False False None None 1 other_info JSON other_info Dict (inf, 1) False False None None 2 address PickleType address Address (inf, 2) False False None None Now create a new container object that contains an address for insertion. new_obj = Container6( name = 'Devin J. Cornell', other_info = {'favorite_color': 'blue'}, address = Address('123 Main St.', 'San Francisco', 'CA', '94122'), ) new_obj Container6(name='Devin J. Cornell', other_info={'favorite_color': 'blue'}, address=Address(street='123 Main St.', city='San Francisco', state='CA', zip='94122')) Now we open a new database, insert the row, and query it back - you can see that the dict data was converted to json and back again, and the address was converted to pickle data and back again. core = doctable.ConnectCore.open(':memory:', 'sqlite') with core.begin_ddl() as ddl: tab = ddl.create_table(Container6) with tab.query() as q: q.insert_single(new_obj) with core.query() as q: result = q.select(tab.all_cols()) result.first() ('Devin J. Cornell', {'favorite_color': 'blue'}, Address(street='123 Main St.', city='San Francisco', state='CA', zip='94122')) Multi-table Schemas The example below shows two linked tables: one for colors, and the other for people. Each person has a favorite color that is constrained by a foriegn key to the colors table. The colors table also has a unique constraint on the color name. I demonstrate use of the Column function to describe behavior of columns - specifically the use of ColumnArgs to specify additional column features that are not conveyed through type annotations or attribute names. I also show use of the Index object for creating indexes, the UniqueConstraint object for creating unique constraints, and the ForeignKey object for creating foreign key constraints. Note that the container object representing the database schema is also a usable dataclass that can used like any other container object. In fact, tables created according to this schema can insert these objects directly and will wrap return values issued via select queries. import datetime @doctable.table_schema( table_name='color', constraints = [ doctable.UniqueConstraint('name'), ] ) class Color: name: str id: int = doctable.Column( column_args=doctable.ColumnArgs( primary_key=True, autoincrement=True, ) ) # lets say we use this instead of an int class PersonID(int): pass # add table-level parameters to this decorator @doctable.table_schema( table_name='person', indices = { 'ind_name_birthday': doctable.Index('name', 'birthday', unique=True), }, constraints = [ # these constraints are set on the database doctable.CheckConstraint('length(address) > 0'), # cannot have a blank address doctable.UniqueConstraint('birthday', 'fav_color'), doctable.ForeignKey(['fav_color'], ['color.name'], onupdate='CASCADE', ondelete='CASCADE'), ], frozen = True, # parameter passed to dataclasses.dataclass ) class Person: name: str # default value will be \"not provided\" - good standardization address: str = doctable.Column( column_args=doctable.ColumnArgs( server_default='not provided', ) ) # provided as datetime, set to be indexed birthday: datetime.datetime = doctable.Column( column_args=doctable.ColumnArgs( index = True, ) ) # note that this has a foreign key constraint above fav_color: str = doctable.Column( column_args=doctable.ColumnArgs( nullable=False, ) ) id: PersonID = doctable.Column( # standard id column column_args=doctable.ColumnArgs( order=0, # will be the first column primary_key=True, autoincrement=True ), ) # doctable will define default and onupdate when inserting into database added: datetime.datetime = doctable.Column( column_args=doctable.ColumnArgs( index=True, default=datetime.datetime.utcnow, onupdate=datetime.datetime.utcnow ) ) # this property will not be stored in the database # - it acts like any other property @property def age(self): return datetime.datetime.now() - self.birthday core = doctable.ConnectCore.open( target=':memory:', dialect='sqlite' ) # NOTE: weird error when trying to run this twice after defining containers with core.begin_ddl() as emitter: core.enable_foreign_keys() # NOTE: NEEDED TO ENABLE FOREIGN KEYS color_tab = emitter.create_table_if_not_exists(container_type=Color) person_tab = emitter.create_table_if_not_exists(container_type=Person) for col_info in person_tab.inspect_columns(): print(f'{col_info[\"name\"]}: {col_info[\"type\"]}') id: INTEGER name: VARCHAR address: VARCHAR birthday: DATETIME fav_color: VARCHAR added: DATETIME Insertion into the color table is fairly straightforward. color_names = ['red', 'green', 'blue'] colors = [Color(name=name) for name in color_names] with color_tab.query() as q: q.insert_multi(colors) for c in q.select(): print(c) #print(q.select()) Color(name='red', id=1) Color(name='green', id=2) Color(name='blue', id=3) Insertion into the person table is similar, and note that we see an exception if we try to insert a person with a favorite color that is not in the color table. persons = [ Person(name='John', birthday=datetime.datetime(1990, 1, 1), fav_color='red'), Person(name='Sue', birthday=datetime.datetime(1991, 1, 1), fav_color='green'), Person(name='Ren', birthday=datetime.datetime(1995, 1, 1), fav_color='blue'), ] other_person = Person( name='Bob', address='123 Main St', birthday=datetime.datetime(1990, 1, 1), fav_color='other', # NOTE: THIS WILL CAUSE AN ERROR (NOT IN COLOR TABLE) ) import sqlalchemy.exc sec_in_one_year = 24*60*60*365 with person_tab.query() as q: q.insert_multi(persons, ifnotunique='replace') try: q.insert_single(other_person, ifnotunique='replace') print(f'THIS SHOULD NOT APPEAR') except sqlalchemy.exc.IntegrityError as e: print(f'successfully threw exception: {e}') for p in q.select(): print(f'{p.name} ({p.fav_color}): {p.age.total_seconds()//sec_in_one_year:0.0f} y/o') successfully threw exception: (sqlite3.IntegrityError) FOREIGN KEY constraint failed [SQL: INSERT OR REPLACE INTO person (name, address, birthday, fav_color, added) VALUES (?, ?, ?, ?, ?)] [parameters: ('Bob', '123 Main St', '1990-01-01 00:00:00.000000', 'other', '2023-11-14 22:17:40.402308')] (Background on this error at: https://sqlalche.me/e/20/gkpj) John (red): 33 y/o Sue (green): 32 y/o Ren (blue): 28 y/o The foreign key works as expected because we set onupdate : changing that value in the parent table will update the value in the child table. with color_tab.query() as q: q.update_single(dict(name='reddish'), where=color_tab['name']=='red') for c in q.select(): print(c) with person_tab.query() as q: for p in q.select(): print(f'{p.name} ({p.fav_color}): {p.age.total_seconds()//sec_in_one_year:0.0f} y/o') Color(name='reddish', id=1) Color(name='green', id=2) Color(name='blue', id=3) John (reddish): 33 y/o Sue (green): 32 y/o Ren (blue): 28 y/o","title":"Table Schemas"},{"location":"documentation/ex_schemas/#table-schemas","text":"In this document, I give some examples for defining single and multi-table database schemas in Python. import sys sys.path.append('../') import doctable import pprint","title":"Table Schemas"},{"location":"documentation/ex_schemas/#containers-and-the-table_schema-decorator","text":"The first step in using doctable is to define a container object. Container objects are defined using the table_schema decorator, and are used both to define the schema of a database table and to wrap the data for insertion and selection. Container objects act very similar to normal dataclasses - in fact, they actually are dataclasses with additional information needed to create the database table attached. This informaiton is collected at the time when the decorator is used, and thus the decorator serves only to parse the database schema from the class definition, attach that information to the container class, and return the container type as a dataclass. @doctable.table_schema # equivalent to @doctable.table_schema() class Container1: name: str ins = doctable.inspect_schema(Container1) print(ins.table_name()) ins.column_info_df() Container1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Col Name Col Type Attr Name Hint Order Primary Key Foreign Key Index Default 0 name String name str (inf, 0) False False None None @doctable.table_schema(table_name='container2') class Container2: name: str age: int ins = doctable.inspect_schema(Container2) print(ins.table_name()) ins.column_info_df() container2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Col Name Col Type Attr Name Hint Order Primary Key Foreign Key Index Default 0 name String name str (inf, 0) False False None None 1 age Integer age int (inf, 1) False False None None","title":"Containers and the table_schema decorator"},{"location":"documentation/ex_schemas/#specifying-column-properties","text":"There are two sets of parameters you may adjust to change the behavior of a column: ColumnArgs : adjust the behavior of the generated column. This does not affect the container object, but does affect the database column. FieldArgs : adjust the behavior of container attribute by passing arguments to dataclasses.field() . This does not affect the database column, but does affect the way the container object can be used. Both are passed directly to the Column function, which, as you can see, simply returns a dataclasses.field object with column arguments passed to the metadata attribute. Note that by default, the default argument is set to doctable.MISSING , so the parameter is optional and will be populated with that value. Missing values will be ignored when inserting the object into the database. doctable.Column( column_args=doctable.ColumnArgs(), field_args=doctable.FieldArgs(), ) Field(name=None,type=None,default=MISSING,default_factory=<dataclasses._MISSING_TYPE object at 0x7fc0f9afe590>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({'_column_args': ColumnArgs(order=inf, column_name=None, type_kwargs={}, use_type=None, sqlalchemy_type=None, autoincrement=False, nullable=True, unique=None, primary_key=False, index=None, foreign_key=None, default=None, onupdate=None, server_default=None, server_onupdate=None, comment=None, other_kwargs={})}),kw_only=<dataclasses._MISSING_TYPE object at 0x7fc0f9afe590>,_field_type=None) import datetime class PhoneNumber(str): pass class Address(str): pass @doctable.table_schema(table_name='container3') class Container3: name: str age: int = doctable.Column(field_args=doctable.FieldArgs(init_required=True)) address: Address = doctable.Column() phone: PhoneNumber = doctable.Column() # this column will appear first in the database, even though this attribute is later _id: int = doctable.Column( column_args=doctable.ColumnArgs( column_name='id', # name of the column in the db (might not want to have an attr called 'id') order = 0, # affects the ordering of the columns in the db primary_key=True, autoincrement=True, ), ) # doctable will define default and onupdate when inserting into database added: datetime.datetime = doctable.Column( column_args=doctable.ColumnArgs( default=datetime.datetime.now, onupdate=datetime.datetime.now ), field_args = doctable.FieldArgs( repr=False, # don't show this field when printing ) ) doctable.inspect_schema(Container3).column_info_df() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Col Name Col Type Attr Name Hint Order Primary Key Foreign Key Index Default 0 id Integer _id int (0, 4) True False None None 1 name String name str (inf, 0) False False None None 2 age Integer age int (inf, 1) False False None None 3 address String address Address (inf, 2) False False None None 4 phone String phone PhoneNumber (inf, 3) False False None None 5 added DateTime added datetime (inf, 5) False False None now Notice that the string representation does not show the added attribute, as specified via FieldAargs(repr=False) . Container3('Devin J. Cornell', 30) Container3(name='Devin J. Cornell', age=30, address=MISSING, phone=MISSING, _id=MISSING)","title":"Specifying Column Properties"},{"location":"documentation/ex_schemas/#indices","text":"Indices may be added to a table by passing a dictionary of name, Index pairs to the indices parameter of the table_schema decorator. The arguments are the columns, and any additional keyword arguments may be passed after. @doctable.table_schema( table_name='container4', indices = { 'ind_name': doctable.Index('name'), 'ind_name_age': doctable.Index('name', 'age', unique=True), } ) class Container4: name: str age: int ins = doctable.inspect_schema(Container4) ins.index_info_df() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name columns kwargs 0 ind_name name 1 ind_name_age name, age unique: True","title":"Indices"},{"location":"documentation/ex_schemas/#constraints","text":"You may pass constraints through the constraint parameter of the table_schema decorator. There are several types of constraints you may want to use in your schema. The following methods are thin wrappers over the SQLAlchemy objects of the same name . docs Constraint Description link ForeignKey(local_columns, foreign_columns, optional[onupdate], optional[ondelete]) A foreign key constraint. link CheckConstraint(text, optional[Name]) A unique constraint. link UniqueConstraint(*column_names, optional[name]) A unique constraint. link PrimaryKeyConstraint(*column_names, optional[name]) A unique constraint. @doctable.table_schema( table_name='container5', constraints = [ #doctable.ForeignKey(..), # see multi-table schemas below doctable.CheckConstraint('age >= 0', name='check_age'), doctable.UniqueConstraint('age', 'name', name='unique_age_name'), doctable.PrimaryKeyConstraint('id'), ] ) class Container5: id: int # this is the primary key now name: str age: int","title":"Constraints"},{"location":"documentation/ex_schemas/#column-types","text":"The column type resolution works according to the following steps: Check ColumnArgs.sqlalchemy_type and use this if it is not None . Check if column is foreign key - if it is, ask sqlalchemy to resolve the type Check ColumnArgs.use_type and use this if it is provided. Use the provided type hint to resolve the type. The valid type hints and their sqlalchemy equivalents are listed below. Type Hint SQLAlchemy Type int sqlalchemy.Integer float sqlalchemy.Float bool sqlalchemy.Boolean str sqlalchemy.String bytes sqlalchemy.LargeBinary datetime.datetime sqlalchemy.DateTime datetime.time sqlalchemy.Time datetime.date sqlalchemy.Date typing.Any sqlalchemy.PickleType 'datetime.datetime' sqlalchemy.DateTime 'datetime.time' sqlalchemy.Time 'datetime.date' sqlalchemy.Date 'Any' sqlalchemy.PickleType You can get the mappings programatically if needed as well: doctable.type_mappings() {int: sqlalchemy.sql.sqltypes.Integer, float: sqlalchemy.sql.sqltypes.Float, bool: sqlalchemy.sql.sqltypes.Boolean, str: sqlalchemy.sql.sqltypes.String, bytes: sqlalchemy.sql.sqltypes.LargeBinary, datetime.datetime: sqlalchemy.sql.sqltypes.DateTime, datetime.time: sqlalchemy.sql.sqltypes.Time, datetime.date: sqlalchemy.sql.sqltypes.Date, doctable.schema.column.column_types.PickleType: sqlalchemy.sql.sqltypes.PickleType, 'datetime.datetime': sqlalchemy.sql.sqltypes.DateTime, 'datetime.time': sqlalchemy.sql.sqltypes.Time, 'datetime.date': sqlalchemy.sql.sqltypes.Date, doctable.schema.column.column_types.JSON: sqlalchemy.sql.sqltypes.JSON}","title":"Column Types"},{"location":"documentation/ex_schemas/#special-column-types","text":"There are several special column types that can be used in your schemas. Type Hint SQLAlchemy Type Description doctable.JSON sqlalchemy.JSON Calls json.dumps on write, json.loads on read. doctable.PickleType sqlalchemy.PickleType Calls pickle.dumps on write, pickle.loads on read. import dataclasses import typing @dataclasses.dataclass class Address: street: str city: str state: str zip: str @doctable.table_schema class Container6: name: str # NOTE: will be serialized as a JSON string in the database # notice how we can use a more accurate type hint and still specify # the column type using use_type other_info: typing.Dict[str, typing.Union[str,int,float]] = doctable.Column( column_args=doctable.ColumnArgs( use_type=doctable.JSON, ), field_args=doctable.FieldArgs(default_factory=dict), ) # NOTE: will be pickled in the database address: Address = doctable.Column( column_args=doctable.ColumnArgs( use_type=doctable.PickleType, ) ) doctable.inspect_schema(Container6).column_info_df() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Col Name Col Type Attr Name Hint Order Primary Key Foreign Key Index Default 0 name String name str (inf, 0) False False None None 1 other_info JSON other_info Dict (inf, 1) False False None None 2 address PickleType address Address (inf, 2) False False None None Now create a new container object that contains an address for insertion. new_obj = Container6( name = 'Devin J. Cornell', other_info = {'favorite_color': 'blue'}, address = Address('123 Main St.', 'San Francisco', 'CA', '94122'), ) new_obj Container6(name='Devin J. Cornell', other_info={'favorite_color': 'blue'}, address=Address(street='123 Main St.', city='San Francisco', state='CA', zip='94122')) Now we open a new database, insert the row, and query it back - you can see that the dict data was converted to json and back again, and the address was converted to pickle data and back again. core = doctable.ConnectCore.open(':memory:', 'sqlite') with core.begin_ddl() as ddl: tab = ddl.create_table(Container6) with tab.query() as q: q.insert_single(new_obj) with core.query() as q: result = q.select(tab.all_cols()) result.first() ('Devin J. Cornell', {'favorite_color': 'blue'}, Address(street='123 Main St.', city='San Francisco', state='CA', zip='94122'))","title":"Special Column Types"},{"location":"documentation/ex_schemas/#multi-table-schemas","text":"The example below shows two linked tables: one for colors, and the other for people. Each person has a favorite color that is constrained by a foriegn key to the colors table. The colors table also has a unique constraint on the color name. I demonstrate use of the Column function to describe behavior of columns - specifically the use of ColumnArgs to specify additional column features that are not conveyed through type annotations or attribute names. I also show use of the Index object for creating indexes, the UniqueConstraint object for creating unique constraints, and the ForeignKey object for creating foreign key constraints. Note that the container object representing the database schema is also a usable dataclass that can used like any other container object. In fact, tables created according to this schema can insert these objects directly and will wrap return values issued via select queries. import datetime @doctable.table_schema( table_name='color', constraints = [ doctable.UniqueConstraint('name'), ] ) class Color: name: str id: int = doctable.Column( column_args=doctable.ColumnArgs( primary_key=True, autoincrement=True, ) ) # lets say we use this instead of an int class PersonID(int): pass # add table-level parameters to this decorator @doctable.table_schema( table_name='person', indices = { 'ind_name_birthday': doctable.Index('name', 'birthday', unique=True), }, constraints = [ # these constraints are set on the database doctable.CheckConstraint('length(address) > 0'), # cannot have a blank address doctable.UniqueConstraint('birthday', 'fav_color'), doctable.ForeignKey(['fav_color'], ['color.name'], onupdate='CASCADE', ondelete='CASCADE'), ], frozen = True, # parameter passed to dataclasses.dataclass ) class Person: name: str # default value will be \"not provided\" - good standardization address: str = doctable.Column( column_args=doctable.ColumnArgs( server_default='not provided', ) ) # provided as datetime, set to be indexed birthday: datetime.datetime = doctable.Column( column_args=doctable.ColumnArgs( index = True, ) ) # note that this has a foreign key constraint above fav_color: str = doctable.Column( column_args=doctable.ColumnArgs( nullable=False, ) ) id: PersonID = doctable.Column( # standard id column column_args=doctable.ColumnArgs( order=0, # will be the first column primary_key=True, autoincrement=True ), ) # doctable will define default and onupdate when inserting into database added: datetime.datetime = doctable.Column( column_args=doctable.ColumnArgs( index=True, default=datetime.datetime.utcnow, onupdate=datetime.datetime.utcnow ) ) # this property will not be stored in the database # - it acts like any other property @property def age(self): return datetime.datetime.now() - self.birthday core = doctable.ConnectCore.open( target=':memory:', dialect='sqlite' ) # NOTE: weird error when trying to run this twice after defining containers with core.begin_ddl() as emitter: core.enable_foreign_keys() # NOTE: NEEDED TO ENABLE FOREIGN KEYS color_tab = emitter.create_table_if_not_exists(container_type=Color) person_tab = emitter.create_table_if_not_exists(container_type=Person) for col_info in person_tab.inspect_columns(): print(f'{col_info[\"name\"]}: {col_info[\"type\"]}') id: INTEGER name: VARCHAR address: VARCHAR birthday: DATETIME fav_color: VARCHAR added: DATETIME Insertion into the color table is fairly straightforward. color_names = ['red', 'green', 'blue'] colors = [Color(name=name) for name in color_names] with color_tab.query() as q: q.insert_multi(colors) for c in q.select(): print(c) #print(q.select()) Color(name='red', id=1) Color(name='green', id=2) Color(name='blue', id=3) Insertion into the person table is similar, and note that we see an exception if we try to insert a person with a favorite color that is not in the color table. persons = [ Person(name='John', birthday=datetime.datetime(1990, 1, 1), fav_color='red'), Person(name='Sue', birthday=datetime.datetime(1991, 1, 1), fav_color='green'), Person(name='Ren', birthday=datetime.datetime(1995, 1, 1), fav_color='blue'), ] other_person = Person( name='Bob', address='123 Main St', birthday=datetime.datetime(1990, 1, 1), fav_color='other', # NOTE: THIS WILL CAUSE AN ERROR (NOT IN COLOR TABLE) ) import sqlalchemy.exc sec_in_one_year = 24*60*60*365 with person_tab.query() as q: q.insert_multi(persons, ifnotunique='replace') try: q.insert_single(other_person, ifnotunique='replace') print(f'THIS SHOULD NOT APPEAR') except sqlalchemy.exc.IntegrityError as e: print(f'successfully threw exception: {e}') for p in q.select(): print(f'{p.name} ({p.fav_color}): {p.age.total_seconds()//sec_in_one_year:0.0f} y/o') successfully threw exception: (sqlite3.IntegrityError) FOREIGN KEY constraint failed [SQL: INSERT OR REPLACE INTO person (name, address, birthday, fav_color, added) VALUES (?, ?, ?, ?, ?)] [parameters: ('Bob', '123 Main St', '1990-01-01 00:00:00.000000', 'other', '2023-11-14 22:17:40.402308')] (Background on this error at: https://sqlalche.me/e/20/gkpj) John (red): 33 y/o Sue (green): 32 y/o Ren (blue): 28 y/o The foreign key works as expected because we set onupdate : changing that value in the parent table will update the value in the child table. with color_tab.query() as q: q.update_single(dict(name='reddish'), where=color_tab['name']=='red') for c in q.select(): print(c) with person_tab.query() as q: for p in q.select(): print(f'{p.name} ({p.fav_color}): {p.age.total_seconds()//sec_in_one_year:0.0f} y/o') Color(name='reddish', id=1) Color(name='green', id=2) Color(name='blue', id=3) John (reddish): 33 y/o Sue (green): 32 y/o Ren (blue): 28 y/o","title":"Multi-table Schemas"},{"location":"documentation/ex_select/","text":"Select Queries with doctable In this document I will describe the interface for performing select queries with doctable . import pandas as pd import numpy as np import typing import sys sys.path.append('..') import doctable Define a demonstration schema The very first step is to define a table schema that will be appropriate for our examples. This table includes the typical id column (the first column, specified by order=0 ), as well as string, integer, and boolean attributes. The object used to specify the schema is called a container , and I will use that terminology as we go. @doctable.table_schema class Record: name: str = doctable.Column(column_args=doctable.ColumnArgs(nullable=False, unique=True)) age: int = doctable.Column() is_old: bool = doctable.Column() id: int = doctable.Column( column_args=doctable.ColumnArgs( order = 0, primary_key=True, autoincrement=True ), ) core = doctable.ConnectCore.open(target=':memory:', dialect='sqlite', echo=True) with core.begin_ddl() as ddl: rtab = ddl.create_table_if_not_exists(container_type=Record) 2023-11-13 08:38:32,059 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,060 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"Record\") 2023-11-13 08:38:32,060 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-13 08:38:32,061 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"Record\") 2023-11-13 08:38:32,062 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-13 08:38:32,063 INFO sqlalchemy.engine.Engine CREATE TABLE \"Record\" ( id INTEGER, age INTEGER, is_old INTEGER, name VARCHAR NOT NULL, PRIMARY KEY (id), UNIQUE (name) ) 2023-11-13 08:38:32,063 INFO sqlalchemy.engine.Engine [no key 0.00039s] () 2023-11-13 08:38:32,064 INFO sqlalchemy.engine.Engine COMMIT Insert test data We insert the test data using the TableQuery interface. Because this document is about select queries, feel free to look over this for now. I show the contents of the table as a dataframe below. The interface for doing this will be covered later in this document. import random random.seed(0) new_records: typing.List[Record] = list() for i in range(10): age = int(random.random()*100) # number in [0,1] is_old = age > 50 new_records.append(Record(name='user_'+str(i), age=age, is_old=is_old)) # insert new records with rtab.query() as q: print(q.insert_multi(new_records)) # dataframe select (for example purposes - .df() will be covered later) with core.query() as q: r = q.select(rtab.all_cols()).df() r 2023-11-13 08:38:32,104 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,104 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (age, is_old, name) VALUES (?, ?, ?) 2023-11-13 08:38:32,105 INFO sqlalchemy.engine.Engine [generated in 0.00140s] [(84, True, 'user_0'), (75, True, 'user_1'), (42, False, 'user_2'), (25, False, 'user_3'), (51, True, 'user_4'), (40, False, 'user_5'), (78, True, 'user_6'), (30, False, 'user_7'), (47, False, 'user_8'), (58, True, 'user_9')] <sqlalchemy.engine.cursor.CursorResult object at 0x7feef3d133f0> 2023-11-13 08:38:32,106 INFO sqlalchemy.engine.Engine COMMIT 2023-11-13 08:38:32,109 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,109 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 08:38:32,109 INFO sqlalchemy.engine.Engine [generated in 0.00102s] () 2023-11-13 08:38:32,112 INFO sqlalchemy.engine.Engine COMMIT .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 1 84 1 user_0 1 2 75 1 user_1 2 3 42 0 user_2 3 4 25 0 user_3 4 5 51 1 user_4 5 6 40 0 user_5 6 7 78 1 user_6 7 8 30 0 user_7 8 9 47 0 user_8 9 10 58 1 user_9 Two Interfaces: ConnectQuery and TableQuery There are two interfaces for performing queries: ConnectQuery and TableQuery . ConnectQuery table-agnostic interface for querying any table in any result format. TableQuery table-specific interface for querying a specific table. Insert and select from container objects used to define the schema. # ConnectQuery - table agnostic with core.query() as q: print(type(q)) # TableQuery - queries are relative to specific # table, results appear as container objects with rtab.query() as q: print(type(q)) <class 'doctable.query.connectquery.ConnectQuery'> <class 'doctable.query.tablequery.TableQuery'> ConnectQuery Basics First I will discuss the ConnectQuery interface, which is created via the ConnectCore.query() method. This object maintains a database connection, and, when used as a context manager, will commit all changes upon exit. It is fine to use the ConnectQuery object without a context manager for queries that do not require commits. This example is the most basic select query we can execute. Note that ConnectQuery methods are table-agnostic, so we must specify columns to be selected - in this case, we provide rtab.all_cols() to specify that we want to query all columns from the Record table. It returns a sqlalchemy.CursorResult object that we will discuss later. core.query().select(rtab.all_cols()) 2023-11-13 08:38:32,204 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,205 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 08:38:32,205 INFO sqlalchemy.engine.Engine [cached since 0.09654s ago] () <sqlalchemy.engine.cursor.CursorResult at 0x7feef3d88130> Selecting Specific Columns In many cases, you would not want to select all columns from a given table - for this reason, there are several methods you can use to specify the desired columns. In addition to .all_cols() in the above snippet, you may use any of these methods. Method Description .all_cols() specify that we want all columns .cols('col1', 'col2') specify set of columns table['col1'] specify single column table[['col1', 'col2']] specify multiple columns table['col1':'col3'] specify sequential range of columns # select all columns rtab.all_cols() [Column('id', Integer(), table=<Record>, primary_key=True), Column('age', Integer(), table=<Record>), Column('is_old', Integer(), table=<Record>), Column('name', String(), table=<Record>, nullable=False)] # .cols method rtab.cols('id', 'name') [Column('id', Integer(), table=<Record>, primary_key=True), Column('name', String(), table=<Record>, nullable=False)] # single-column subscript rtab['age'] Column('age', Integer(), table=<Record>) # list of columns rtab[['id','is_old']] [Column('id', Integer(), table=<Record>, primary_key=True), Column('is_old', Integer(), table=<Record>)] # slice select rtab['id':'is_old'] [Column('id', Integer(), table=<Record>, primary_key=True), Column('age', Integer(), table=<Record>), Column('is_old', Integer(), table=<Record>)] Note that the .select() method requires a list of columns, so we can combine these methods by combining the lists they return. Obviously, the order matters for the returned values. rtab.cols('id','is_old') + [rtab['name']] [Column('id', Integer(), table=<Record>, primary_key=True), Column('is_old', Integer(), table=<Record>), Column('name', String(), table=<Record>, nullable=False)] The .select() method always accepts a list of columns, so be sure to wrap single-column selections in a list. core.query().select([rtab['name']]) 2023-11-13 08:38:32,540 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,541 INFO sqlalchemy.engine.Engine SELECT \"Record\".name FROM \"Record\" 2023-11-13 08:38:32,542 INFO sqlalchemy.engine.Engine [generated in 0.00130s] () <sqlalchemy.engine.cursor.CursorResult at 0x7feef3d12f90> core.query().select(rtab.cols('id','is_old') + [rtab['name']]) 2023-11-13 08:38:32,589 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,590 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 08:38:32,590 INFO sqlalchemy.engine.Engine [generated in 0.00142s] () <sqlalchemy.engine.cursor.CursorResult at 0x7feef3d89010> Working with Query Results Now we turn to working with the results objects. So far I have demonstrated values for returning sqlalchemy.CursorResult objects, but additional methods are required to return the results in a usable format. The following methods are available for various purposes: Method Description result.all() return all results in query result.df() return multiple results as a dataframe result.first() return first result in query result.one() return exactly one result in query. NOTE: raises exception if not exactly one result. result.scalar_one() return single result, end query. NOTE: raises exception if not exactly one result. result.scalars().all() return single column of results with core.query() as q: r = q.select(rtab.all_cols(), limit=3) r.all() 2023-11-13 08:38:32,637 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,638 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:32,639 INFO sqlalchemy.engine.Engine [generated in 0.00185s] (3, 0) 2023-11-13 08:38:32,640 INFO sqlalchemy.engine.Engine COMMIT [(1, 84, 1, 'user_0'), (2, 75, 1, 'user_1'), (3, 42, 0, 'user_2')] core.query().select(rtab.all_cols(), limit=3).df() 2023-11-13 08:38:32,689 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,690 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:32,690 INFO sqlalchemy.engine.Engine [cached since 0.05336s ago] (3, 0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 1 84 1 user_0 1 2 75 1 user_1 2 3 42 0 user_2 # raises exception without limit=1 core.query().select(rtab.all_cols()).first() 2023-11-13 08:38:32,740 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,741 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 08:38:32,742 INFO sqlalchemy.engine.Engine [cached since 0.6331s ago] () (1, 84, 1, 'user_0') # raises exception if more than one result is returned # (here I forced this with limit=1) core.query().select(rtab.all_cols(), limit=1).one() 2023-11-13 08:38:32,788 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,789 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:32,790 INFO sqlalchemy.engine.Engine [cached since 0.1527s ago] (1, 0) (1, 84, 1, 'user_0') # this returns the first column from the first row, then closes the cursor core.query().select(rtab.all_cols(), limit=1).scalar_one() 2023-11-13 08:38:32,837 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,838 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:32,838 INFO sqlalchemy.engine.Engine [cached since 0.2012s ago] (1, 0) 1 # it makes more sense to query a single column core.query().select(rtab.cols('is_old'), limit=1).scalar_one() 2023-11-13 08:38:32,889 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,890 INFO sqlalchemy.engine.Engine SELECT \"Record\".is_old FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:32,890 INFO sqlalchemy.engine.Engine [generated in 0.00136s] (1, 0) 1 # and when when you need a single column, use .scalars() instead of .all() core.query().select(rtab.cols('is_old')).scalars().all() 2023-11-13 08:38:32,940 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,941 INFO sqlalchemy.engine.Engine SELECT \"Record\".is_old FROM \"Record\" 2023-11-13 08:38:32,941 INFO sqlalchemy.engine.Engine [generated in 0.00138s] () [1, 1, 0, 0, 1, 0, 1, 0, 0, 1] Conditional Select Statements operator description & , doctable.exp.and_() and \\| , doctable.exp.or_() or == equals != , doctable.exp.not_() not equals > greater than >= greater than or equal to < less than <= less than or equal to in_() in list contains() check if item is substring like() like string ilike() case-insensitive like string between() , doctable.exp.between() between two values is_() is value isnot() is not value startswith() starts with string core.query().select(rtab.all_cols(), where=rtab['id']==2).df() 2023-11-13 08:38:32,989 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,990 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE \"Record\".id = ? 2023-11-13 08:38:32,990 INFO sqlalchemy.engine.Engine [generated in 0.00131s] (2,) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 2 75 1 user_1 core.query().select(rtab.all_cols(), where=rtab['id']<rtab['id']).df() 2023-11-13 08:38:33,037 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,038 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE \"Record\".id < \"Record\".id 2023-11-13 08:38:33,038 INFO sqlalchemy.engine.Engine [generated in 0.00128s] () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } core.query().select(rtab.all_cols(), where=(rtab['id']%2)==0).df() 2023-11-13 08:38:33,089 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,090 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE \"Record\".id % ? = ? 2023-11-13 08:38:33,091 INFO sqlalchemy.engine.Engine [generated in 0.00138s] (2, 0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 2 75 1 user_1 1 4 25 0 user_3 2 6 40 0 user_5 3 8 30 0 user_7 4 10 58 1 user_9 condition = (rtab['id']>=2) & (rtab['id']<=4) & (rtab['name']!='user_2') core.query().select(rtab.all_cols(), where=condition).df() 2023-11-13 08:38:33,142 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,143 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE \"Record\".id >= ? AND \"Record\".id <= ? AND \"Record\".name != ? 2023-11-13 08:38:33,143 INFO sqlalchemy.engine.Engine [generated in 0.00147s] (2, 4, 'user_2') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 2 75 1 user_1 1 4 25 0 user_3 condition = rtab['name'].in_(('user_2','user_3')) core.query().select(rtab.all_cols(), where=condition).df() 2023-11-13 08:38:33,194 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,195 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE \"Record\".name IN (?, ?) 2023-11-13 08:38:33,195 INFO sqlalchemy.engine.Engine [generated in 0.00146s] ('user_2', 'user_3') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 3 42 0 user_2 1 4 25 0 user_3 core.query().select(rtab.all_cols(), where=rtab['id'].between(2,4)).df() 2023-11-13 08:38:33,246 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,247 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE \"Record\".id BETWEEN ? AND ? 2023-11-13 08:38:33,247 INFO sqlalchemy.engine.Engine [generated in 0.00134s] (2, 4) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 2 75 1 user_1 1 3 42 0 user_2 2 4 25 0 user_3 condition = ~(rtab['name'].in_(('user_2','user_3'))) & (rtab['id'] < 4) core.query().select(rtab.all_cols(), where=condition).df() 2023-11-13 08:38:33,298 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,299 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE (\"Record\".name NOT IN (?, ?)) AND \"Record\".id < ? 2023-11-13 08:38:33,300 INFO sqlalchemy.engine.Engine [generated in 0.00171s] ('user_2', 'user_3', 4) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 1 84 1 user_0 1 2 75 1 user_1 condition = doctable.exp.or_(doctable.exp.not_(rtab['id']==4)) & (rtab['id'] <= 2) core.query().select(rtab.all_cols(), where=condition).df() 2023-11-13 08:38:33,350 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,351 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE \"Record\".id != ? AND \"Record\".id <= ? 2023-11-13 08:38:33,352 INFO sqlalchemy.engine.Engine [generated in 0.00181s] (4, 2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 1 84 1 user_0 1 2 75 1 user_1 with core.query() as q: ages = q.select([rtab['age']]).scalars().all() mean_age = sum(ages)/len(ages) result = q.select(rtab.all_cols(), where=rtab['age']>mean_age) result.df() 2023-11-13 08:38:33,402 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,404 INFO sqlalchemy.engine.Engine SELECT \"Record\".age FROM \"Record\" 2023-11-13 08:38:33,404 INFO sqlalchemy.engine.Engine [generated in 0.00155s] () 2023-11-13 08:38:33,406 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE \"Record\".age > ? 2023-11-13 08:38:33,406 INFO sqlalchemy.engine.Engine [generated in 0.00045s] (53.0,) 2023-11-13 08:38:33,407 INFO sqlalchemy.engine.Engine COMMIT .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 1 84 1 user_0 1 2 75 1 user_1 2 7 78 1 user_6 3 10 58 1 user_9 Column Operators In addition to any of the methods used for conditional selects, there are several additional methods that can be used to transform columns in the select statement. Method Description .label() rename column in result (particularly useful after transformations) .min() , doctable.f.min() max of column values .max() , doctable.f.max() max of column values .sum() , doctable.f.sum() sum of column .count() , doctable.f.count() count number of results. NOTE : must use f.count() when counting transformed columns. .distinct() , doctable.f.distinct() get distinct values / divide * multiply + add - subtract % modulo .concat() concatenate strings columns = [ (rtab['id'] % 2).label('mod_id'), rtab['name'].label('myname') ] core.query().select(columns, where=rtab['is_old']).df() 2023-11-13 08:38:33,457 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,458 INFO sqlalchemy.engine.Engine SELECT \"Record\".id % ? AS mod_id, \"Record\".name AS myname FROM \"Record\" WHERE \"Record\".is_old 2023-11-13 08:38:33,458 INFO sqlalchemy.engine.Engine [generated in 0.00153s] (2,) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mod_id myname 0 1 user_0 1 0 user_1 2 1 user_4 3 1 user_6 4 0 user_9 formula = rtab['age'].sum() / rtab['age'].count() core.query().select([formula]).scalar_one() 2023-11-13 08:38:33,510 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,511 INFO sqlalchemy.engine.Engine SELECT sum(\"Record\".age) / (count(\"Record\".age) + 0.0) AS anon_1 FROM \"Record\" 2023-11-13 08:38:33,511 INFO sqlalchemy.engine.Engine [generated in 0.00153s] () Decimal('53.0000000000') formula = rtab['age'].max() - rtab['age'].min() core.query().select([formula]).scalar_one() 2023-11-13 08:38:33,561 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,562 INFO sqlalchemy.engine.Engine SELECT max(\"Record\".age) - min(\"Record\".age) AS anon_1 FROM \"Record\" 2023-11-13 08:38:33,563 INFO sqlalchemy.engine.Engine [generated in 0.00155s] () 59 # average age of individuals over 30 formula = rtab['age'].sum() / rtab['age'].count() condition = rtab['age'] > 30 core.query().select([formula], where=condition).scalar_one() 2023-11-13 08:38:33,614 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,615 INFO sqlalchemy.engine.Engine SELECT sum(\"Record\".age) / (count(\"Record\".age) + 0.0) AS anon_1 FROM \"Record\" WHERE \"Record\".age > ? 2023-11-13 08:38:33,615 INFO sqlalchemy.engine.Engine [generated in 0.00140s] (30,) Decimal('59.3750000000') # descriptive stats on age of individuals over 30 columns = [ (rtab['age'].sum() / rtab['age'].count()).label('mean'), rtab['age'].max().label('max'), rtab['age'].min().label('min'), ] condition = rtab['age'] > 30 core.query().select(columns, where=condition).df() 2023-11-13 08:38:33,666 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,667 INFO sqlalchemy.engine.Engine SELECT sum(\"Record\".age) / (count(\"Record\".age) + 0.0) AS mean, max(\"Record\".age) AS max, min(\"Record\".age) AS min FROM \"Record\" WHERE \"Record\".age > ? 2023-11-13 08:38:33,667 INFO sqlalchemy.engine.Engine [generated in 0.00142s] (30,) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean max min 0 59.3750000000 84 40 # all distinct values formula = rtab['is_old'].distinct() core.query().select([formula]).df() 2023-11-13 08:38:33,717 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,718 INFO sqlalchemy.engine.Engine SELECT distinct(\"Record\".is_old) AS distinct_1 FROM \"Record\" 2023-11-13 08:38:33,718 INFO sqlalchemy.engine.Engine [generated in 0.00171s] () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } distinct_1 0 1 1 0 # count individuals over 30 core.query().select([doctable.f.count()], where=rtab['age']>30).scalar_one() 2023-11-13 08:38:33,765 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,766 INFO sqlalchemy.engine.Engine SELECT count(*) AS count_1 FROM \"Record\" WHERE \"Record\".age > ? 2023-11-13 08:38:33,767 INFO sqlalchemy.engine.Engine [generated in 0.00157s] (30,) 8 # similar to previous core.query().select([rtab['id'].count()], where=rtab['age']>30).scalar_one() 2023-11-13 08:38:33,817 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,818 INFO sqlalchemy.engine.Engine SELECT count(\"Record\".id) AS count_1 FROM \"Record\" WHERE \"Record\".age > ? 2023-11-13 08:38:33,818 INFO sqlalchemy.engine.Engine [generated in 0.00127s] (30,) 8 Additional Parameters: Order By, Group By, Limit, Offset More complicated queries involving ordering, grouping, limiting, and specifying offset can be specified using parameters to the .select() method. Parameter Description limit limit number of results order_by list of columns to order by group_by list of columns to group by offset offset results by specified number Order By and Limits # get the five youngest individuals in order core.query().select(rtab.all_cols(), order_by=rtab['age'], limit=5).df() 2023-11-13 08:38:33,865 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,865 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" ORDER BY \"Record\".age LIMIT ? OFFSET ? 2023-11-13 08:38:33,866 INFO sqlalchemy.engine.Engine [generated in 0.00127s] (5, 0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 4 25 0 user_3 1 8 30 0 user_7 2 6 40 0 user_5 3 3 42 0 user_2 4 9 47 0 user_8 # get the five oldest now core.query().select(rtab.all_cols(), order_by=rtab['age'].desc(), limit=5).df() 2023-11-13 08:38:33,913 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,914 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" ORDER BY \"Record\".age DESC LIMIT ? OFFSET ? 2023-11-13 08:38:33,915 INFO sqlalchemy.engine.Engine [generated in 0.00165s] (5, 0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 1 84 1 user_0 1 7 78 1 user_6 2 2 75 1 user_1 3 10 58 1 user_9 4 5 51 1 user_4 # order by is_old, but preserve order of id otherwise order = [ rtab['is_old'].desc(), rtab['id'].asc(), ] core.query().select(rtab.all_cols(), order_by=order, limit=5).df() 2023-11-13 08:38:33,965 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,966 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" ORDER BY \"Record\".is_old DESC, \"Record\".id ASC LIMIT ? OFFSET ? 2023-11-13 08:38:33,966 INFO sqlalchemy.engine.Engine [generated in 0.00135s] (5, 0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 1 84 1 user_0 1 2 75 1 user_1 2 5 51 1 user_4 3 7 78 1 user_6 4 10 58 1 user_9 Grouping and Column Operators # summary stats by is_old cols = [ #rtab['is_old'].count().label('count'), doctable.f.count().label('count'), rtab['age'].min().label('min'), rtab['age'].max().label('max'), (rtab['age'].sum()/rtab['age'].count()).label('mean'), ] core.query().select(cols, group_by=rtab['is_old']).df() 2023-11-13 08:38:34,018 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:34,019 INFO sqlalchemy.engine.Engine SELECT count(*) AS count, min(\"Record\".age) AS min, max(\"Record\".age) AS max, sum(\"Record\".age) / (count(\"Record\".age) + 0.0) AS mean FROM \"Record\" GROUP BY \"Record\".is_old 2023-11-13 08:38:34,019 INFO sqlalchemy.engine.Engine [generated in 0.00141s] () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count min max mean 0 5 25 47 36.8000000000 1 5 51 84 69.2000000000 # summarize age by decade decade_expression = doctable.f.round(rtab['age'] / 10) cols = [ decade_expression.label('decade'), rtab['age'].count().label('count'), rtab['age'].min().label('min'), rtab['age'].max().label('max'), (rtab['age'].sum()/rtab['age'].count()).label('mean'), ] core.query().select(cols, group_by=decade_expression).df() 2023-11-13 08:38:34,070 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:34,071 INFO sqlalchemy.engine.Engine SELECT round(\"Record\".age / (? + 0.0)) AS decade, count(\"Record\".age) AS count, min(\"Record\".age) AS min, max(\"Record\".age) AS max, sum(\"Record\".age) / (count(\"Record\".age) + 0.0) AS mean FROM \"Record\" GROUP BY round(\"Record\".age / (? + 0.0)) 2023-11-13 08:38:34,072 INFO sqlalchemy.engine.Engine [generated in 0.00134s] (10, 10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } decade count min max mean 0 3.0 2 25 30 27.5000000000 1 4.0 2 40 42 41.0000000000 2 5.0 2 47 51 49.0000000000 3 6.0 1 58 58 58.0000000000 4 8.0 3 75 84 79.0000000000 Offset and Selecting Chunks The offset parameter is used to pagify results into multiple queries - something that is particularly useful if the result set is too larget to fit into memory. # get the three oldest individuals, offset by three core.query().select(rtab.all_cols(), order_by=rtab['age'].desc(), limit=3, offset=3).df() 2023-11-13 08:38:34,121 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:34,121 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" ORDER BY \"Record\".age DESC LIMIT ? OFFSET ? 2023-11-13 08:38:34,122 INFO sqlalchemy.engine.Engine [generated in 0.00137s] (3, 3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 10 58 1 user_9 1 5 51 1 user_4 2 9 47 0 user_8 for chunk in core.query().select_chunks(rtab.all_cols(), chunksize=3): print(chunk) 2023-11-13 08:38:34,169 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:34,169 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:34,170 INFO sqlalchemy.engine.Engine [generated in 0.00117s] (3, 0) [(1, 84, 1, 'user_0'), (2, 75, 1, 'user_1'), (3, 42, 0, 'user_2')] 2023-11-13 08:38:34,171 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:34,171 INFO sqlalchemy.engine.Engine [cached since 0.002855s ago] (3, 3) [(4, 25, 0, 'user_3'), (5, 51, 1, 'user_4'), (6, 40, 0, 'user_5')] 2023-11-13 08:38:34,172 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:34,173 INFO sqlalchemy.engine.Engine [cached since 0.004328s ago] (3, 6) [(7, 78, 1, 'user_6'), (8, 30, 0, 'user_7'), (9, 47, 0, 'user_8')] 2023-11-13 08:38:34,175 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:34,175 INFO sqlalchemy.engine.Engine [cached since 0.006853s ago] (3, 9) [(10, 58, 1, 'user_9')] TableQuery Basics The TableQuery interface is used to make table-specific queries, and, in exchange for this restriction, allows you to insert and select container objects directly. Queries on tables look much like their table-agnostic counterparts, with a few exceptions. Every query still begins with the .query() method, which returns a TableQuery object with methods for inserting and selecting container objects. List of selected columns is optional - if you do not specify, the query will default to all columns in the table. Otherwise, you should provide a subset of the columns, where all attributes that were not received will refer to doctable.MISSING , which you may check for downstream. Results of a select query are returned as a list of container objects. This means that we have called the .all() method on the sqlalchemy.CursorResult object. All returned results must match the attributes of the container object. Most often you will want to select raw database rows, but transformations via group_by and other operators are also possible as long as the result set attributes match the container attributes - that is, they can be expanded to the container constructor. Behavior of where , order_by , limit , are offset all operate as-expected. Below you can see a few examples demonstrating this behavior. rtab.query().select(where=rtab['age']>50) 2023-11-13 08:38:34,217 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:34,218 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE \"Record\".age > ? 2023-11-13 08:38:34,219 INFO sqlalchemy.engine.Engine [generated in 0.00128s] (50,) [Record(name='user_0', age=84, is_old=1, id=1), Record(name='user_1', age=75, is_old=1, id=2), Record(name='user_4', age=51, is_old=1, id=5), Record(name='user_6', age=78, is_old=1, id=7), Record(name='user_9', age=58, is_old=1, id=10)] result = rtab.query().select(rtab.cols('id', 'age'), where=rtab['is_old']) f'{result[0].name is doctable.MISSING=}', result 2023-11-13 08:38:34,268 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:34,269 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age FROM \"Record\" WHERE \"Record\".is_old 2023-11-13 08:38:34,270 INFO sqlalchemy.engine.Engine [generated in 0.00134s] () ('result[0].name is doctable.MISSING=True', [Record(name=MISSING, age=84, is_old=MISSING, id=1), Record(name=MISSING, age=75, is_old=MISSING, id=2), Record(name=MISSING, age=51, is_old=MISSING, id=5), Record(name=MISSING, age=78, is_old=MISSING, id=7), Record(name=MISSING, age=58, is_old=MISSING, id=10)]) # this is valid, although perhaps not recommended cols = [ (rtab['age'].sum()/rtab['age'].count()).label('age'), ] rtab.query().select(cols=cols) 2023-11-13 08:38:34,317 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:34,318 INFO sqlalchemy.engine.Engine SELECT sum(\"Record\".age) / (count(\"Record\".age) + 0.0) AS age FROM \"Record\" 2023-11-13 08:38:34,318 INFO sqlalchemy.engine.Engine [generated in 0.00123s] () [Record(name=MISSING, age=Decimal('53.0000000000'), is_old=MISSING, id=MISSING)] .select_chunks() also more or less works as-expected, with the chunks being converted to container objects. for chunk in rtab.query().select_chunks(chunksize=3): print(chunk) 2023-11-13 08:38:34,364 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:34,365 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:34,365 INFO sqlalchemy.engine.Engine [cached since 0.1964s ago] (3, 0) [Record(name='user_0', age=84, is_old=1, id=1), Record(name='user_1', age=75, is_old=1, id=2), Record(name='user_2', age=42, is_old=0, id=3)] 2023-11-13 08:38:34,366 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:34,367 INFO sqlalchemy.engine.Engine [cached since 0.198s ago] (3, 3) [Record(name='user_3', age=25, is_old=0, id=4), Record(name='user_4', age=51, is_old=1, id=5), Record(name='user_5', age=40, is_old=0, id=6)] 2023-11-13 08:38:34,368 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:34,368 INFO sqlalchemy.engine.Engine [cached since 0.1994s ago] (3, 6) [Record(name='user_6', age=78, is_old=1, id=7), Record(name='user_7', age=30, is_old=0, id=8), Record(name='user_8', age=47, is_old=0, id=9)] 2023-11-13 08:38:34,369 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:34,369 INFO sqlalchemy.engine.Engine [cached since 0.2007s ago] (3, 9) [Record(name='user_9', age=58, is_old=1, id=10)]","title":"Select Queries with doctable"},{"location":"documentation/ex_select/#select-queries-with-doctable","text":"In this document I will describe the interface for performing select queries with doctable . import pandas as pd import numpy as np import typing import sys sys.path.append('..') import doctable","title":"Select Queries with doctable"},{"location":"documentation/ex_select/#define-a-demonstration-schema","text":"The very first step is to define a table schema that will be appropriate for our examples. This table includes the typical id column (the first column, specified by order=0 ), as well as string, integer, and boolean attributes. The object used to specify the schema is called a container , and I will use that terminology as we go. @doctable.table_schema class Record: name: str = doctable.Column(column_args=doctable.ColumnArgs(nullable=False, unique=True)) age: int = doctable.Column() is_old: bool = doctable.Column() id: int = doctable.Column( column_args=doctable.ColumnArgs( order = 0, primary_key=True, autoincrement=True ), ) core = doctable.ConnectCore.open(target=':memory:', dialect='sqlite', echo=True) with core.begin_ddl() as ddl: rtab = ddl.create_table_if_not_exists(container_type=Record) 2023-11-13 08:38:32,059 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,060 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"Record\") 2023-11-13 08:38:32,060 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-13 08:38:32,061 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"Record\") 2023-11-13 08:38:32,062 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-13 08:38:32,063 INFO sqlalchemy.engine.Engine CREATE TABLE \"Record\" ( id INTEGER, age INTEGER, is_old INTEGER, name VARCHAR NOT NULL, PRIMARY KEY (id), UNIQUE (name) ) 2023-11-13 08:38:32,063 INFO sqlalchemy.engine.Engine [no key 0.00039s] () 2023-11-13 08:38:32,064 INFO sqlalchemy.engine.Engine COMMIT","title":"Define a demonstration schema"},{"location":"documentation/ex_select/#insert-test-data","text":"We insert the test data using the TableQuery interface. Because this document is about select queries, feel free to look over this for now. I show the contents of the table as a dataframe below. The interface for doing this will be covered later in this document. import random random.seed(0) new_records: typing.List[Record] = list() for i in range(10): age = int(random.random()*100) # number in [0,1] is_old = age > 50 new_records.append(Record(name='user_'+str(i), age=age, is_old=is_old)) # insert new records with rtab.query() as q: print(q.insert_multi(new_records)) # dataframe select (for example purposes - .df() will be covered later) with core.query() as q: r = q.select(rtab.all_cols()).df() r 2023-11-13 08:38:32,104 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,104 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO \"Record\" (age, is_old, name) VALUES (?, ?, ?) 2023-11-13 08:38:32,105 INFO sqlalchemy.engine.Engine [generated in 0.00140s] [(84, True, 'user_0'), (75, True, 'user_1'), (42, False, 'user_2'), (25, False, 'user_3'), (51, True, 'user_4'), (40, False, 'user_5'), (78, True, 'user_6'), (30, False, 'user_7'), (47, False, 'user_8'), (58, True, 'user_9')] <sqlalchemy.engine.cursor.CursorResult object at 0x7feef3d133f0> 2023-11-13 08:38:32,106 INFO sqlalchemy.engine.Engine COMMIT 2023-11-13 08:38:32,109 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,109 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 08:38:32,109 INFO sqlalchemy.engine.Engine [generated in 0.00102s] () 2023-11-13 08:38:32,112 INFO sqlalchemy.engine.Engine COMMIT .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 1 84 1 user_0 1 2 75 1 user_1 2 3 42 0 user_2 3 4 25 0 user_3 4 5 51 1 user_4 5 6 40 0 user_5 6 7 78 1 user_6 7 8 30 0 user_7 8 9 47 0 user_8 9 10 58 1 user_9","title":"Insert test data"},{"location":"documentation/ex_select/#two-interfaces-connectquery-and-tablequery","text":"There are two interfaces for performing queries: ConnectQuery and TableQuery . ConnectQuery table-agnostic interface for querying any table in any result format. TableQuery table-specific interface for querying a specific table. Insert and select from container objects used to define the schema. # ConnectQuery - table agnostic with core.query() as q: print(type(q)) # TableQuery - queries are relative to specific # table, results appear as container objects with rtab.query() as q: print(type(q)) <class 'doctable.query.connectquery.ConnectQuery'> <class 'doctable.query.tablequery.TableQuery'>","title":"Two Interfaces: ConnectQuery and TableQuery"},{"location":"documentation/ex_select/#connectquery-basics","text":"First I will discuss the ConnectQuery interface, which is created via the ConnectCore.query() method. This object maintains a database connection, and, when used as a context manager, will commit all changes upon exit. It is fine to use the ConnectQuery object without a context manager for queries that do not require commits. This example is the most basic select query we can execute. Note that ConnectQuery methods are table-agnostic, so we must specify columns to be selected - in this case, we provide rtab.all_cols() to specify that we want to query all columns from the Record table. It returns a sqlalchemy.CursorResult object that we will discuss later. core.query().select(rtab.all_cols()) 2023-11-13 08:38:32,204 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,205 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 08:38:32,205 INFO sqlalchemy.engine.Engine [cached since 0.09654s ago] () <sqlalchemy.engine.cursor.CursorResult at 0x7feef3d88130>","title":"ConnectQuery Basics"},{"location":"documentation/ex_select/#selecting-specific-columns","text":"In many cases, you would not want to select all columns from a given table - for this reason, there are several methods you can use to specify the desired columns. In addition to .all_cols() in the above snippet, you may use any of these methods. Method Description .all_cols() specify that we want all columns .cols('col1', 'col2') specify set of columns table['col1'] specify single column table[['col1', 'col2']] specify multiple columns table['col1':'col3'] specify sequential range of columns # select all columns rtab.all_cols() [Column('id', Integer(), table=<Record>, primary_key=True), Column('age', Integer(), table=<Record>), Column('is_old', Integer(), table=<Record>), Column('name', String(), table=<Record>, nullable=False)] # .cols method rtab.cols('id', 'name') [Column('id', Integer(), table=<Record>, primary_key=True), Column('name', String(), table=<Record>, nullable=False)] # single-column subscript rtab['age'] Column('age', Integer(), table=<Record>) # list of columns rtab[['id','is_old']] [Column('id', Integer(), table=<Record>, primary_key=True), Column('is_old', Integer(), table=<Record>)] # slice select rtab['id':'is_old'] [Column('id', Integer(), table=<Record>, primary_key=True), Column('age', Integer(), table=<Record>), Column('is_old', Integer(), table=<Record>)] Note that the .select() method requires a list of columns, so we can combine these methods by combining the lists they return. Obviously, the order matters for the returned values. rtab.cols('id','is_old') + [rtab['name']] [Column('id', Integer(), table=<Record>, primary_key=True), Column('is_old', Integer(), table=<Record>), Column('name', String(), table=<Record>, nullable=False)] The .select() method always accepts a list of columns, so be sure to wrap single-column selections in a list. core.query().select([rtab['name']]) 2023-11-13 08:38:32,540 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,541 INFO sqlalchemy.engine.Engine SELECT \"Record\".name FROM \"Record\" 2023-11-13 08:38:32,542 INFO sqlalchemy.engine.Engine [generated in 0.00130s] () <sqlalchemy.engine.cursor.CursorResult at 0x7feef3d12f90> core.query().select(rtab.cols('id','is_old') + [rtab['name']]) 2023-11-13 08:38:32,589 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,590 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 08:38:32,590 INFO sqlalchemy.engine.Engine [generated in 0.00142s] () <sqlalchemy.engine.cursor.CursorResult at 0x7feef3d89010>","title":"Selecting Specific Columns"},{"location":"documentation/ex_select/#working-with-query-results","text":"Now we turn to working with the results objects. So far I have demonstrated values for returning sqlalchemy.CursorResult objects, but additional methods are required to return the results in a usable format. The following methods are available for various purposes: Method Description result.all() return all results in query result.df() return multiple results as a dataframe result.first() return first result in query result.one() return exactly one result in query. NOTE: raises exception if not exactly one result. result.scalar_one() return single result, end query. NOTE: raises exception if not exactly one result. result.scalars().all() return single column of results with core.query() as q: r = q.select(rtab.all_cols(), limit=3) r.all() 2023-11-13 08:38:32,637 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,638 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:32,639 INFO sqlalchemy.engine.Engine [generated in 0.00185s] (3, 0) 2023-11-13 08:38:32,640 INFO sqlalchemy.engine.Engine COMMIT [(1, 84, 1, 'user_0'), (2, 75, 1, 'user_1'), (3, 42, 0, 'user_2')] core.query().select(rtab.all_cols(), limit=3).df() 2023-11-13 08:38:32,689 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,690 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:32,690 INFO sqlalchemy.engine.Engine [cached since 0.05336s ago] (3, 0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 1 84 1 user_0 1 2 75 1 user_1 2 3 42 0 user_2 # raises exception without limit=1 core.query().select(rtab.all_cols()).first() 2023-11-13 08:38:32,740 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,741 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" 2023-11-13 08:38:32,742 INFO sqlalchemy.engine.Engine [cached since 0.6331s ago] () (1, 84, 1, 'user_0') # raises exception if more than one result is returned # (here I forced this with limit=1) core.query().select(rtab.all_cols(), limit=1).one() 2023-11-13 08:38:32,788 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,789 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:32,790 INFO sqlalchemy.engine.Engine [cached since 0.1527s ago] (1, 0) (1, 84, 1, 'user_0') # this returns the first column from the first row, then closes the cursor core.query().select(rtab.all_cols(), limit=1).scalar_one() 2023-11-13 08:38:32,837 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,838 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:32,838 INFO sqlalchemy.engine.Engine [cached since 0.2012s ago] (1, 0) 1 # it makes more sense to query a single column core.query().select(rtab.cols('is_old'), limit=1).scalar_one() 2023-11-13 08:38:32,889 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,890 INFO sqlalchemy.engine.Engine SELECT \"Record\".is_old FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:32,890 INFO sqlalchemy.engine.Engine [generated in 0.00136s] (1, 0) 1 # and when when you need a single column, use .scalars() instead of .all() core.query().select(rtab.cols('is_old')).scalars().all() 2023-11-13 08:38:32,940 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,941 INFO sqlalchemy.engine.Engine SELECT \"Record\".is_old FROM \"Record\" 2023-11-13 08:38:32,941 INFO sqlalchemy.engine.Engine [generated in 0.00138s] () [1, 1, 0, 0, 1, 0, 1, 0, 0, 1]","title":"Working with Query Results"},{"location":"documentation/ex_select/#conditional-select-statements","text":"operator description & , doctable.exp.and_() and \\| , doctable.exp.or_() or == equals != , doctable.exp.not_() not equals > greater than >= greater than or equal to < less than <= less than or equal to in_() in list contains() check if item is substring like() like string ilike() case-insensitive like string between() , doctable.exp.between() between two values is_() is value isnot() is not value startswith() starts with string core.query().select(rtab.all_cols(), where=rtab['id']==2).df() 2023-11-13 08:38:32,989 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:32,990 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE \"Record\".id = ? 2023-11-13 08:38:32,990 INFO sqlalchemy.engine.Engine [generated in 0.00131s] (2,) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 2 75 1 user_1 core.query().select(rtab.all_cols(), where=rtab['id']<rtab['id']).df() 2023-11-13 08:38:33,037 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,038 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE \"Record\".id < \"Record\".id 2023-11-13 08:38:33,038 INFO sqlalchemy.engine.Engine [generated in 0.00128s] () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } core.query().select(rtab.all_cols(), where=(rtab['id']%2)==0).df() 2023-11-13 08:38:33,089 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,090 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE \"Record\".id % ? = ? 2023-11-13 08:38:33,091 INFO sqlalchemy.engine.Engine [generated in 0.00138s] (2, 0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 2 75 1 user_1 1 4 25 0 user_3 2 6 40 0 user_5 3 8 30 0 user_7 4 10 58 1 user_9 condition = (rtab['id']>=2) & (rtab['id']<=4) & (rtab['name']!='user_2') core.query().select(rtab.all_cols(), where=condition).df() 2023-11-13 08:38:33,142 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,143 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE \"Record\".id >= ? AND \"Record\".id <= ? AND \"Record\".name != ? 2023-11-13 08:38:33,143 INFO sqlalchemy.engine.Engine [generated in 0.00147s] (2, 4, 'user_2') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 2 75 1 user_1 1 4 25 0 user_3 condition = rtab['name'].in_(('user_2','user_3')) core.query().select(rtab.all_cols(), where=condition).df() 2023-11-13 08:38:33,194 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,195 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE \"Record\".name IN (?, ?) 2023-11-13 08:38:33,195 INFO sqlalchemy.engine.Engine [generated in 0.00146s] ('user_2', 'user_3') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 3 42 0 user_2 1 4 25 0 user_3 core.query().select(rtab.all_cols(), where=rtab['id'].between(2,4)).df() 2023-11-13 08:38:33,246 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,247 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE \"Record\".id BETWEEN ? AND ? 2023-11-13 08:38:33,247 INFO sqlalchemy.engine.Engine [generated in 0.00134s] (2, 4) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 2 75 1 user_1 1 3 42 0 user_2 2 4 25 0 user_3 condition = ~(rtab['name'].in_(('user_2','user_3'))) & (rtab['id'] < 4) core.query().select(rtab.all_cols(), where=condition).df() 2023-11-13 08:38:33,298 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,299 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE (\"Record\".name NOT IN (?, ?)) AND \"Record\".id < ? 2023-11-13 08:38:33,300 INFO sqlalchemy.engine.Engine [generated in 0.00171s] ('user_2', 'user_3', 4) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 1 84 1 user_0 1 2 75 1 user_1 condition = doctable.exp.or_(doctable.exp.not_(rtab['id']==4)) & (rtab['id'] <= 2) core.query().select(rtab.all_cols(), where=condition).df() 2023-11-13 08:38:33,350 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,351 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE \"Record\".id != ? AND \"Record\".id <= ? 2023-11-13 08:38:33,352 INFO sqlalchemy.engine.Engine [generated in 0.00181s] (4, 2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 1 84 1 user_0 1 2 75 1 user_1 with core.query() as q: ages = q.select([rtab['age']]).scalars().all() mean_age = sum(ages)/len(ages) result = q.select(rtab.all_cols(), where=rtab['age']>mean_age) result.df() 2023-11-13 08:38:33,402 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,404 INFO sqlalchemy.engine.Engine SELECT \"Record\".age FROM \"Record\" 2023-11-13 08:38:33,404 INFO sqlalchemy.engine.Engine [generated in 0.00155s] () 2023-11-13 08:38:33,406 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE \"Record\".age > ? 2023-11-13 08:38:33,406 INFO sqlalchemy.engine.Engine [generated in 0.00045s] (53.0,) 2023-11-13 08:38:33,407 INFO sqlalchemy.engine.Engine COMMIT .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 1 84 1 user_0 1 2 75 1 user_1 2 7 78 1 user_6 3 10 58 1 user_9","title":"Conditional Select Statements"},{"location":"documentation/ex_select/#column-operators","text":"In addition to any of the methods used for conditional selects, there are several additional methods that can be used to transform columns in the select statement. Method Description .label() rename column in result (particularly useful after transformations) .min() , doctable.f.min() max of column values .max() , doctable.f.max() max of column values .sum() , doctable.f.sum() sum of column .count() , doctable.f.count() count number of results. NOTE : must use f.count() when counting transformed columns. .distinct() , doctable.f.distinct() get distinct values / divide * multiply + add - subtract % modulo .concat() concatenate strings columns = [ (rtab['id'] % 2).label('mod_id'), rtab['name'].label('myname') ] core.query().select(columns, where=rtab['is_old']).df() 2023-11-13 08:38:33,457 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,458 INFO sqlalchemy.engine.Engine SELECT \"Record\".id % ? AS mod_id, \"Record\".name AS myname FROM \"Record\" WHERE \"Record\".is_old 2023-11-13 08:38:33,458 INFO sqlalchemy.engine.Engine [generated in 0.00153s] (2,) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mod_id myname 0 1 user_0 1 0 user_1 2 1 user_4 3 1 user_6 4 0 user_9 formula = rtab['age'].sum() / rtab['age'].count() core.query().select([formula]).scalar_one() 2023-11-13 08:38:33,510 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,511 INFO sqlalchemy.engine.Engine SELECT sum(\"Record\".age) / (count(\"Record\".age) + 0.0) AS anon_1 FROM \"Record\" 2023-11-13 08:38:33,511 INFO sqlalchemy.engine.Engine [generated in 0.00153s] () Decimal('53.0000000000') formula = rtab['age'].max() - rtab['age'].min() core.query().select([formula]).scalar_one() 2023-11-13 08:38:33,561 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,562 INFO sqlalchemy.engine.Engine SELECT max(\"Record\".age) - min(\"Record\".age) AS anon_1 FROM \"Record\" 2023-11-13 08:38:33,563 INFO sqlalchemy.engine.Engine [generated in 0.00155s] () 59 # average age of individuals over 30 formula = rtab['age'].sum() / rtab['age'].count() condition = rtab['age'] > 30 core.query().select([formula], where=condition).scalar_one() 2023-11-13 08:38:33,614 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,615 INFO sqlalchemy.engine.Engine SELECT sum(\"Record\".age) / (count(\"Record\".age) + 0.0) AS anon_1 FROM \"Record\" WHERE \"Record\".age > ? 2023-11-13 08:38:33,615 INFO sqlalchemy.engine.Engine [generated in 0.00140s] (30,) Decimal('59.3750000000') # descriptive stats on age of individuals over 30 columns = [ (rtab['age'].sum() / rtab['age'].count()).label('mean'), rtab['age'].max().label('max'), rtab['age'].min().label('min'), ] condition = rtab['age'] > 30 core.query().select(columns, where=condition).df() 2023-11-13 08:38:33,666 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,667 INFO sqlalchemy.engine.Engine SELECT sum(\"Record\".age) / (count(\"Record\".age) + 0.0) AS mean, max(\"Record\".age) AS max, min(\"Record\".age) AS min FROM \"Record\" WHERE \"Record\".age > ? 2023-11-13 08:38:33,667 INFO sqlalchemy.engine.Engine [generated in 0.00142s] (30,) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean max min 0 59.3750000000 84 40 # all distinct values formula = rtab['is_old'].distinct() core.query().select([formula]).df() 2023-11-13 08:38:33,717 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,718 INFO sqlalchemy.engine.Engine SELECT distinct(\"Record\".is_old) AS distinct_1 FROM \"Record\" 2023-11-13 08:38:33,718 INFO sqlalchemy.engine.Engine [generated in 0.00171s] () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } distinct_1 0 1 1 0 # count individuals over 30 core.query().select([doctable.f.count()], where=rtab['age']>30).scalar_one() 2023-11-13 08:38:33,765 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,766 INFO sqlalchemy.engine.Engine SELECT count(*) AS count_1 FROM \"Record\" WHERE \"Record\".age > ? 2023-11-13 08:38:33,767 INFO sqlalchemy.engine.Engine [generated in 0.00157s] (30,) 8 # similar to previous core.query().select([rtab['id'].count()], where=rtab['age']>30).scalar_one() 2023-11-13 08:38:33,817 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,818 INFO sqlalchemy.engine.Engine SELECT count(\"Record\".id) AS count_1 FROM \"Record\" WHERE \"Record\".age > ? 2023-11-13 08:38:33,818 INFO sqlalchemy.engine.Engine [generated in 0.00127s] (30,) 8","title":"Column Operators"},{"location":"documentation/ex_select/#additional-parameters-order-by-group-by-limit-offset","text":"More complicated queries involving ordering, grouping, limiting, and specifying offset can be specified using parameters to the .select() method. Parameter Description limit limit number of results order_by list of columns to order by group_by list of columns to group by offset offset results by specified number","title":"Additional Parameters: Order By, Group By, Limit, Offset"},{"location":"documentation/ex_select/#order-by-and-limits","text":"# get the five youngest individuals in order core.query().select(rtab.all_cols(), order_by=rtab['age'], limit=5).df() 2023-11-13 08:38:33,865 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,865 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" ORDER BY \"Record\".age LIMIT ? OFFSET ? 2023-11-13 08:38:33,866 INFO sqlalchemy.engine.Engine [generated in 0.00127s] (5, 0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 4 25 0 user_3 1 8 30 0 user_7 2 6 40 0 user_5 3 3 42 0 user_2 4 9 47 0 user_8 # get the five oldest now core.query().select(rtab.all_cols(), order_by=rtab['age'].desc(), limit=5).df() 2023-11-13 08:38:33,913 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,914 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" ORDER BY \"Record\".age DESC LIMIT ? OFFSET ? 2023-11-13 08:38:33,915 INFO sqlalchemy.engine.Engine [generated in 0.00165s] (5, 0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 1 84 1 user_0 1 7 78 1 user_6 2 2 75 1 user_1 3 10 58 1 user_9 4 5 51 1 user_4 # order by is_old, but preserve order of id otherwise order = [ rtab['is_old'].desc(), rtab['id'].asc(), ] core.query().select(rtab.all_cols(), order_by=order, limit=5).df() 2023-11-13 08:38:33,965 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:33,966 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" ORDER BY \"Record\".is_old DESC, \"Record\".id ASC LIMIT ? OFFSET ? 2023-11-13 08:38:33,966 INFO sqlalchemy.engine.Engine [generated in 0.00135s] (5, 0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 1 84 1 user_0 1 2 75 1 user_1 2 5 51 1 user_4 3 7 78 1 user_6 4 10 58 1 user_9","title":"Order By and Limits"},{"location":"documentation/ex_select/#grouping-and-column-operators","text":"# summary stats by is_old cols = [ #rtab['is_old'].count().label('count'), doctable.f.count().label('count'), rtab['age'].min().label('min'), rtab['age'].max().label('max'), (rtab['age'].sum()/rtab['age'].count()).label('mean'), ] core.query().select(cols, group_by=rtab['is_old']).df() 2023-11-13 08:38:34,018 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:34,019 INFO sqlalchemy.engine.Engine SELECT count(*) AS count, min(\"Record\".age) AS min, max(\"Record\".age) AS max, sum(\"Record\".age) / (count(\"Record\".age) + 0.0) AS mean FROM \"Record\" GROUP BY \"Record\".is_old 2023-11-13 08:38:34,019 INFO sqlalchemy.engine.Engine [generated in 0.00141s] () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count min max mean 0 5 25 47 36.8000000000 1 5 51 84 69.2000000000 # summarize age by decade decade_expression = doctable.f.round(rtab['age'] / 10) cols = [ decade_expression.label('decade'), rtab['age'].count().label('count'), rtab['age'].min().label('min'), rtab['age'].max().label('max'), (rtab['age'].sum()/rtab['age'].count()).label('mean'), ] core.query().select(cols, group_by=decade_expression).df() 2023-11-13 08:38:34,070 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:34,071 INFO sqlalchemy.engine.Engine SELECT round(\"Record\".age / (? + 0.0)) AS decade, count(\"Record\".age) AS count, min(\"Record\".age) AS min, max(\"Record\".age) AS max, sum(\"Record\".age) / (count(\"Record\".age) + 0.0) AS mean FROM \"Record\" GROUP BY round(\"Record\".age / (? + 0.0)) 2023-11-13 08:38:34,072 INFO sqlalchemy.engine.Engine [generated in 0.00134s] (10, 10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } decade count min max mean 0 3.0 2 25 30 27.5000000000 1 4.0 2 40 42 41.0000000000 2 5.0 2 47 51 49.0000000000 3 6.0 1 58 58 58.0000000000 4 8.0 3 75 84 79.0000000000","title":"Grouping and Column Operators"},{"location":"documentation/ex_select/#offset-and-selecting-chunks","text":"The offset parameter is used to pagify results into multiple queries - something that is particularly useful if the result set is too larget to fit into memory. # get the three oldest individuals, offset by three core.query().select(rtab.all_cols(), order_by=rtab['age'].desc(), limit=3, offset=3).df() 2023-11-13 08:38:34,121 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:34,121 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" ORDER BY \"Record\".age DESC LIMIT ? OFFSET ? 2023-11-13 08:38:34,122 INFO sqlalchemy.engine.Engine [generated in 0.00137s] (3, 3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age is_old name 0 10 58 1 user_9 1 5 51 1 user_4 2 9 47 0 user_8 for chunk in core.query().select_chunks(rtab.all_cols(), chunksize=3): print(chunk) 2023-11-13 08:38:34,169 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:34,169 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:34,170 INFO sqlalchemy.engine.Engine [generated in 0.00117s] (3, 0) [(1, 84, 1, 'user_0'), (2, 75, 1, 'user_1'), (3, 42, 0, 'user_2')] 2023-11-13 08:38:34,171 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:34,171 INFO sqlalchemy.engine.Engine [cached since 0.002855s ago] (3, 3) [(4, 25, 0, 'user_3'), (5, 51, 1, 'user_4'), (6, 40, 0, 'user_5')] 2023-11-13 08:38:34,172 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:34,173 INFO sqlalchemy.engine.Engine [cached since 0.004328s ago] (3, 6) [(7, 78, 1, 'user_6'), (8, 30, 0, 'user_7'), (9, 47, 0, 'user_8')] 2023-11-13 08:38:34,175 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:34,175 INFO sqlalchemy.engine.Engine [cached since 0.006853s ago] (3, 9) [(10, 58, 1, 'user_9')]","title":"Offset and Selecting Chunks"},{"location":"documentation/ex_select/#tablequery-basics","text":"The TableQuery interface is used to make table-specific queries, and, in exchange for this restriction, allows you to insert and select container objects directly. Queries on tables look much like their table-agnostic counterparts, with a few exceptions. Every query still begins with the .query() method, which returns a TableQuery object with methods for inserting and selecting container objects. List of selected columns is optional - if you do not specify, the query will default to all columns in the table. Otherwise, you should provide a subset of the columns, where all attributes that were not received will refer to doctable.MISSING , which you may check for downstream. Results of a select query are returned as a list of container objects. This means that we have called the .all() method on the sqlalchemy.CursorResult object. All returned results must match the attributes of the container object. Most often you will want to select raw database rows, but transformations via group_by and other operators are also possible as long as the result set attributes match the container attributes - that is, they can be expanded to the container constructor. Behavior of where , order_by , limit , are offset all operate as-expected. Below you can see a few examples demonstrating this behavior. rtab.query().select(where=rtab['age']>50) 2023-11-13 08:38:34,217 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:34,218 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" WHERE \"Record\".age > ? 2023-11-13 08:38:34,219 INFO sqlalchemy.engine.Engine [generated in 0.00128s] (50,) [Record(name='user_0', age=84, is_old=1, id=1), Record(name='user_1', age=75, is_old=1, id=2), Record(name='user_4', age=51, is_old=1, id=5), Record(name='user_6', age=78, is_old=1, id=7), Record(name='user_9', age=58, is_old=1, id=10)] result = rtab.query().select(rtab.cols('id', 'age'), where=rtab['is_old']) f'{result[0].name is doctable.MISSING=}', result 2023-11-13 08:38:34,268 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:34,269 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age FROM \"Record\" WHERE \"Record\".is_old 2023-11-13 08:38:34,270 INFO sqlalchemy.engine.Engine [generated in 0.00134s] () ('result[0].name is doctable.MISSING=True', [Record(name=MISSING, age=84, is_old=MISSING, id=1), Record(name=MISSING, age=75, is_old=MISSING, id=2), Record(name=MISSING, age=51, is_old=MISSING, id=5), Record(name=MISSING, age=78, is_old=MISSING, id=7), Record(name=MISSING, age=58, is_old=MISSING, id=10)]) # this is valid, although perhaps not recommended cols = [ (rtab['age'].sum()/rtab['age'].count()).label('age'), ] rtab.query().select(cols=cols) 2023-11-13 08:38:34,317 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:34,318 INFO sqlalchemy.engine.Engine SELECT sum(\"Record\".age) / (count(\"Record\".age) + 0.0) AS age FROM \"Record\" 2023-11-13 08:38:34,318 INFO sqlalchemy.engine.Engine [generated in 0.00123s] () [Record(name=MISSING, age=Decimal('53.0000000000'), is_old=MISSING, id=MISSING)] .select_chunks() also more or less works as-expected, with the chunks being converted to container objects. for chunk in rtab.query().select_chunks(chunksize=3): print(chunk) 2023-11-13 08:38:34,364 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-13 08:38:34,365 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:34,365 INFO sqlalchemy.engine.Engine [cached since 0.1964s ago] (3, 0) [Record(name='user_0', age=84, is_old=1, id=1), Record(name='user_1', age=75, is_old=1, id=2), Record(name='user_2', age=42, is_old=0, id=3)] 2023-11-13 08:38:34,366 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:34,367 INFO sqlalchemy.engine.Engine [cached since 0.198s ago] (3, 3) [Record(name='user_3', age=25, is_old=0, id=4), Record(name='user_4', age=51, is_old=1, id=5), Record(name='user_5', age=40, is_old=0, id=6)] 2023-11-13 08:38:34,368 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:34,368 INFO sqlalchemy.engine.Engine [cached since 0.1994s ago] (3, 6) [Record(name='user_6', age=78, is_old=1, id=7), Record(name='user_7', age=30, is_old=0, id=8), Record(name='user_8', age=47, is_old=0, id=9)] 2023-11-13 08:38:34,369 INFO sqlalchemy.engine.Engine SELECT \"Record\".id, \"Record\".age, \"Record\".is_old, \"Record\".name FROM \"Record\" LIMIT ? OFFSET ? 2023-11-13 08:38:34,369 INFO sqlalchemy.engine.Engine [cached since 0.2007s ago] (3, 9) [Record(name='user_9', age=58, is_old=1, id=10)]","title":"TableQuery Basics"},{"location":"documentation/iris_example/","text":"from __future__ import annotations import pprint import pandas as pd import sqlalchemy import sys sys.path.append('../') import doctable The iris dataset is simply a list of flowers with information about the sepal, petal, and species. iris_df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv') iris_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa Start by opening a connection to the database with a ConnectCore object. This object maintains the sqlalchemy metatdata, engine, and connections and is used to access all objects representing tables, queries, and more. core = doctable.ConnectCore.open( target=':memory:', # use a filename for a sqlite to write to disk dialect='sqlite', echo=True, ) core ConnectCore(target=':memory:', dialect='sqlite', engine=Engine(sqlite:///:memory:), metadata=MetaData()) Define a table using the table_schema decorator and listing attributes as you would a dataframe. To give more detail about a column, you can set the default value to Column() , which accepts FieldArgs to control the behavior of the dataframe container object, and ColumnArgs() to control behavior related to the database schema. import datetime @doctable.table_schema(table_name='iris', slots=True) class Iris: sepal_length: float sepal_width: float petal_length: float petal_width: float species: str id: int = doctable.Column( column_args=doctable.ColumnArgs(order=0, primary_key=True, autoincrement=True), ) updated: datetime.datetime = doctable.Column( column_args=doctable.ColumnArgs(default=datetime.datetime.utcnow), ) added: datetime.datetime = doctable.Column( column_args=doctable.ColumnArgs( default=datetime.datetime.utcnow, onupdate=datetime.datetime.utcnow ) ) @classmethod def from_row(cls, row: pd.Series): return cls(**row) Iris(sepal_length=1, sepal_width=2, petal_length=3, petal_width=4, species='setosa') Iris(sepal_length=1, sepal_width=2, petal_length=3, petal_width=4, species='setosa', id=MISSING, updated=MISSING, added=MISSING) We can start by creating new container object instances using the factory method constructor we created. irises = [Iris.from_row(row) for _, row in iris_df.iterrows()] print(irises[0]) Iris(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2, species='setosa', id=MISSING, updated=MISSING, added=MISSING) Use the begin_ddl() context manager to create database tables. with core.begin_ddl() as emitter: itab = emitter.create_table_if_not_exists(Iris) itab.inspect_columns() 2023-11-07 18:23:30,147 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 18:23:30,154 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"species\") 2023-11-07 18:23:30,156 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-07 18:23:30,156 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"iris_entry\") 2023-11-07 18:23:30,157 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-07 18:23:30,158 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"iris\") 2023-11-07 18:23:30,159 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-07 18:23:30,160 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"iris\") 2023-11-07 18:23:30,160 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-07 18:23:30,162 INFO sqlalchemy.engine.Engine CREATE TABLE iris ( id INTEGER, added DATETIME, petal_length FLOAT, petal_width FLOAT, sepal_length FLOAT, sepal_width FLOAT, species VARCHAR, updated DATETIME, PRIMARY KEY (id) ) 2023-11-07 18:23:30,163 INFO sqlalchemy.engine.Engine [no key 0.00088s] () 2023-11-07 18:23:30,164 INFO sqlalchemy.engine.Engine COMMIT 2023-11-07 18:23:30,165 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 18:23:30,171 INFO sqlalchemy.engine.Engine PRAGMA main.table_xinfo(\"iris\") 2023-11-07 18:23:30,172 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-07 18:23:30,173 INFO sqlalchemy.engine.Engine ROLLBACK [{'name': 'id', 'type': INTEGER(), 'nullable': True, 'default': None, 'primary_key': 1}, {'name': 'added', 'type': DATETIME(), 'nullable': True, 'default': None, 'primary_key': 0}, {'name': 'petal_length', 'type': FLOAT(), 'nullable': True, 'default': None, 'primary_key': 0}, {'name': 'petal_width', 'type': FLOAT(), 'nullable': True, 'default': None, 'primary_key': 0}, {'name': 'sepal_length', 'type': FLOAT(), 'nullable': True, 'default': None, 'primary_key': 0}, {'name': 'sepal_width', 'type': FLOAT(), 'nullable': True, 'default': None, 'primary_key': 0}, {'name': 'species', 'type': VARCHAR(), 'nullable': True, 'default': None, 'primary_key': 0}, {'name': 'updated', 'type': DATETIME(), 'nullable': True, 'default': None, 'primary_key': 0}] Running Queries General Queries Use the query() method of ConnectCore to perform queries using the doctable interface. with core.query() as q: q.delete(itab, all=True) q.insert_single(itab, { 'sepal_length': 1,'sepal_width': 2,'petal_length': 3,'petal_width': 4,'species': 'setosa' }) print(q.insert_multi.__doc__) q.insert_multi(itab, [ {'sepal_length': 1, 'sepal_width': 2, 'petal_length': 3, 'petal_width': 4, 'species': 'setosa'}, {'sepal_length': 1, 'sepal_width': 2, 'petal_length': 3, 'petal_width': 4, 'species': 'setosa'}, ]) print(q.select(itab).fetchall()) 2023-11-07 15:57:55,319 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,320 INFO sqlalchemy.engine.Engine DELETE FROM iris 2023-11-07 15:57:55,321 INFO sqlalchemy.engine.Engine [generated in 0.00228s] () 2023-11-07 15:57:55,325 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO iris (added, petal_length, petal_width, sepal_length, sepal_width, species, updated) VALUES (?, ?, ?, ?, ?, ?, ?) 2023-11-07 15:57:55,326 INFO sqlalchemy.engine.Engine [generated in 0.00199s] ('2023-11-07 20:57:55.324953', 3.0, 4.0, 1.0, 2.0, 'setosa', '2023-11-07 20:57:55.324955') Insert multiple rows into the database using executemany-style parameter binding. 2023-11-07 15:57:55,328 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO iris (added, petal_length, petal_width, sepal_length, sepal_width, species, updated) VALUES (?, ?, ?, ?, ?, ?, ?) 2023-11-07 15:57:55,329 INFO sqlalchemy.engine.Engine [generated in 0.00088s] [('2023-11-07 20:57:55.328713', 3.0, 4.0, 1.0, 2.0, 'setosa', '2023-11-07 20:57:55.328715'), ('2023-11-07 20:57:55.328719', 3.0, 4.0, 1.0, 2.0, 'setosa', '2023-11-07 20:57:55.328720')] 2023-11-07 15:57:55,332 INFO sqlalchemy.engine.Engine SELECT iris.id, iris.added, iris.petal_length, iris.petal_width, iris.sepal_length, iris.sepal_width, iris.species, iris.updated FROM iris 2023-11-07 15:57:55,333 INFO sqlalchemy.engine.Engine [generated in 0.00102s] () [(1, datetime.datetime(2023, 11, 7, 20, 57, 55, 324953), 3.0, 4.0, 1.0, 2.0, 'setosa', datetime.datetime(2023, 11, 7, 20, 57, 55, 324955)), (2, datetime.datetime(2023, 11, 7, 20, 57, 55, 328713), 3.0, 4.0, 1.0, 2.0, 'setosa', datetime.datetime(2023, 11, 7, 20, 57, 55, 328715)), (3, datetime.datetime(2023, 11, 7, 20, 57, 55, 328719), 3.0, 4.0, 1.0, 2.0, 'setosa', datetime.datetime(2023, 11, 7, 20, 57, 55, 328720))] 2023-11-07 15:57:55,334 INFO sqlalchemy.engine.Engine COMMIT Use cols() or __call__() to return a list of column objects associated with the given table. Column objects also have bound operators such as sum() , max() , and distinct() (see comment below for more). with core.query() as q: # use table.cols to specify which columns to select columns = itab.cols('sepal_length', 'sepal_width') pprint.pprint(q.select(columns).fetchall()) # use subscript to specify table for each column. use for table joins columns = [itab['sepal_length'], itab['sepal_width']] results = q.select(columns).fetchall() pprint.pprint(results) # use .sum(), .min(), .max(), .count(), .sum(), and .unique() to specify aggregate functions columns = [itab['species'].distinct()] result = q.select(columns).scalars() pprint.pprint(f'{result=}') # use in conjunction with group_by to specify groupings columns = [itab['sepal_length'].sum()] result = q.select(columns, group_by=[itab['species']]).scalar_one() pprint.pprint(f'{result=}') 2023-11-07 15:57:55,371 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,372 INFO sqlalchemy.engine.Engine SELECT iris.sepal_length, iris.sepal_width FROM iris 2023-11-07 15:57:55,375 INFO sqlalchemy.engine.Engine [generated in 0.00435s] () [(1.0, 2.0), (1.0, 2.0), (1.0, 2.0)] 2023-11-07 15:57:55,377 INFO sqlalchemy.engine.Engine SELECT iris.sepal_length, iris.sepal_width FROM iris 2023-11-07 15:57:55,377 INFO sqlalchemy.engine.Engine [cached since 0.006684s ago] () [(1.0, 2.0), (1.0, 2.0), (1.0, 2.0)] 2023-11-07 15:57:55,380 INFO sqlalchemy.engine.Engine SELECT distinct(iris.species) AS distinct_1 FROM iris 2023-11-07 15:57:55,381 INFO sqlalchemy.engine.Engine [generated in 0.00088s] () 'result=<sqlalchemy.engine.result.ScalarResult object at 0x7f40935dc730>' 2023-11-07 15:57:55,383 INFO sqlalchemy.engine.Engine SELECT sum(iris.sepal_length) AS sum_1 FROM iris GROUP BY iris.species 2023-11-07 15:57:55,385 INFO sqlalchemy.engine.Engine [generated in 0.00185s] () 'result=3.0' 2023-11-07 15:57:55,386 INFO sqlalchemy.engine.Engine COMMIT Table-specific Queries Use the query() method on a table to reference column names as strings and wrap results in container instances. with itab.query() as q: pprint.pprint(q.select()) 2023-11-07 15:57:55,420 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,421 INFO sqlalchemy.engine.Engine SELECT iris.id, iris.added, iris.petal_length, iris.petal_width, iris.sepal_length, iris.sepal_width, iris.species, iris.updated FROM iris 2023-11-07 15:57:55,422 INFO sqlalchemy.engine.Engine [cached since 0.08979s ago] () [Iris(sepal_length=1.0, sepal_width=2.0, petal_length=3.0, petal_width=4.0, species='setosa', id=1, updated=datetime.datetime(2023, 11, 7, 20, 57, 55, 324955), added=datetime.datetime(2023, 11, 7, 20, 57, 55, 324953)), Iris(sepal_length=1.0, sepal_width=2.0, petal_length=3.0, petal_width=4.0, species='setosa', id=2, updated=datetime.datetime(2023, 11, 7, 20, 57, 55, 328715), added=datetime.datetime(2023, 11, 7, 20, 57, 55, 328713)), Iris(sepal_length=1.0, sepal_width=2.0, petal_length=3.0, petal_width=4.0, species='setosa', id=3, updated=datetime.datetime(2023, 11, 7, 20, 57, 55, 328720), added=datetime.datetime(2023, 11, 7, 20, 57, 55, 328719))] 2023-11-07 15:57:55,423 INFO sqlalchemy.engine.Engine COMMIT All of the same query types can be used. with itab.query() as q: q.delete(all=True) q.insert_multi(irises) db_irises = q.select() print(len(db_irises)) pprint.pprint(db_irises[:2]) 2023-11-07 15:57:55,471 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,473 INFO sqlalchemy.engine.Engine DELETE FROM iris 2023-11-07 15:57:55,474 INFO sqlalchemy.engine.Engine [cached since 0.1547s ago] () 2023-11-07 15:57:55,486 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO iris (added, petal_length, petal_width, sepal_length, sepal_width, species, updated) VALUES (?, ?, ?, ?, ?, ?, ?) 2023-11-07 15:57:55,487 INFO sqlalchemy.engine.Engine [cached since 0.159s ago] [('2023-11-07 20:57:55.484095', 1.4, 0.2, 5.1, 3.5, 'setosa', '2023-11-07 20:57:55.484099'), ('2023-11-07 20:57:55.484100', 1.4, 0.2, 4.9, 3.0, 'setosa', '2023-11-07 20:57:55.484101'), ('2023-11-07 20:57:55.484101', 1.3, 0.2, 4.7, 3.2, 'setosa', '2023-11-07 20:57:55.484102'), ('2023-11-07 20:57:55.484103', 1.5, 0.2, 4.6, 3.1, 'setosa', '2023-11-07 20:57:55.484103'), ('2023-11-07 20:57:55.484104', 1.4, 0.2, 5.0, 3.6, 'setosa', '2023-11-07 20:57:55.484104'), ('2023-11-07 20:57:55.484105', 1.7, 0.4, 5.4, 3.9, 'setosa', '2023-11-07 20:57:55.484106'), ('2023-11-07 20:57:55.484106', 1.4, 0.3, 4.6, 3.4, 'setosa', '2023-11-07 20:57:55.484107'), ('2023-11-07 20:57:55.484108', 1.5, 0.2, 5.0, 3.4, 'setosa', '2023-11-07 20:57:55.484108') ... displaying 10 of 150 total bound parameter sets ... ('2023-11-07 20:57:55.484270', 5.4, 2.3, 6.2, 3.4, 'virginica', '2023-11-07 20:57:55.484270'), ('2023-11-07 20:57:55.484271', 5.1, 1.8, 5.9, 3.0, 'virginica', '2023-11-07 20:57:55.484271')] 2023-11-07 15:57:55,489 INFO sqlalchemy.engine.Engine SELECT iris.id, iris.added, iris.petal_length, iris.petal_width, iris.sepal_length, iris.sepal_width, iris.species, iris.updated FROM iris 2023-11-07 15:57:55,490 INFO sqlalchemy.engine.Engine [cached since 0.1582s ago] () 150 [Iris(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2, species='setosa', id=1, updated=datetime.datetime(2023, 11, 7, 20, 57, 55, 484099), added=datetime.datetime(2023, 11, 7, 20, 57, 55, 484095)), Iris(sepal_length=4.9, sepal_width=3.0, petal_length=1.4, petal_width=0.2, species='setosa', id=2, updated=datetime.datetime(2023, 11, 7, 20, 57, 55, 484101), added=datetime.datetime(2023, 11, 7, 20, 57, 55, 484100))] 2023-11-07 15:57:55,494 INFO sqlalchemy.engine.Engine COMMIT Attributes that were not requested from the database reference the doctable.MISSING sentinel value. with itab.query() as q: db_irises = q.select(['id', 'sepal_width', 'sepal_length']) pprint.pprint(db_irises[:2]) 2023-11-07 15:57:55,524 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,526 INFO sqlalchemy.engine.Engine SELECT iris.id, iris.sepal_width, iris.sepal_length FROM iris 2023-11-07 15:57:55,527 INFO sqlalchemy.engine.Engine [generated in 0.00249s] () [Iris(sepal_length=5.1, sepal_width=3.5, petal_length=MISSING, petal_width=MISSING, species=MISSING, id=1, updated=MISSING, added=MISSING), Iris(sepal_length=4.9, sepal_width=3.0, petal_length=MISSING, petal_width=MISSING, species=MISSING, id=2, updated=MISSING, added=MISSING)] 2023-11-07 15:57:55,530 INFO sqlalchemy.engine.Engine COMMIT Working with Multple Tables Now I'll demonstrate how to create and work with multi-table schemas with foreign key relationships. print(iris_df['species'].unique()) species_data = { 'setosa':'bristle-pointed iris', 'versicolor':'Southern blue flag', 'virginica':'Northern blue flag', } ['setosa' 'versicolor' 'virginica'] Here we create a foreign key constraint on the iris entries table that references a new species table. import typing @doctable.table_schema(table_name='species') class Species: name: str = doctable.Column(doctable.ColumnArgs(unique=True)) common_name: str = doctable.Column( column_args=doctable.ColumnArgs(nullable=True), ) id: int = doctable.Column(# will appear as the first column in the table column_args=doctable.ColumnArgs(order=0, primary_key=True, autoincrement=True), ) @classmethod def from_dict(cls, data: typing.Dict[str, str]) -> typing.List[Species]: return [cls(name=n, common_name=cn) for n,cn in data.items()] @doctable.table_schema( table_name='iris_entry', constraints=[ doctable.ForeignKey(['species'], ['species.name'], ondelete='CASCADE', onupdate='CASCADE'), ], ) class IrisEntry: sepal_length: float sepal_width: float petal_length: float petal_width: float species: str = doctable.Column( # NOTE: here I could add foreign_key='species.name' instead of adding fk constraint column_args=doctable.ColumnArgs(nullable=False), ) id: int = doctable.Column(# will appear as the first column in the table column_args=doctable.ColumnArgs(order=0, primary_key=True, autoincrement=True), ) @classmethod def from_dataframe(cls, df: pd.DataFrame) -> typing.List[IrisEntry]: return [cls(**row) for _,row in df.iterrows()] core = doctable.ConnectCore.open( target=':memory:', # use a filename for a sqlite to write to disk dialect='sqlite', echo=True, ) with core.begin_ddl() as emitter: core.enable_foreign_keys() spec_tab = emitter.create_table_if_not_exists(Species) iris_tab = emitter.create_table_if_not_exists(IrisEntry) print(spec_tab.inspect_columns()) 2023-11-07 15:57:55,647 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,648 INFO sqlalchemy.engine.Engine pragma foreign_keys=ON 2023-11-07 15:57:55,649 INFO sqlalchemy.engine.Engine [generated in 0.00072s] () 2023-11-07 15:57:55,650 INFO sqlalchemy.engine.Engine COMMIT 2023-11-07 15:57:55,653 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,653 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"species\") 2023-11-07 15:57:55,654 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-07 15:57:55,656 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"species\") 2023-11-07 15:57:55,656 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-07 15:57:55,657 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"iris_entry\") 2023-11-07 15:57:55,658 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-07 15:57:55,659 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"iris_entry\") 2023-11-07 15:57:55,659 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-07 15:57:55,660 INFO sqlalchemy.engine.Engine CREATE TABLE species ( id INTEGER, common_name VARCHAR, name VARCHAR, PRIMARY KEY (id), UNIQUE (name) ) 2023-11-07 15:57:55,661 INFO sqlalchemy.engine.Engine [no key 0.00057s] () 2023-11-07 15:57:55,664 INFO sqlalchemy.engine.Engine CREATE TABLE iris_entry ( id INTEGER, petal_length FLOAT, petal_width FLOAT, sepal_length FLOAT, sepal_width FLOAT, species VARCHAR NOT NULL, PRIMARY KEY (id), FOREIGN KEY(species) REFERENCES species (name) ON DELETE CASCADE ON UPDATE CASCADE ) 2023-11-07 15:57:55,664 INFO sqlalchemy.engine.Engine [no key 0.00054s] () 2023-11-07 15:57:55,665 INFO sqlalchemy.engine.Engine COMMIT 2023-11-07 15:57:55,666 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,667 INFO sqlalchemy.engine.Engine PRAGMA main.table_xinfo(\"species\") 2023-11-07 15:57:55,668 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-07 15:57:55,668 INFO sqlalchemy.engine.Engine ROLLBACK [{'name': 'id', 'type': INTEGER(), 'nullable': True, 'default': None, 'primary_key': 1}, {'name': 'common_name', 'type': VARCHAR(), 'nullable': True, 'default': None, 'primary_key': 0}, {'name': 'name', 'type': VARCHAR(), 'nullable': True, 'default': None, 'primary_key': 0}] Start by populating the species table. with spec_tab.query() as q: q.insert_multi(Species.from_dict(species_data), ifnotunique='replace') pprint.pprint(q.select()) 2023-11-07 15:57:55,766 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,767 INFO sqlalchemy.engine.Engine INSERT OR REPLACE INTO species (common_name, name) VALUES (?, ?) 2023-11-07 15:57:55,768 INFO sqlalchemy.engine.Engine [generated in 0.00240s] [('bristle-pointed iris', 'setosa'), ('Southern blue flag', 'versicolor'), ('Northern blue flag', 'virginica')] 2023-11-07 15:57:55,770 INFO sqlalchemy.engine.Engine SELECT species.id, species.common_name, species.name FROM species 2023-11-07 15:57:55,771 INFO sqlalchemy.engine.Engine [generated in 0.00073s] () [Species(name='setosa', common_name='bristle-pointed iris', id=1), Species(name='versicolor', common_name='Southern blue flag', id=2), Species(name='virginica', common_name='Northern blue flag', id=3)] 2023-11-07 15:57:55,772 INFO sqlalchemy.engine.Engine COMMIT We will get an error if the provided species does not correspond to a row in the species table. try: with iris_tab.query() as q: q.insert_single(IrisEntry( sepal_length=1, sepal_width=2, petal_length=3, petal_width=4, species='wrongname' # THIS PART CAUSED THE ERROR! )) except sqlalchemy.exc.IntegrityError: print('The species_name column is a foreign key to the species table, so it must be a valid species name.') 2023-11-07 15:57:55,821 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,823 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO iris_entry (petal_length, petal_width, sepal_length, sepal_width, species) VALUES (?, ?, ?, ?, ?) 2023-11-07 15:57:55,823 INFO sqlalchemy.engine.Engine [generated in 0.00239s] (3.0, 4.0, 1.0, 2.0, 'wrongname') 2023-11-07 15:57:55,824 INFO sqlalchemy.engine.Engine COMMIT The species_name column is a foreign key to the species table, so it must be a valid species name. Now that the species table is populated, we can insert the iris data. with iris_tab.query() as q: q.insert_multi(IrisEntry.from_dataframe(iris_df)) print(q.select(limit=2)) 2023-11-07 15:57:55,939 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,940 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO iris_entry (petal_length, petal_width, sepal_length, sepal_width, species) VALUES (?, ?, ?, ?, ?) 2023-11-07 15:57:55,941 INFO sqlalchemy.engine.Engine [generated in 0.00323s] [(1.4, 0.2, 5.1, 3.5, 'setosa'), (1.4, 0.2, 4.9, 3.0, 'setosa'), (1.3, 0.2, 4.7, 3.2, 'setosa'), (1.5, 0.2, 4.6, 3.1, 'setosa'), (1.4, 0.2, 5.0, 3.6, 'setosa'), (1.7, 0.4, 5.4, 3.9, 'setosa'), (1.4, 0.3, 4.6, 3.4, 'setosa'), (1.5, 0.2, 5.0, 3.4, 'setosa') ... displaying 10 of 150 total bound parameter sets ... (5.4, 2.3, 6.2, 3.4, 'virginica'), (5.1, 1.8, 5.9, 3.0, 'virginica')] 2023-11-07 15:57:55,944 INFO sqlalchemy.engine.Engine SELECT iris_entry.id, iris_entry.petal_length, iris_entry.petal_width, iris_entry.sepal_length, iris_entry.sepal_width, iris_entry.species FROM iris_entry LIMIT ? OFFSET ? 2023-11-07 15:57:55,944 INFO sqlalchemy.engine.Engine [generated in 0.00088s] (2, 0) [IrisEntry(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2, species='setosa', id=1), IrisEntry(sepal_length=4.9, sepal_width=3.0, petal_length=1.4, petal_width=0.2, species='setosa', id=2)] 2023-11-07 15:57:55,945 INFO sqlalchemy.engine.Engine COMMIT When the entry is deleted from the species tab, all associated irises are deleted. with spec_tab.query() as q: q.delete(spec_tab['name']=='setosa') print(f'{len(q.select())=}') with core.query() as q: print(q.select([iris_tab['species'].distinct()]).fetchall()) 2023-11-07 15:59:34,029 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:59:34,031 INFO sqlalchemy.engine.Engine DELETE FROM species WHERE species.name = ? 2023-11-07 15:59:34,031 INFO sqlalchemy.engine.Engine [cached since 98.05s ago] ('setosa',) 2023-11-07 15:59:34,032 INFO sqlalchemy.engine.Engine SELECT species.id, species.common_name, species.name FROM species 2023-11-07 15:59:34,033 INFO sqlalchemy.engine.Engine [cached since 98.26s ago] () len(q.select())=2 2023-11-07 15:59:34,034 INFO sqlalchemy.engine.Engine COMMIT 2023-11-07 15:59:34,035 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:59:34,036 INFO sqlalchemy.engine.Engine SELECT distinct(iris_entry.species) AS distinct_1 FROM iris_entry 2023-11-07 15:59:34,037 INFO sqlalchemy.engine.Engine [cached since 98.05s ago] () [('versicolor',), ('virginica',)] 2023-11-07 15:59:34,038 INFO sqlalchemy.engine.Engine COMMIT","title":"Iris example"},{"location":"documentation/iris_example/#running-queries","text":"","title":"Running Queries"},{"location":"documentation/iris_example/#general-queries","text":"Use the query() method of ConnectCore to perform queries using the doctable interface. with core.query() as q: q.delete(itab, all=True) q.insert_single(itab, { 'sepal_length': 1,'sepal_width': 2,'petal_length': 3,'petal_width': 4,'species': 'setosa' }) print(q.insert_multi.__doc__) q.insert_multi(itab, [ {'sepal_length': 1, 'sepal_width': 2, 'petal_length': 3, 'petal_width': 4, 'species': 'setosa'}, {'sepal_length': 1, 'sepal_width': 2, 'petal_length': 3, 'petal_width': 4, 'species': 'setosa'}, ]) print(q.select(itab).fetchall()) 2023-11-07 15:57:55,319 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,320 INFO sqlalchemy.engine.Engine DELETE FROM iris 2023-11-07 15:57:55,321 INFO sqlalchemy.engine.Engine [generated in 0.00228s] () 2023-11-07 15:57:55,325 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO iris (added, petal_length, petal_width, sepal_length, sepal_width, species, updated) VALUES (?, ?, ?, ?, ?, ?, ?) 2023-11-07 15:57:55,326 INFO sqlalchemy.engine.Engine [generated in 0.00199s] ('2023-11-07 20:57:55.324953', 3.0, 4.0, 1.0, 2.0, 'setosa', '2023-11-07 20:57:55.324955') Insert multiple rows into the database using executemany-style parameter binding. 2023-11-07 15:57:55,328 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO iris (added, petal_length, petal_width, sepal_length, sepal_width, species, updated) VALUES (?, ?, ?, ?, ?, ?, ?) 2023-11-07 15:57:55,329 INFO sqlalchemy.engine.Engine [generated in 0.00088s] [('2023-11-07 20:57:55.328713', 3.0, 4.0, 1.0, 2.0, 'setosa', '2023-11-07 20:57:55.328715'), ('2023-11-07 20:57:55.328719', 3.0, 4.0, 1.0, 2.0, 'setosa', '2023-11-07 20:57:55.328720')] 2023-11-07 15:57:55,332 INFO sqlalchemy.engine.Engine SELECT iris.id, iris.added, iris.petal_length, iris.petal_width, iris.sepal_length, iris.sepal_width, iris.species, iris.updated FROM iris 2023-11-07 15:57:55,333 INFO sqlalchemy.engine.Engine [generated in 0.00102s] () [(1, datetime.datetime(2023, 11, 7, 20, 57, 55, 324953), 3.0, 4.0, 1.0, 2.0, 'setosa', datetime.datetime(2023, 11, 7, 20, 57, 55, 324955)), (2, datetime.datetime(2023, 11, 7, 20, 57, 55, 328713), 3.0, 4.0, 1.0, 2.0, 'setosa', datetime.datetime(2023, 11, 7, 20, 57, 55, 328715)), (3, datetime.datetime(2023, 11, 7, 20, 57, 55, 328719), 3.0, 4.0, 1.0, 2.0, 'setosa', datetime.datetime(2023, 11, 7, 20, 57, 55, 328720))] 2023-11-07 15:57:55,334 INFO sqlalchemy.engine.Engine COMMIT Use cols() or __call__() to return a list of column objects associated with the given table. Column objects also have bound operators such as sum() , max() , and distinct() (see comment below for more). with core.query() as q: # use table.cols to specify which columns to select columns = itab.cols('sepal_length', 'sepal_width') pprint.pprint(q.select(columns).fetchall()) # use subscript to specify table for each column. use for table joins columns = [itab['sepal_length'], itab['sepal_width']] results = q.select(columns).fetchall() pprint.pprint(results) # use .sum(), .min(), .max(), .count(), .sum(), and .unique() to specify aggregate functions columns = [itab['species'].distinct()] result = q.select(columns).scalars() pprint.pprint(f'{result=}') # use in conjunction with group_by to specify groupings columns = [itab['sepal_length'].sum()] result = q.select(columns, group_by=[itab['species']]).scalar_one() pprint.pprint(f'{result=}') 2023-11-07 15:57:55,371 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,372 INFO sqlalchemy.engine.Engine SELECT iris.sepal_length, iris.sepal_width FROM iris 2023-11-07 15:57:55,375 INFO sqlalchemy.engine.Engine [generated in 0.00435s] () [(1.0, 2.0), (1.0, 2.0), (1.0, 2.0)] 2023-11-07 15:57:55,377 INFO sqlalchemy.engine.Engine SELECT iris.sepal_length, iris.sepal_width FROM iris 2023-11-07 15:57:55,377 INFO sqlalchemy.engine.Engine [cached since 0.006684s ago] () [(1.0, 2.0), (1.0, 2.0), (1.0, 2.0)] 2023-11-07 15:57:55,380 INFO sqlalchemy.engine.Engine SELECT distinct(iris.species) AS distinct_1 FROM iris 2023-11-07 15:57:55,381 INFO sqlalchemy.engine.Engine [generated in 0.00088s] () 'result=<sqlalchemy.engine.result.ScalarResult object at 0x7f40935dc730>' 2023-11-07 15:57:55,383 INFO sqlalchemy.engine.Engine SELECT sum(iris.sepal_length) AS sum_1 FROM iris GROUP BY iris.species 2023-11-07 15:57:55,385 INFO sqlalchemy.engine.Engine [generated in 0.00185s] () 'result=3.0' 2023-11-07 15:57:55,386 INFO sqlalchemy.engine.Engine COMMIT","title":"General Queries"},{"location":"documentation/iris_example/#table-specific-queries","text":"Use the query() method on a table to reference column names as strings and wrap results in container instances. with itab.query() as q: pprint.pprint(q.select()) 2023-11-07 15:57:55,420 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,421 INFO sqlalchemy.engine.Engine SELECT iris.id, iris.added, iris.petal_length, iris.petal_width, iris.sepal_length, iris.sepal_width, iris.species, iris.updated FROM iris 2023-11-07 15:57:55,422 INFO sqlalchemy.engine.Engine [cached since 0.08979s ago] () [Iris(sepal_length=1.0, sepal_width=2.0, petal_length=3.0, petal_width=4.0, species='setosa', id=1, updated=datetime.datetime(2023, 11, 7, 20, 57, 55, 324955), added=datetime.datetime(2023, 11, 7, 20, 57, 55, 324953)), Iris(sepal_length=1.0, sepal_width=2.0, petal_length=3.0, petal_width=4.0, species='setosa', id=2, updated=datetime.datetime(2023, 11, 7, 20, 57, 55, 328715), added=datetime.datetime(2023, 11, 7, 20, 57, 55, 328713)), Iris(sepal_length=1.0, sepal_width=2.0, petal_length=3.0, petal_width=4.0, species='setosa', id=3, updated=datetime.datetime(2023, 11, 7, 20, 57, 55, 328720), added=datetime.datetime(2023, 11, 7, 20, 57, 55, 328719))] 2023-11-07 15:57:55,423 INFO sqlalchemy.engine.Engine COMMIT All of the same query types can be used. with itab.query() as q: q.delete(all=True) q.insert_multi(irises) db_irises = q.select() print(len(db_irises)) pprint.pprint(db_irises[:2]) 2023-11-07 15:57:55,471 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,473 INFO sqlalchemy.engine.Engine DELETE FROM iris 2023-11-07 15:57:55,474 INFO sqlalchemy.engine.Engine [cached since 0.1547s ago] () 2023-11-07 15:57:55,486 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO iris (added, petal_length, petal_width, sepal_length, sepal_width, species, updated) VALUES (?, ?, ?, ?, ?, ?, ?) 2023-11-07 15:57:55,487 INFO sqlalchemy.engine.Engine [cached since 0.159s ago] [('2023-11-07 20:57:55.484095', 1.4, 0.2, 5.1, 3.5, 'setosa', '2023-11-07 20:57:55.484099'), ('2023-11-07 20:57:55.484100', 1.4, 0.2, 4.9, 3.0, 'setosa', '2023-11-07 20:57:55.484101'), ('2023-11-07 20:57:55.484101', 1.3, 0.2, 4.7, 3.2, 'setosa', '2023-11-07 20:57:55.484102'), ('2023-11-07 20:57:55.484103', 1.5, 0.2, 4.6, 3.1, 'setosa', '2023-11-07 20:57:55.484103'), ('2023-11-07 20:57:55.484104', 1.4, 0.2, 5.0, 3.6, 'setosa', '2023-11-07 20:57:55.484104'), ('2023-11-07 20:57:55.484105', 1.7, 0.4, 5.4, 3.9, 'setosa', '2023-11-07 20:57:55.484106'), ('2023-11-07 20:57:55.484106', 1.4, 0.3, 4.6, 3.4, 'setosa', '2023-11-07 20:57:55.484107'), ('2023-11-07 20:57:55.484108', 1.5, 0.2, 5.0, 3.4, 'setosa', '2023-11-07 20:57:55.484108') ... displaying 10 of 150 total bound parameter sets ... ('2023-11-07 20:57:55.484270', 5.4, 2.3, 6.2, 3.4, 'virginica', '2023-11-07 20:57:55.484270'), ('2023-11-07 20:57:55.484271', 5.1, 1.8, 5.9, 3.0, 'virginica', '2023-11-07 20:57:55.484271')] 2023-11-07 15:57:55,489 INFO sqlalchemy.engine.Engine SELECT iris.id, iris.added, iris.petal_length, iris.petal_width, iris.sepal_length, iris.sepal_width, iris.species, iris.updated FROM iris 2023-11-07 15:57:55,490 INFO sqlalchemy.engine.Engine [cached since 0.1582s ago] () 150 [Iris(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2, species='setosa', id=1, updated=datetime.datetime(2023, 11, 7, 20, 57, 55, 484099), added=datetime.datetime(2023, 11, 7, 20, 57, 55, 484095)), Iris(sepal_length=4.9, sepal_width=3.0, petal_length=1.4, petal_width=0.2, species='setosa', id=2, updated=datetime.datetime(2023, 11, 7, 20, 57, 55, 484101), added=datetime.datetime(2023, 11, 7, 20, 57, 55, 484100))] 2023-11-07 15:57:55,494 INFO sqlalchemy.engine.Engine COMMIT Attributes that were not requested from the database reference the doctable.MISSING sentinel value. with itab.query() as q: db_irises = q.select(['id', 'sepal_width', 'sepal_length']) pprint.pprint(db_irises[:2]) 2023-11-07 15:57:55,524 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,526 INFO sqlalchemy.engine.Engine SELECT iris.id, iris.sepal_width, iris.sepal_length FROM iris 2023-11-07 15:57:55,527 INFO sqlalchemy.engine.Engine [generated in 0.00249s] () [Iris(sepal_length=5.1, sepal_width=3.5, petal_length=MISSING, petal_width=MISSING, species=MISSING, id=1, updated=MISSING, added=MISSING), Iris(sepal_length=4.9, sepal_width=3.0, petal_length=MISSING, petal_width=MISSING, species=MISSING, id=2, updated=MISSING, added=MISSING)] 2023-11-07 15:57:55,530 INFO sqlalchemy.engine.Engine COMMIT","title":"Table-specific Queries"},{"location":"documentation/iris_example/#working-with-multple-tables","text":"Now I'll demonstrate how to create and work with multi-table schemas with foreign key relationships. print(iris_df['species'].unique()) species_data = { 'setosa':'bristle-pointed iris', 'versicolor':'Southern blue flag', 'virginica':'Northern blue flag', } ['setosa' 'versicolor' 'virginica'] Here we create a foreign key constraint on the iris entries table that references a new species table. import typing @doctable.table_schema(table_name='species') class Species: name: str = doctable.Column(doctable.ColumnArgs(unique=True)) common_name: str = doctable.Column( column_args=doctable.ColumnArgs(nullable=True), ) id: int = doctable.Column(# will appear as the first column in the table column_args=doctable.ColumnArgs(order=0, primary_key=True, autoincrement=True), ) @classmethod def from_dict(cls, data: typing.Dict[str, str]) -> typing.List[Species]: return [cls(name=n, common_name=cn) for n,cn in data.items()] @doctable.table_schema( table_name='iris_entry', constraints=[ doctable.ForeignKey(['species'], ['species.name'], ondelete='CASCADE', onupdate='CASCADE'), ], ) class IrisEntry: sepal_length: float sepal_width: float petal_length: float petal_width: float species: str = doctable.Column( # NOTE: here I could add foreign_key='species.name' instead of adding fk constraint column_args=doctable.ColumnArgs(nullable=False), ) id: int = doctable.Column(# will appear as the first column in the table column_args=doctable.ColumnArgs(order=0, primary_key=True, autoincrement=True), ) @classmethod def from_dataframe(cls, df: pd.DataFrame) -> typing.List[IrisEntry]: return [cls(**row) for _,row in df.iterrows()] core = doctable.ConnectCore.open( target=':memory:', # use a filename for a sqlite to write to disk dialect='sqlite', echo=True, ) with core.begin_ddl() as emitter: core.enable_foreign_keys() spec_tab = emitter.create_table_if_not_exists(Species) iris_tab = emitter.create_table_if_not_exists(IrisEntry) print(spec_tab.inspect_columns()) 2023-11-07 15:57:55,647 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,648 INFO sqlalchemy.engine.Engine pragma foreign_keys=ON 2023-11-07 15:57:55,649 INFO sqlalchemy.engine.Engine [generated in 0.00072s] () 2023-11-07 15:57:55,650 INFO sqlalchemy.engine.Engine COMMIT 2023-11-07 15:57:55,653 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,653 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"species\") 2023-11-07 15:57:55,654 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-07 15:57:55,656 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"species\") 2023-11-07 15:57:55,656 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-07 15:57:55,657 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"iris_entry\") 2023-11-07 15:57:55,658 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-07 15:57:55,659 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"iris_entry\") 2023-11-07 15:57:55,659 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-07 15:57:55,660 INFO sqlalchemy.engine.Engine CREATE TABLE species ( id INTEGER, common_name VARCHAR, name VARCHAR, PRIMARY KEY (id), UNIQUE (name) ) 2023-11-07 15:57:55,661 INFO sqlalchemy.engine.Engine [no key 0.00057s] () 2023-11-07 15:57:55,664 INFO sqlalchemy.engine.Engine CREATE TABLE iris_entry ( id INTEGER, petal_length FLOAT, petal_width FLOAT, sepal_length FLOAT, sepal_width FLOAT, species VARCHAR NOT NULL, PRIMARY KEY (id), FOREIGN KEY(species) REFERENCES species (name) ON DELETE CASCADE ON UPDATE CASCADE ) 2023-11-07 15:57:55,664 INFO sqlalchemy.engine.Engine [no key 0.00054s] () 2023-11-07 15:57:55,665 INFO sqlalchemy.engine.Engine COMMIT 2023-11-07 15:57:55,666 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,667 INFO sqlalchemy.engine.Engine PRAGMA main.table_xinfo(\"species\") 2023-11-07 15:57:55,668 INFO sqlalchemy.engine.Engine [raw sql] () 2023-11-07 15:57:55,668 INFO sqlalchemy.engine.Engine ROLLBACK [{'name': 'id', 'type': INTEGER(), 'nullable': True, 'default': None, 'primary_key': 1}, {'name': 'common_name', 'type': VARCHAR(), 'nullable': True, 'default': None, 'primary_key': 0}, {'name': 'name', 'type': VARCHAR(), 'nullable': True, 'default': None, 'primary_key': 0}] Start by populating the species table. with spec_tab.query() as q: q.insert_multi(Species.from_dict(species_data), ifnotunique='replace') pprint.pprint(q.select()) 2023-11-07 15:57:55,766 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,767 INFO sqlalchemy.engine.Engine INSERT OR REPLACE INTO species (common_name, name) VALUES (?, ?) 2023-11-07 15:57:55,768 INFO sqlalchemy.engine.Engine [generated in 0.00240s] [('bristle-pointed iris', 'setosa'), ('Southern blue flag', 'versicolor'), ('Northern blue flag', 'virginica')] 2023-11-07 15:57:55,770 INFO sqlalchemy.engine.Engine SELECT species.id, species.common_name, species.name FROM species 2023-11-07 15:57:55,771 INFO sqlalchemy.engine.Engine [generated in 0.00073s] () [Species(name='setosa', common_name='bristle-pointed iris', id=1), Species(name='versicolor', common_name='Southern blue flag', id=2), Species(name='virginica', common_name='Northern blue flag', id=3)] 2023-11-07 15:57:55,772 INFO sqlalchemy.engine.Engine COMMIT We will get an error if the provided species does not correspond to a row in the species table. try: with iris_tab.query() as q: q.insert_single(IrisEntry( sepal_length=1, sepal_width=2, petal_length=3, petal_width=4, species='wrongname' # THIS PART CAUSED THE ERROR! )) except sqlalchemy.exc.IntegrityError: print('The species_name column is a foreign key to the species table, so it must be a valid species name.') 2023-11-07 15:57:55,821 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,823 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO iris_entry (petal_length, petal_width, sepal_length, sepal_width, species) VALUES (?, ?, ?, ?, ?) 2023-11-07 15:57:55,823 INFO sqlalchemy.engine.Engine [generated in 0.00239s] (3.0, 4.0, 1.0, 2.0, 'wrongname') 2023-11-07 15:57:55,824 INFO sqlalchemy.engine.Engine COMMIT The species_name column is a foreign key to the species table, so it must be a valid species name. Now that the species table is populated, we can insert the iris data. with iris_tab.query() as q: q.insert_multi(IrisEntry.from_dataframe(iris_df)) print(q.select(limit=2)) 2023-11-07 15:57:55,939 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:57:55,940 INFO sqlalchemy.engine.Engine INSERT OR FAIL INTO iris_entry (petal_length, petal_width, sepal_length, sepal_width, species) VALUES (?, ?, ?, ?, ?) 2023-11-07 15:57:55,941 INFO sqlalchemy.engine.Engine [generated in 0.00323s] [(1.4, 0.2, 5.1, 3.5, 'setosa'), (1.4, 0.2, 4.9, 3.0, 'setosa'), (1.3, 0.2, 4.7, 3.2, 'setosa'), (1.5, 0.2, 4.6, 3.1, 'setosa'), (1.4, 0.2, 5.0, 3.6, 'setosa'), (1.7, 0.4, 5.4, 3.9, 'setosa'), (1.4, 0.3, 4.6, 3.4, 'setosa'), (1.5, 0.2, 5.0, 3.4, 'setosa') ... displaying 10 of 150 total bound parameter sets ... (5.4, 2.3, 6.2, 3.4, 'virginica'), (5.1, 1.8, 5.9, 3.0, 'virginica')] 2023-11-07 15:57:55,944 INFO sqlalchemy.engine.Engine SELECT iris_entry.id, iris_entry.petal_length, iris_entry.petal_width, iris_entry.sepal_length, iris_entry.sepal_width, iris_entry.species FROM iris_entry LIMIT ? OFFSET ? 2023-11-07 15:57:55,944 INFO sqlalchemy.engine.Engine [generated in 0.00088s] (2, 0) [IrisEntry(sepal_length=5.1, sepal_width=3.5, petal_length=1.4, petal_width=0.2, species='setosa', id=1), IrisEntry(sepal_length=4.9, sepal_width=3.0, petal_length=1.4, petal_width=0.2, species='setosa', id=2)] 2023-11-07 15:57:55,945 INFO sqlalchemy.engine.Engine COMMIT When the entry is deleted from the species tab, all associated irises are deleted. with spec_tab.query() as q: q.delete(spec_tab['name']=='setosa') print(f'{len(q.select())=}') with core.query() as q: print(q.select([iris_tab['species'].distinct()]).fetchall()) 2023-11-07 15:59:34,029 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:59:34,031 INFO sqlalchemy.engine.Engine DELETE FROM species WHERE species.name = ? 2023-11-07 15:59:34,031 INFO sqlalchemy.engine.Engine [cached since 98.05s ago] ('setosa',) 2023-11-07 15:59:34,032 INFO sqlalchemy.engine.Engine SELECT species.id, species.common_name, species.name FROM species 2023-11-07 15:59:34,033 INFO sqlalchemy.engine.Engine [cached since 98.26s ago] () len(q.select())=2 2023-11-07 15:59:34,034 INFO sqlalchemy.engine.Engine COMMIT 2023-11-07 15:59:34,035 INFO sqlalchemy.engine.Engine BEGIN (implicit) 2023-11-07 15:59:34,036 INFO sqlalchemy.engine.Engine SELECT distinct(iris_entry.species) AS distinct_1 FROM iris_entry 2023-11-07 15:59:34,037 INFO sqlalchemy.engine.Engine [cached since 98.05s ago] () [('versicolor',), ('virginica',)] 2023-11-07 15:59:34,038 INFO sqlalchemy.engine.Engine COMMIT","title":"Working with Multple Tables"},{"location":"legacy_documentation/dataclass_example/","text":"Dataclass Schema Example In this vignette I'll show how to use a Python dataclass (introduced in Python 3.7) to specify a schema for a DocTable. The advantage of this schema format is that you can use custom classes to represent each row, and easily convert your existing python objects into a format that it easy to store in a sqlite database. from datetime import datetime from pprint import pprint import pandas as pd import sys sys.path.append('..') import doctable Basic dataclass usage For our first example, we show how a basic dataclass object can be used as a DocTable schema. First we create a python dataclass using the @dataclass decorator. This object has three members, each defaulted to None . We can create this object using the constructor provided by dataclass . from dataclasses import dataclass @doctable.schema class User: __slots__ = [] name: str = None age: int = None height: float = None User() User(name=None, age=None, height=None) And it is relatively easy to create a new doctable using the schema provided by our dataclass User by providing the class definition to the schema argument. We can see that DocTable uses the dataclass schema to create a new table that follows the specified Python datatypes. db = doctable.DocTable(schema=User, target=':memory:') db.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 name VARCHAR True None auto 0 1 age INTEGER True None auto 0 2 height FLOAT True None auto 0 Now we insert several new objects into the table and view them using DocTable.head() . Note that the datbase actually inserted the object's defaulted values into the table. db.insert([User('kevin'), User('tyrone', age=12), User('carlos', age=25, height=6.5)]) db.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name age height 0 kevin NaN NaN 1 tyrone 12.0 NaN 2 carlos 25.0 6.5 Using a normal select() , we can extract the results as the original objects. With no parameters, the select statement extracts all columns as they are stored and they exactly match the original data we entered. As expected from the python object, we can access these as properties of the object. Due to the base class doctable.DocTableRow , we can also access properties using the __getitem__ indexing. I'll show why there is a difference btween the two later. users = db.select() for user in users: print(f\"{user.name}:\\n\\tage: {user.age}\\n\\theight: {user['height']}\") kevin: age: None height: None tyrone: age: 12 height: None carlos: age: 25 height: 6.5 Example using doctable.Col In this example, we will show how to create a dataclass with functionality that supports more complicated database operations. A key to this approach is to use the doctable.Col function as default values for our parameters. Note that when we initialize the object, the default values of all columns except for name are set to EmptyValue . This is important, because EmptyValue will indicate values that are not meant to be inserted into the database or are not retrieved from the database after selecting. @doctable.schema class User: __slots__ = [] name: str = doctable.Col() age: int = doctable.Col() height: float = doctable.Col() User() User() Given that the type specifications are the same as the previous example, we get exactly the same database schema. We insert entries just as before. The User data contained EmptyValue s, and so that column data was not presented to the database at all - instead, the schema's column defaults were used. Consistent with our schema (not the object defaults, the default values were set to None. db = doctable.DocTable(schema=User, target=':memory:') print(db.schema_table()) db.insert([User('kevin'), User('tyrone', age=12), User('carlos', age=25, height=6.5)]) for user in db.select(): print(f\"{user}\") name type nullable default autoincrement primary_key 0 name VARCHAR True None auto 0 1 age INTEGER True None auto 0 2 height FLOAT True None auto 0 User(name='kevin', age=None, height=None) User(name='tyrone', age=None, height=None) User(name='carlos', age=None, height=None) Now let's try to select only a subset of the columns - in this case, 'name' and 'age'. users = db.select(['name', 'age']) users[0] User(name='kevin', age=None) Note that the user height was set to EmptyValue . When we try to access height as an index, we get an error indicating that the data was not retrived in the select statement. try: users[0]['height'] except KeyError as e: print(e) 'The column \"height\" was not retreived in the select statement.' On the contrary, if we try to access as an attribute, the actual EmptyValue object is retrieved. Object properties work as they always have, but indexing into columns will check for errors in the program logic. This implementation shows how dataclass schemas walk the line between regular python objects and database rows, and thus accessing these values can be done differently depending on how much the table entries should be treated like regular objects vs database rows. This is all determined based on how the dataclass columns are configured. users[0].height EmptyValue() Special column types Now I'll introduce two special data column types provided by doctable: IDCol() , which represents a regular id column in sqlite with autoindex and primary_key parameters set, and UpdatedCol() , which records the datetime that an object was added to the database. When we create a new user using the dataclass constructor, these values are set to EmptyValue , and are relevant primarily to the database. By setting the repr parameter in the @dataclass decorator, we can use the __repr__ of the DocTableRow base class, which hides EmptyValue columns. This is optional. from dataclasses import field, fields @doctable.schema(repr=False) class User: __slots__ = [] id: int = doctable.IDCol() # shortcut for autoindex, primary_key column. updated: datetime = doctable.UpdatedCol() # shortcut for automatically name: str = doctable.Col(nullable=False) age: int = doctable.Col(None) # accessing sqlalchemy column keywords arguments user = User(name='carlos', age=15) user User(name='carlos', age=15) And we can see the relevance of those columns by inserting them into the database and selecting them again. You can see from the result of .head() that the primary key id and the updated columns were appropriately filled upon insertion. After selecting, these objects also contain valid values. db = doctable.DocTable(schema=User, target=':memory:') print(db.schema_table()) db.insert([User(name='kevin'), User(name='tyrone', age=12), User(name='carlos', age=25)]) db.head() name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 updated DATETIME True None auto 0 2 name VARCHAR False None auto 0 3 age INTEGER True None auto 0 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id updated name age 0 1 2021-07-21 19:03:04.550804 kevin NaN 1 2 2021-07-21 19:03:04.550809 tyrone 12.0 2 3 2021-07-21 19:03:04.550811 carlos 25.0 This was just an example of how regular Python dataclass objects can contain additional data which is relevant to the database, but which is otherwise unneeded. After retrieving from database, we can also use .update() to modify the entry. user = db.select_first() user.age = 10 db.update(user, where=db['id']==user['id']) db.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id updated name age 0 1 2021-07-21 19:03:04.550804 kevin 10 1 2 2021-07-21 19:03:04.550809 tyrone 12 2 3 2021-07-21 19:03:04.550811 carlos 25 We can use the convenience function update_dataclass() to update a single row corresponding to the object. user.age = 11 db.update_dataclass(user) db.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id updated name age 0 1 2021-07-21 19:03:04.550804 kevin 11 1 2 2021-07-21 19:03:04.550809 tyrone 12 2 3 2021-07-21 19:03:04.550811 carlos 25","title":"Dataclass Schema Example"},{"location":"legacy_documentation/dataclass_example/#dataclass-schema-example","text":"In this vignette I'll show how to use a Python dataclass (introduced in Python 3.7) to specify a schema for a DocTable. The advantage of this schema format is that you can use custom classes to represent each row, and easily convert your existing python objects into a format that it easy to store in a sqlite database. from datetime import datetime from pprint import pprint import pandas as pd import sys sys.path.append('..') import doctable","title":"Dataclass Schema Example"},{"location":"legacy_documentation/dataclass_example/#basic-dataclass-usage","text":"For our first example, we show how a basic dataclass object can be used as a DocTable schema. First we create a python dataclass using the @dataclass decorator. This object has three members, each defaulted to None . We can create this object using the constructor provided by dataclass . from dataclasses import dataclass @doctable.schema class User: __slots__ = [] name: str = None age: int = None height: float = None User() User(name=None, age=None, height=None) And it is relatively easy to create a new doctable using the schema provided by our dataclass User by providing the class definition to the schema argument. We can see that DocTable uses the dataclass schema to create a new table that follows the specified Python datatypes. db = doctable.DocTable(schema=User, target=':memory:') db.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 name VARCHAR True None auto 0 1 age INTEGER True None auto 0 2 height FLOAT True None auto 0 Now we insert several new objects into the table and view them using DocTable.head() . Note that the datbase actually inserted the object's defaulted values into the table. db.insert([User('kevin'), User('tyrone', age=12), User('carlos', age=25, height=6.5)]) db.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name age height 0 kevin NaN NaN 1 tyrone 12.0 NaN 2 carlos 25.0 6.5 Using a normal select() , we can extract the results as the original objects. With no parameters, the select statement extracts all columns as they are stored and they exactly match the original data we entered. As expected from the python object, we can access these as properties of the object. Due to the base class doctable.DocTableRow , we can also access properties using the __getitem__ indexing. I'll show why there is a difference btween the two later. users = db.select() for user in users: print(f\"{user.name}:\\n\\tage: {user.age}\\n\\theight: {user['height']}\") kevin: age: None height: None tyrone: age: 12 height: None carlos: age: 25 height: 6.5","title":"Basic dataclass usage"},{"location":"legacy_documentation/dataclass_example/#example-using-doctablecol","text":"In this example, we will show how to create a dataclass with functionality that supports more complicated database operations. A key to this approach is to use the doctable.Col function as default values for our parameters. Note that when we initialize the object, the default values of all columns except for name are set to EmptyValue . This is important, because EmptyValue will indicate values that are not meant to be inserted into the database or are not retrieved from the database after selecting. @doctable.schema class User: __slots__ = [] name: str = doctable.Col() age: int = doctable.Col() height: float = doctable.Col() User() User() Given that the type specifications are the same as the previous example, we get exactly the same database schema. We insert entries just as before. The User data contained EmptyValue s, and so that column data was not presented to the database at all - instead, the schema's column defaults were used. Consistent with our schema (not the object defaults, the default values were set to None. db = doctable.DocTable(schema=User, target=':memory:') print(db.schema_table()) db.insert([User('kevin'), User('tyrone', age=12), User('carlos', age=25, height=6.5)]) for user in db.select(): print(f\"{user}\") name type nullable default autoincrement primary_key 0 name VARCHAR True None auto 0 1 age INTEGER True None auto 0 2 height FLOAT True None auto 0 User(name='kevin', age=None, height=None) User(name='tyrone', age=None, height=None) User(name='carlos', age=None, height=None) Now let's try to select only a subset of the columns - in this case, 'name' and 'age'. users = db.select(['name', 'age']) users[0] User(name='kevin', age=None) Note that the user height was set to EmptyValue . When we try to access height as an index, we get an error indicating that the data was not retrived in the select statement. try: users[0]['height'] except KeyError as e: print(e) 'The column \"height\" was not retreived in the select statement.' On the contrary, if we try to access as an attribute, the actual EmptyValue object is retrieved. Object properties work as they always have, but indexing into columns will check for errors in the program logic. This implementation shows how dataclass schemas walk the line between regular python objects and database rows, and thus accessing these values can be done differently depending on how much the table entries should be treated like regular objects vs database rows. This is all determined based on how the dataclass columns are configured. users[0].height EmptyValue()","title":"Example using doctable.Col"},{"location":"legacy_documentation/dataclass_example/#special-column-types","text":"Now I'll introduce two special data column types provided by doctable: IDCol() , which represents a regular id column in sqlite with autoindex and primary_key parameters set, and UpdatedCol() , which records the datetime that an object was added to the database. When we create a new user using the dataclass constructor, these values are set to EmptyValue , and are relevant primarily to the database. By setting the repr parameter in the @dataclass decorator, we can use the __repr__ of the DocTableRow base class, which hides EmptyValue columns. This is optional. from dataclasses import field, fields @doctable.schema(repr=False) class User: __slots__ = [] id: int = doctable.IDCol() # shortcut for autoindex, primary_key column. updated: datetime = doctable.UpdatedCol() # shortcut for automatically name: str = doctable.Col(nullable=False) age: int = doctable.Col(None) # accessing sqlalchemy column keywords arguments user = User(name='carlos', age=15) user User(name='carlos', age=15) And we can see the relevance of those columns by inserting them into the database and selecting them again. You can see from the result of .head() that the primary key id and the updated columns were appropriately filled upon insertion. After selecting, these objects also contain valid values. db = doctable.DocTable(schema=User, target=':memory:') print(db.schema_table()) db.insert([User(name='kevin'), User(name='tyrone', age=12), User(name='carlos', age=25)]) db.head() name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 updated DATETIME True None auto 0 2 name VARCHAR False None auto 0 3 age INTEGER True None auto 0 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id updated name age 0 1 2021-07-21 19:03:04.550804 kevin NaN 1 2 2021-07-21 19:03:04.550809 tyrone 12.0 2 3 2021-07-21 19:03:04.550811 carlos 25.0 This was just an example of how regular Python dataclass objects can contain additional data which is relevant to the database, but which is otherwise unneeded. After retrieving from database, we can also use .update() to modify the entry. user = db.select_first() user.age = 10 db.update(user, where=db['id']==user['id']) db.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id updated name age 0 1 2021-07-21 19:03:04.550804 kevin 10 1 2 2021-07-21 19:03:04.550809 tyrone 12 2 3 2021-07-21 19:03:04.550811 carlos 25 We can use the convenience function update_dataclass() to update a single row corresponding to the object. user.age = 11 db.update_dataclass(user) db.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id updated name age 0 1 2021-07-21 19:03:04.550804 kevin 11 1 2 2021-07-21 19:03:04.550809 tyrone 12 2 3 2021-07-21 19:03:04.550811 carlos 25","title":"Special column types"},{"location":"legacy_documentation/dataclass_vignette_advanced/","text":"Advanced dataclass schema vignette In this vignette I'll show how to create more complicated database schemas from dataclasses.","title":"Advanced dataclass schema vignette"},{"location":"legacy_documentation/dataclass_vignette_advanced/#advanced-dataclass-schema-vignette","text":"In this vignette I'll show how to create more complicated database schemas from dataclasses.","title":"Advanced dataclass schema vignette"},{"location":"legacy_documentation/depric_vignette_newsgroups/","text":"NewsGroups Dataset Vignette In this vignette, I will show you how to create a database for storing and manipulating Introduction to dataset We will be using the 20 Newsgroups dataset for this vignette. This is the sklearn website description : The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date. We use sklearn's fetch_20newsgroups method to download and access articles from the politics newsgroup. import sklearn.datasets newsgroups = sklearn.datasets.fetch_20newsgroups(categories=['talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc']) newsgroups.keys(), len(newsgroups['data']) (dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR']), 1575) This is an example of a newsgroup post. print(newsgroups['data'][0]) From: golchowy@alchemy.chem.utoronto.ca (Gerald Olchowy) Subject: Re: Help fight the Clinton Administration's invasion of your privacy Organization: University of Toronto Chemistry Department Lines: 16 In article <9308@blue.cis.pitt.edu> cjp+@pitt.edu (Casimir J Palowitch) writes: >The Clinton Administration wants to \"manage\" your use of digital >encryption. This includes a proposal which would limit your use of >encryption to a standard developed by the NSA, the technical details of >which would remain classified with the government. > >This cannot be allowed to happen. > It is a bit unfair to call blame the Clinton Administration alone...this initiative was underway under the Bush Administration...it is basically a bipartisan effort of the establishment Demopublicans and Republicrats...the same bipartisan effort that brought the S&L scandal, and BCCI, etc. Gerald It looks very similar to an email, so we will use Python's email package to parse the text and return a dictionary containing the various relevant fields. Our parse_email function shows how we can extract metadata fields like author, subject, and organization from the message, as well as the main text body. import email def parse_newsgroup(email_text): message = email.message_from_string(email_text) return { 'author': message['from'], 'subject': message['Subject'], 'organization': message['Organization'], 'lines': int(message['Lines']), 'text': message.get_payload(), } parse_newsgroup(newsgroups['data'][0]) {'author': 'golchowy@alchemy.chem.utoronto.ca (Gerald Olchowy)', 'subject': \"Re: Help fight the Clinton Administration's invasion of your privacy\", 'organization': 'University of Toronto Chemistry Department', 'lines': 16, 'text': 'In article <9308@blue.cis.pitt.edu> cjp+@pitt.edu (Casimir J Palowitch) writes:\\n>The Clinton Administration wants to \"manage\" your use of digital\\n>encryption. This includes a proposal which would limit your use of\\n>encryption to a standard developed by the NSA, the technical details of \\n>which would remain classified with the government.\\n>\\n>This cannot be allowed to happen.\\n>\\n\\nIt is a bit unfair to call blame the Clinton Administration alone...this\\ninitiative was underway under the Bush Administration...it is basically\\na bipartisan effort of the establishment Demopublicans and\\nRepublicrats...the same bipartisan effort that brought the S&L scandal,\\nand BCCI, etc.\\n\\nGerald\\n'} Creating a database schema The first step will be to create a database schema that is appropriate for the newsgroup dataset by defining a container dataclass using the @schema decorator. The schema decorator will convert the class into a dataclass with slots enabled (provided __slots__ = [] is given in the definition), and inherit from DocTableRow to add some additional functionality. The type hints associated with each variable will be used in the schema definition for the new tables, and arguments to Col() , IDCol() , AddedCol() , and UpdatedCol() will mostly be passed to dataclasses.field (see docs for more detail), so all dataclass functionality is maintained. The doctable schema guide explains more about schema and schema object definitions. Here I define a NewsgroupDoc class to represent a single document and define __slots__ so the decorator can automatically create a slot class. Each member variable will act as a column in our database schema, and the first variable we define is an id column with the defaulted value IDCol() . This is a special function that will translate to a schema that uses the id colum as the primary key and enable auto-incrementing. Because id is defaulted, we must default our other variables as well. I also define a couple of methods as part of our schema class - they are ignored in the schema creation process, but allow us to manipulate the object within Python. The author_email property will extract just the email address from the author field. Note that even though it is a property, it is defined as a method and therefore will not be considered when creating the class schema. I also define a classmethod that can be used to create a new NewsgroupDoc from the newsgroup text - this replaces the functionality of the parse_email function we created above. This way, the class knows how to create itself from the raw newsgroup text. import sys sys.path.append('..') import doctable import re import email import dataclasses def try_int(text): try: return int(text.split()[0]) except: return None @doctable.schema class NewsgroupDoc: __slots__ = [] # schema columns id: int = doctable.IDCol() author: str = None subject: str = None organization: str = None length: int = None text: str = None @property def author_email(self, pattern=re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')): '''Get the author\\'s email address from the author field text. ''' return re.search(pattern, self.author)[0] @classmethod def from_string(cls, newsgroup_text): '''Code to create a NewsGroupDoc object from the original newsgroup string. ''' message = email.message_from_string(newsgroup_text) return cls( author = message['from'], subject = message['Subject'], organization = message['Organization'], length = len(message.get_payload()), text = message.get_payload(), ) # for example, we create a new NewsGroupDoc from the first newsgroup article ngdoc = NewsgroupDoc.from_string(newsgroups['data'][0]) print(ngdoc.author) ngdoc.author_email golchowy@alchemy.chem.utoronto.ca (Gerald Olchowy) 'golchowy@alchemy.chem.utoronto.ca' To make sure the NewsgroupDoc will translate to the database schema we expect, we can create a new DocTable object that uses it as a schema. We use the schema argument of the DocTable constructor to specify the schema, and print it below. See that most fields were translated to VARCHAR type fields, but id and length were translated to INTEGER types based on their type hints. ng_table = doctable.DocTable(target=':memory:', tabname='documents', schema=NewsgroupDoc) ng_table.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 author VARCHAR True None auto 0 2 subject VARCHAR True None auto 0 3 organization VARCHAR True None auto 0 4 length INTEGER True None auto 0 5 text VARCHAR True None auto 0 To better describe the data we are interested in, we now create a class that inherits from DocTable . This class will act as the main interface for working with our dataset. We use the _tabname_ and _schema_ properties to define the table name and schema so we don't need to include them in the constructor. We also define a method count_author_emails - we will describe the behavior of this method later. import collections class NewsgroupTable(doctable.DocTable): _tabname_ = 'documents' _schema_ = NewsgroupDoc def count_author_emails(self, *args, **kwargs): author_emails = self.select('author', *args, **kwargs) return collections.Counter(author_emails) Instead of using target=':memory:' , we want to create a database on our filesystem so we can store data. By default, DocTable uses sqlite as the database engine, so with target we need only specify a filename. Because this is just a demonstration, we will create the database in a temporary folder using the tempfile package. This database does not exist yet, so we use the new_db flag to indicate that a new one should be created. import tempfile tempfolder = tempfile.TemporaryDirectory() table_fname = f'{tempfolder.name}/tmp1.db' ng_table = NewsgroupTable(target=table_fname, new_db=True) ng_table.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 author VARCHAR True None auto 0 2 subject VARCHAR True None auto 0 3 organization VARCHAR True None auto 0 4 length INTEGER True None auto 0 5 text VARCHAR True None auto 0 Parsing and storing documents Now we would like to parse our documents for storage in the database. It is relatively straighforward to create a list of parsed texts using the from_string method. After doing this, we could potentially just insert them directly into the database. %timeit [NewsgroupDoc.from_string(text) for text in newsgroups['data']] 191 ms \u00b1 527 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) This is a relatively straigtforward task with a dataset of this size, but if we had a larger dataset or used more costly parsing algorithms, we would want to distribute parsing across multiple processes - we will take that approach for demonstration. First we define the process_and_store class to be used in each worker process. def thread_func(numbers, db): print(f'starting process') db.reopen_engine() # create all new connections db.insert([{'subject': i} for i in numbers]) #for num in numbers: # db.insert({'process': process_id, 'number': num}) # time.sleep(0.01) numbers = list(range(100)) # these numbers are to be inserted into the database ng_table.delete() with doctable.Distribute(5) as d: d.map_chunk(thread_func, numbers, ng_table) ng_table.head(10) starting process starting process starting process starting process starting process .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id author subject organization length text 0 1 None 0 None None None 1 2 None 1 None None None 2 3 None 2 None None None 3 4 None 3 None None None 4 5 None 4 None None None 5 6 None 5 None None None 6 7 None 6 None None None 7 8 None 7 None None None 8 9 None 8 None None None 9 10 None 9 None None None def printer(x, table): print(x, table) with doctable.WorkerPool(3, verbose=False) as p: assert(p.any_alive()) print(f'av efficiency: {p.av_efficiency()}') p.map(printer, list(range(100)), table=ng_table) # test most basic map function #elements = list(range(100)) #assert(pool.map(example_func, elements) == [example_func(e) for e in elements]) print(f'av efficiency: {p.av_efficiency()}') --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-10-48aa27ce5dc0> in <module> 3 4 with doctable.WorkerPool(3, verbose=False) as p: ----> 5 assert(p.any_alive()) 6 print(f'av efficiency: {p.av_efficiency()}') 7 AssertionError: import pickle pickle.dumps(ng_table.schema_info) import multiprocessing class parse_thread: def __init__(self, table: doctable.DocTable): self.table = table def __call__(self, texts): with self.table as t: #records = [NewsgroupDoc.from_string(text) for text in texts] t.insert(NewsgroupDoc(1000)) def parse_thread2(x): return None chunks = doctable.chunk(newsgroups['data'], chunk_size=100) #parse_func = parse_thread(ng_table) with multiprocessing.Pool(4) as p: %time p.map(parse_thread(ng_table), chunks, 100) #%time map(parse_thread(1), chunks) class process_and_store: table: doctable.DocTable = None def __init__(self, table_cls, *table_args, **table_kwargs): '''Store info to construct the table. ''' self.table_cls = table_cls self.table_args = table_args self.table_kwargs = table_kwargs def connect_db(self): '''Make a new connection to the database and return the associated table. ''' if self.table is None: self.table = self.table_cls(*self.table_args, **self.table_kwargs) return self.table def __call__(self, text): '''Execute function in worker process. ''' table = self.connect_db() record = NewsgroupDoc.from_string(text) table.insert(record) import multiprocessing with multiprocessing.Pool(4) as p: %time p.map(process_and_store(NewsgroupTable, target=table_fname), newsgroups['data']) Notice that this takes very little CPU time, but a long \"wall time\" (overall time it takes to run the program). This is because the threads are IO-starved - they spend a lot of time waiting on each other to commit database transactions. This might be a good opportunity to use variations on threading models, but most parsing classes class process_and_store_chunk(process_and_store): def __call__(self, texts): '''Execute function in worker process. ''' table = self.connect_db() records = [NewsgroupDoc.from_string(text) for text in texts] table.insert(records) chunked_newsgroups = doctable.chunk(newsgroups['data'], chunk_size=500) with multiprocessing.Pool(4) as p: %time p.map(process_and_store_chunk(NewsgroupTable, target=table_fname), chunked_newsgroups) parser = ParsePipeline([ parse_email ]) for email_text in newsgroups['data']: email_data = parse_email(email_text) import multiprocessing with multiprocessing.Pool(10) as p: print(p)","title":"NewsGroups Dataset Vignette"},{"location":"legacy_documentation/depric_vignette_newsgroups/#newsgroups-dataset-vignette","text":"In this vignette, I will show you how to create a database for storing and manipulating","title":"NewsGroups Dataset Vignette"},{"location":"legacy_documentation/depric_vignette_newsgroups/#introduction-to-dataset","text":"We will be using the 20 Newsgroups dataset for this vignette. This is the sklearn website description : The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date. We use sklearn's fetch_20newsgroups method to download and access articles from the politics newsgroup. import sklearn.datasets newsgroups = sklearn.datasets.fetch_20newsgroups(categories=['talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc']) newsgroups.keys(), len(newsgroups['data']) (dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR']), 1575) This is an example of a newsgroup post. print(newsgroups['data'][0]) From: golchowy@alchemy.chem.utoronto.ca (Gerald Olchowy) Subject: Re: Help fight the Clinton Administration's invasion of your privacy Organization: University of Toronto Chemistry Department Lines: 16 In article <9308@blue.cis.pitt.edu> cjp+@pitt.edu (Casimir J Palowitch) writes: >The Clinton Administration wants to \"manage\" your use of digital >encryption. This includes a proposal which would limit your use of >encryption to a standard developed by the NSA, the technical details of >which would remain classified with the government. > >This cannot be allowed to happen. > It is a bit unfair to call blame the Clinton Administration alone...this initiative was underway under the Bush Administration...it is basically a bipartisan effort of the establishment Demopublicans and Republicrats...the same bipartisan effort that brought the S&L scandal, and BCCI, etc. Gerald It looks very similar to an email, so we will use Python's email package to parse the text and return a dictionary containing the various relevant fields. Our parse_email function shows how we can extract metadata fields like author, subject, and organization from the message, as well as the main text body. import email def parse_newsgroup(email_text): message = email.message_from_string(email_text) return { 'author': message['from'], 'subject': message['Subject'], 'organization': message['Organization'], 'lines': int(message['Lines']), 'text': message.get_payload(), } parse_newsgroup(newsgroups['data'][0]) {'author': 'golchowy@alchemy.chem.utoronto.ca (Gerald Olchowy)', 'subject': \"Re: Help fight the Clinton Administration's invasion of your privacy\", 'organization': 'University of Toronto Chemistry Department', 'lines': 16, 'text': 'In article <9308@blue.cis.pitt.edu> cjp+@pitt.edu (Casimir J Palowitch) writes:\\n>The Clinton Administration wants to \"manage\" your use of digital\\n>encryption. This includes a proposal which would limit your use of\\n>encryption to a standard developed by the NSA, the technical details of \\n>which would remain classified with the government.\\n>\\n>This cannot be allowed to happen.\\n>\\n\\nIt is a bit unfair to call blame the Clinton Administration alone...this\\ninitiative was underway under the Bush Administration...it is basically\\na bipartisan effort of the establishment Demopublicans and\\nRepublicrats...the same bipartisan effort that brought the S&L scandal,\\nand BCCI, etc.\\n\\nGerald\\n'}","title":"Introduction to dataset"},{"location":"legacy_documentation/depric_vignette_newsgroups/#creating-a-database-schema","text":"The first step will be to create a database schema that is appropriate for the newsgroup dataset by defining a container dataclass using the @schema decorator. The schema decorator will convert the class into a dataclass with slots enabled (provided __slots__ = [] is given in the definition), and inherit from DocTableRow to add some additional functionality. The type hints associated with each variable will be used in the schema definition for the new tables, and arguments to Col() , IDCol() , AddedCol() , and UpdatedCol() will mostly be passed to dataclasses.field (see docs for more detail), so all dataclass functionality is maintained. The doctable schema guide explains more about schema and schema object definitions. Here I define a NewsgroupDoc class to represent a single document and define __slots__ so the decorator can automatically create a slot class. Each member variable will act as a column in our database schema, and the first variable we define is an id column with the defaulted value IDCol() . This is a special function that will translate to a schema that uses the id colum as the primary key and enable auto-incrementing. Because id is defaulted, we must default our other variables as well. I also define a couple of methods as part of our schema class - they are ignored in the schema creation process, but allow us to manipulate the object within Python. The author_email property will extract just the email address from the author field. Note that even though it is a property, it is defined as a method and therefore will not be considered when creating the class schema. I also define a classmethod that can be used to create a new NewsgroupDoc from the newsgroup text - this replaces the functionality of the parse_email function we created above. This way, the class knows how to create itself from the raw newsgroup text. import sys sys.path.append('..') import doctable import re import email import dataclasses def try_int(text): try: return int(text.split()[0]) except: return None @doctable.schema class NewsgroupDoc: __slots__ = [] # schema columns id: int = doctable.IDCol() author: str = None subject: str = None organization: str = None length: int = None text: str = None @property def author_email(self, pattern=re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')): '''Get the author\\'s email address from the author field text. ''' return re.search(pattern, self.author)[0] @classmethod def from_string(cls, newsgroup_text): '''Code to create a NewsGroupDoc object from the original newsgroup string. ''' message = email.message_from_string(newsgroup_text) return cls( author = message['from'], subject = message['Subject'], organization = message['Organization'], length = len(message.get_payload()), text = message.get_payload(), ) # for example, we create a new NewsGroupDoc from the first newsgroup article ngdoc = NewsgroupDoc.from_string(newsgroups['data'][0]) print(ngdoc.author) ngdoc.author_email golchowy@alchemy.chem.utoronto.ca (Gerald Olchowy) 'golchowy@alchemy.chem.utoronto.ca' To make sure the NewsgroupDoc will translate to the database schema we expect, we can create a new DocTable object that uses it as a schema. We use the schema argument of the DocTable constructor to specify the schema, and print it below. See that most fields were translated to VARCHAR type fields, but id and length were translated to INTEGER types based on their type hints. ng_table = doctable.DocTable(target=':memory:', tabname='documents', schema=NewsgroupDoc) ng_table.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 author VARCHAR True None auto 0 2 subject VARCHAR True None auto 0 3 organization VARCHAR True None auto 0 4 length INTEGER True None auto 0 5 text VARCHAR True None auto 0 To better describe the data we are interested in, we now create a class that inherits from DocTable . This class will act as the main interface for working with our dataset. We use the _tabname_ and _schema_ properties to define the table name and schema so we don't need to include them in the constructor. We also define a method count_author_emails - we will describe the behavior of this method later. import collections class NewsgroupTable(doctable.DocTable): _tabname_ = 'documents' _schema_ = NewsgroupDoc def count_author_emails(self, *args, **kwargs): author_emails = self.select('author', *args, **kwargs) return collections.Counter(author_emails) Instead of using target=':memory:' , we want to create a database on our filesystem so we can store data. By default, DocTable uses sqlite as the database engine, so with target we need only specify a filename. Because this is just a demonstration, we will create the database in a temporary folder using the tempfile package. This database does not exist yet, so we use the new_db flag to indicate that a new one should be created. import tempfile tempfolder = tempfile.TemporaryDirectory() table_fname = f'{tempfolder.name}/tmp1.db' ng_table = NewsgroupTable(target=table_fname, new_db=True) ng_table.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 author VARCHAR True None auto 0 2 subject VARCHAR True None auto 0 3 organization VARCHAR True None auto 0 4 length INTEGER True None auto 0 5 text VARCHAR True None auto 0","title":"Creating a database schema"},{"location":"legacy_documentation/depric_vignette_newsgroups/#parsing-and-storing-documents","text":"Now we would like to parse our documents for storage in the database. It is relatively straighforward to create a list of parsed texts using the from_string method. After doing this, we could potentially just insert them directly into the database. %timeit [NewsgroupDoc.from_string(text) for text in newsgroups['data']] 191 ms \u00b1 527 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) This is a relatively straigtforward task with a dataset of this size, but if we had a larger dataset or used more costly parsing algorithms, we would want to distribute parsing across multiple processes - we will take that approach for demonstration. First we define the process_and_store class to be used in each worker process. def thread_func(numbers, db): print(f'starting process') db.reopen_engine() # create all new connections db.insert([{'subject': i} for i in numbers]) #for num in numbers: # db.insert({'process': process_id, 'number': num}) # time.sleep(0.01) numbers = list(range(100)) # these numbers are to be inserted into the database ng_table.delete() with doctable.Distribute(5) as d: d.map_chunk(thread_func, numbers, ng_table) ng_table.head(10) starting process starting process starting process starting process starting process .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id author subject organization length text 0 1 None 0 None None None 1 2 None 1 None None None 2 3 None 2 None None None 3 4 None 3 None None None 4 5 None 4 None None None 5 6 None 5 None None None 6 7 None 6 None None None 7 8 None 7 None None None 8 9 None 8 None None None 9 10 None 9 None None None def printer(x, table): print(x, table) with doctable.WorkerPool(3, verbose=False) as p: assert(p.any_alive()) print(f'av efficiency: {p.av_efficiency()}') p.map(printer, list(range(100)), table=ng_table) # test most basic map function #elements = list(range(100)) #assert(pool.map(example_func, elements) == [example_func(e) for e in elements]) print(f'av efficiency: {p.av_efficiency()}') --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) <ipython-input-10-48aa27ce5dc0> in <module> 3 4 with doctable.WorkerPool(3, verbose=False) as p: ----> 5 assert(p.any_alive()) 6 print(f'av efficiency: {p.av_efficiency()}') 7 AssertionError: import pickle pickle.dumps(ng_table.schema_info) import multiprocessing class parse_thread: def __init__(self, table: doctable.DocTable): self.table = table def __call__(self, texts): with self.table as t: #records = [NewsgroupDoc.from_string(text) for text in texts] t.insert(NewsgroupDoc(1000)) def parse_thread2(x): return None chunks = doctable.chunk(newsgroups['data'], chunk_size=100) #parse_func = parse_thread(ng_table) with multiprocessing.Pool(4) as p: %time p.map(parse_thread(ng_table), chunks, 100) #%time map(parse_thread(1), chunks) class process_and_store: table: doctable.DocTable = None def __init__(self, table_cls, *table_args, **table_kwargs): '''Store info to construct the table. ''' self.table_cls = table_cls self.table_args = table_args self.table_kwargs = table_kwargs def connect_db(self): '''Make a new connection to the database and return the associated table. ''' if self.table is None: self.table = self.table_cls(*self.table_args, **self.table_kwargs) return self.table def __call__(self, text): '''Execute function in worker process. ''' table = self.connect_db() record = NewsgroupDoc.from_string(text) table.insert(record) import multiprocessing with multiprocessing.Pool(4) as p: %time p.map(process_and_store(NewsgroupTable, target=table_fname), newsgroups['data']) Notice that this takes very little CPU time, but a long \"wall time\" (overall time it takes to run the program). This is because the threads are IO-starved - they spend a lot of time waiting on each other to commit database transactions. This might be a good opportunity to use variations on threading models, but most parsing classes class process_and_store_chunk(process_and_store): def __call__(self, texts): '''Execute function in worker process. ''' table = self.connect_db() records = [NewsgroupDoc.from_string(text) for text in texts] table.insert(records) chunked_newsgroups = doctable.chunk(newsgroups['data'], chunk_size=500) with multiprocessing.Pool(4) as p: %time p.map(process_and_store_chunk(NewsgroupTable, target=table_fname), chunked_newsgroups) parser = ParsePipeline([ parse_email ]) for email_text in newsgroups['data']: email_data = parse_email(email_text) import multiprocessing with multiprocessing.Pool(10) as p: print(p)","title":"Parsing and storing documents"},{"location":"legacy_documentation/distributed_basics/","text":"Distribute Parallel Processing Basics Due to a number of limitations involving data passed to processes using multiprocessing.Pool() , I've implemented a similar class called Distribute() . The primary difference is that Distribute is meant to distribute chunks of data for parallel processing, so your map function should parse multiple values. There are currently two functions in Distribute: .map_chunk() simply applies a function to a list of elements and returns a list of parsed elements. .map() applies a function to a single element. Same as multiprocessing.Pool().map() . #from IPython import get_ipython import sys sys.path.append('..') import doctable .map() Method Unsurprisingly, this method works exactly like the multiprocessing.Pool().map() . Simply provide a sequence of elements and a function to apply to them, and this method will parse all the elements in parallel. def multiply(x, y=2): return x * y nums = list(range(5)) with doctable.Distribute(3) as d: res = d.map(multiply, nums) print(res) # pass any argument to your function here. # we try multiplying by 5 instead of 2. res = d.map(multiply, nums, 5) print(res) [0, 2, 4, 6, 8] [0, 5, 10, 15, 20] .map_chunk() Method Allows you to write map functions that processes a chunk of your data at a time. This is the lowest-level method for distributed processing. # map function to multiply 1.275 by each num and return a list def multiply_nums(nums): return [num*1.275 for num in nums] # use Distribute(3) to create three separate processes nums = list(range(1000)) with doctable.Distribute(3) as d: res = d.map_chunk(multiply_nums, nums) # won't create new process at all. good for testing with doctable.Distribute(1) as d: res = d.map_chunk(multiply_nums, nums) res[:3] CPU times: user 5.92 ms, sys: 13.3 ms, total: 19.2 ms Wall time: 22.7 ms CPU times: user 63 \u00b5s, sys: 85 \u00b5s, total: 148 \u00b5s Wall time: 154 \u00b5s [0.0, 1.275, 2.55]","title":"Distribute Parallel Processing Basics"},{"location":"legacy_documentation/distributed_basics/#distribute-parallel-processing-basics","text":"Due to a number of limitations involving data passed to processes using multiprocessing.Pool() , I've implemented a similar class called Distribute() . The primary difference is that Distribute is meant to distribute chunks of data for parallel processing, so your map function should parse multiple values. There are currently two functions in Distribute: .map_chunk() simply applies a function to a list of elements and returns a list of parsed elements. .map() applies a function to a single element. Same as multiprocessing.Pool().map() . #from IPython import get_ipython import sys sys.path.append('..') import doctable","title":"Distribute Parallel Processing Basics"},{"location":"legacy_documentation/distributed_basics/#map-method","text":"Unsurprisingly, this method works exactly like the multiprocessing.Pool().map() . Simply provide a sequence of elements and a function to apply to them, and this method will parse all the elements in parallel. def multiply(x, y=2): return x * y nums = list(range(5)) with doctable.Distribute(3) as d: res = d.map(multiply, nums) print(res) # pass any argument to your function here. # we try multiplying by 5 instead of 2. res = d.map(multiply, nums, 5) print(res) [0, 2, 4, 6, 8] [0, 5, 10, 15, 20]","title":".map() Method"},{"location":"legacy_documentation/distributed_basics/#map_chunk-method","text":"Allows you to write map functions that processes a chunk of your data at a time. This is the lowest-level method for distributed processing. # map function to multiply 1.275 by each num and return a list def multiply_nums(nums): return [num*1.275 for num in nums] # use Distribute(3) to create three separate processes nums = list(range(1000)) with doctable.Distribute(3) as d: res = d.map_chunk(multiply_nums, nums) # won't create new process at all. good for testing with doctable.Distribute(1) as d: res = d.map_chunk(multiply_nums, nums) res[:3] CPU times: user 5.92 ms, sys: 13.3 ms, total: 19.2 ms Wall time: 22.7 ms CPU times: user 63 \u00b5s, sys: 85 \u00b5s, total: 148 \u00b5s Wall time: 154 \u00b5s [0.0, 1.275, 2.55]","title":".map_chunk() Method"},{"location":"legacy_documentation/doctable_basics/","text":"DocTable Overview A DocTable acts as an object-oriented interface to a single database table. It combines the utility of dataclasses to create schemas from simple object definitions and sqlalchemy to create connections and execute queries to a database. It should be easy to convert existing data-oriented objects to database schemas, and use those objects when inserting/retrieving data. In this document I'll cover these topics: Creating Schemas Managing Connections Inserting, Deleting, and Selecting Select Queries You may also want to see the vignettes for more examples, the DocTable docs for more information about the class, or the schema guide for more information about creating schemas. I also recommend looking examples for insert, delete , select , and update methods. import random random.seed(0) import pandas as pd import numpy as np from dataclasses import dataclass import sys sys.path.append('..') import doctable 1. Creating a Database Schema DocTable schemas are created using the doctable.schema decorator on a class that uses doctable.Col for defaulted parameters. Check out the schema guide for more detail about schema classes. Our demonstration class will include three columns: id , name , and age , with an additional .is_old property derived from age for example. Note that the id column uses the default value IDCol() which sets the variable to be the primary key and to auto-increment. Arguments passed to the generic Col() function are passed directly to the sqlalchemy metadata to direct column creation. See more in the schema guide . @doctable.schema class Record: __slots__ = [] id: int = doctable.IDCol() name: str = doctable.Col(nullable=False) age: int = doctable.Col() @property def is_old(self): return self.age >= 30 # lol We can instantiate a DocTable by passing a target and schema ( Record in our example) parameters, and I show the resulting schema using .schema_table() . Note that the type hints were used to describe column types, and id was used as the auto-incremented primary key. table = doctable.DocTable(target=':memory:', schema=Record) table.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 name VARCHAR False None auto 0 2 age INTEGER True None auto 0 Probably a more common use case will be to subclass DocTable to provide some basic definitions. class RecordTable(doctable.DocTable): _tabname_ = 'records' _schema_ = Record _indices_ = ( doctable.Index('ind_age', 'age'), ) _constraints_ = ( doctable.Constraint('check', 'age > 0'), ) table = RecordTable(target=':memory:') table <__main__.RecordTable at 0x7f5b40325940> 2. Maintaining Database Connections Obviously a big part of working with databases involves managing connections with the database. By default, DocTable instances DO NOT maintain persistent connections to the database - instead, they open a connection as-needed when executing a query. Benchmark comparisons show that the cost of creating a connection is so low relative to an actual insertion that this probably the approach for most applications. Alternatively, there are several ways of working with connections: as a context manager, using the persistent_conn constructor parameter, manually calling open_conn() and close_conn() (not recommended), and manually requesting a connection to execute using your own sqlalchemy or raw sql library queries. As a context manager. Note that the __enter__ method returns the doctable instance itself, so you can access it using with or without the \"as\" keyword. tab = doctable.DocTable(target=':memory:', schema=Record) print(tab._conn) with tab as t: r = Record(name = 'Devin Cornell', age = 32) print(dir(r)) t.insert_single(Record(name = 'Devin Cornell', age = 32)) print(t._conn) # alternatively, no need to use \"as\" with tab: tab.insert_single(Record(name = 'Devin Cornell', age = 32)) print(tab._conn) tab.head() None ['__annotations__', '__class__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__wrapped__', '_doctable__age', '_doctable__id', '_doctable__name', '_doctable_as_dict', '_doctable_from_db', '_doctable_get_val', 'age', 'as_dict', 'get_val', 'id', 'is_old', 'name'] <sqlalchemy.engine.base.Connection object at 0x7f5b40304550> <sqlalchemy.engine.base.Connection object at 0x7f5b403141c0> /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age 0 1 Devin Cornell 32 1 2 Devin Cornell 32 Using the constructor argument persistent_conn tab = doctable.DocTable(target=':memory:', schema=Record, persistent_conn=False) print(tab._conn) tab = doctable.DocTable(target=':memory:', schema=Record, persistent_conn=True) tab._conn None <sqlalchemy.engine.base.Connection at 0x7f5b402cd280> Manually calling .open_conn() and .close_conn() . I recommend using a context manager if you go this route. tab = doctable.DocTable(target=':memory:', schema=Record) print(tab._conn) tab.open_conn() print(tab._conn) tab.close_conn() print(tab._conn) None <sqlalchemy.engine.base.Connection object at 0x7f5b403097c0> None grabbing a connection object to execute your own sqlalchemy queries conn = tab.connect() conn <sqlalchemy.engine.base.Connection at 0x7f5b40309550> 3. Insert, Delete, and Select The nature of doctable schema definitions means the easiest way to work with database data is often to use the schema class as a normal dataclass. I recommend the schema guide for more detail about the relationship between dataclasses, schema classes, and behavior of the actual database. While the intent behind using dataclasses for database schemas is intuitive and valuable, it can be tricky. NOTE!!!: Unlike ORM-based applications, DocTable instances do not have any connection to instances of the schema class - they are simply used to encapsulate data to be stored and retrieved in the table. This is why the same object can be inserted multiple times in this example. Lets start off by creating some record objects and inserting them into the database with .insert_single() and .insert_many() . In the Record constructor here we do not specify the id value - this is because our database schema dictated that it will be automatically incremented by the database - if we omit the value in the constructor, by default it will simply not pass any value to the database at all (this can be changed later though). See that the results of our call to .head() shows that the rows were given id values upon insertion. table = doctable.DocTable(target=':memory:', schema=Record) o = Record(name='Devin Cornell', age=35) table.insert_single(o, verbose=True) table.insert_single(o) table.insert_many([o, o], verbose=True) table.head() DocTable: INSERT OR FAIL INTO _documents_ (id, name, age) VALUES (?, ?, ?) DocTable: INSERT OR FAIL INTO _documents_ (id, name, age) VALUES (?, ?, ?) /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: .insert_many() is depricated: please use .q.insert_multi() or .q.insert_multi_raw() warnings.warn(f'.insert_many() is depricated: please use .q.insert_multi() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age 0 1 Devin Cornell 35 1 2 Devin Cornell 35 2 3 Devin Cornell 35 3 4 Devin Cornell 35 Now we use .select() to retrieve data from the database. Here we call it with no parameters to simply get all the objects we previously inserted, this time with the id values that the database provided. results = table.select(verbose=True) results DocTable: SELECT _documents_.id, _documents_.name, _documents_.age FROM _documents_ /DataDrive/code/doctable/examples/../doctable/doctable.py:452: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') [Record(id=1, name='Devin Cornell', age=35), Record(id=2, name='Devin Cornell', age=35), Record(id=3, name='Devin Cornell', age=35), Record(id=4, name='Devin Cornell', age=35)] 4. More Complicated Queries And, of course, the most important part of any database library is to execute queries. To do this, DocTable objects keep track of sqlalchemy core MetaData and Table objects and build queries using the select() , delete() , insert() , and update() methods from sqlalchemy core. First, note that subscripting the table object allows you to access the underlying sqlalchemy Column objects, which, as I will show a bit later, can be used to create where conditionals for select and update queries. You can also access specific column data using the .c property of the doctable. table = doctable.DocTable(target=':memory:', schema=Record) table['id'], table.c.id (Column('id', Integer(), table=<_documents_>, primary_key=True, nullable=False), Column('id', Integer(), table=<_documents_>, primary_key=True, nullable=False)) As we'll show later, these column objects also have some operators defined such that they can be used to construct complex queries and functions. You can read more about this in the sqlalchemy operators documentation . table.c.id > 3, table.c.id.in_([1,2]), table.c.age == 4 (<sqlalchemy.sql.elements.BinaryExpression object at 0x7f5b4025cd00>, <sqlalchemy.sql.elements.BinaryExpression object at 0x7f5b4025cc70>, <sqlalchemy.sql.elements.BinaryExpression object at 0x7f5b4025c8e0>) You can use these expressions as part of select() , update() , and delete() operations by passing them to the where argument. table.insert_single(Record(name='Devin Cornell', age=35)) table.insert_single(Record(name='Sam Adams', age=250)) table.insert_single(Record(name='Rando', age=500)) table.select(where=table.c.id >= 3, verbose=True) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age FROM _documents_ WHERE _documents_.id >= ? /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' [Record(id=3, name='Rando', age=500)] table.select_first(where=table.c.name=='Devin Cornell', verbose=True) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age FROM _documents_ WHERE _documents_.name = ? LIMIT ? OFFSET ? /DataDrive/code/doctable/examples/../doctable/doctable.py:427: UserWarning: Method .select_first() is depricated. Please use .q.select_first() instead. warnings.warn('Method .select_first() is depricated. Please use .q.select_first() instead.') Record(id=1, name='Devin Cornell', age=35) Select Statements Now we show how to select data from the table. Use the .count() method to check the number of rows. It also accepts some column conditionals to count entries that satisfy a given criteria table.count(verbose=True), table.count(table['age']>=30, verbose=True) DocTable: SELECT count(_documents_.id) AS count_1 FROM _documents_ LIMIT ? OFFSET ? DocTable: SELECT count(_documents_.id) AS count_1 FROM _documents_ WHERE _documents_.age >= ? LIMIT ? OFFSET ? /DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead. warnings.warn('Method .count() is depricated. Please use .q.count() instead.') (3, 3) Use the .select() method with no arguments to retrieve all rows of the table. You can also choose to select one or more columns to select. table.select(verbose=True) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age FROM _documents_ [Record(id=1, name='Devin Cornell', age=35), Record(id=2, name='Sam Adams', age=250), Record(id=3, name='Rando', age=500)] By specifying a column name, you can retrieve a list of column values, or by offering a list of data, you can request only those datas. table.select('name', verbose=True) DocTable: SELECT _documents_.name FROM _documents_ ['Devin Cornell', 'Sam Adams', 'Rando'] # note we have no access to the ID column - just name, but still part of Record type. table.select(['name'], verbose=True) DocTable: SELECT _documents_.name FROM _documents_ [Record(name='Devin Cornell'), Record(name='Sam Adams'), Record(name='Rando')] Accessing a property which was not retrieved from the database will raise an exception. rec = table.select_first(['name']) try: rec.id except doctable.RowDataNotAvailableError as e: print('This exception was raised:', e) This exception was raised: The \"id\" property is not available. This might happen if you did not retrieve the information from a database or if you did not provide a value in the class constructor. You may also use aggregation functions like .sum . table.select_first(table['age'].sum(), verbose=True) DocTable: SELECT sum(_documents_.age) AS sum_1 FROM _documents_ LIMIT ? OFFSET ? 785 The SUM() and COUNT() SQL functions have been mapped to .sum and .count attributes of columns. Use as_dataclass=False if you do retrieve data which does not fit into a Record object. table.select_first([table['age'].sum(),table['age'].count()], verbose=True) DocTable: SELECT sum(_documents_.age) AS sum_1, count(_documents_.age) AS count_1 FROM _documents_ LIMIT ? OFFSET ? DocTable: SELECT sum(_documents_.age) AS sum_1, count(_documents_.age) AS count_1 FROM _documents_ LIMIT ? OFFSET ? /DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Conversion from row to object failed according to the following error. Please use .q.select_first(..,raw_result=True) next time in the future to avoid this issue. e=RowDataConversionFailed(\"Conversion from <class 'sqlalchemy.engine.row.LegacyRow'> to <class '__main__.Record'> failed.\") warnings.warn(f'Conversion from row to object failed according to the following ' (785, 3) Alternatively, to see the results as a pandas dataframe, we can use .select_df() . table.select_df(verbose=True) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age FROM _documents_ /DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age 0 1 Devin Cornell 35 1 2 Sam Adams 250 2 3 Rando 500 Now we can select specific elements of the db using the where argument of the .select() method. table.select(where=table['age'] >= 1, verbose=True) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age FROM _documents_ WHERE _documents_.age >= ? /DataDrive/code/doctable/examples/../doctable/doctable.py:452: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') [Record(id=1, name='Devin Cornell', age=35), Record(id=2, name='Sam Adams', age=250), Record(id=3, name='Rando', age=500)] table.select(where=table['id']==3, verbose=True) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age FROM _documents_ WHERE _documents_.id = ? [Record(id=3, name='Rando', age=500)] We can update the results in a similar way, using the where argument. table.update({'name':'smartypants'}, where=table['id']==3, verbose=True) table.select() DocTable: UPDATE _documents_ SET name=? WHERE _documents_.id = ? /DataDrive/code/doctable/examples/../doctable/doctable.py:504: UserWarning: Method .update() is depricated. Please use .q.update() instead. warnings.warn('Method .update() is depricated. Please use .q.update() instead.') [Record(id=1, name='Devin Cornell', age=35), Record(id=2, name='Sam Adams', age=250), Record(id=3, name='smartypants', age=500)] print(table['age']*100) table.update({'age':table['age']*100}, verbose=True) table.select() _documents_.age * :age_1 DocTable: UPDATE _documents_ SET age=(_documents_.age * ?) [Record(id=1, name='Devin Cornell', age=3500), Record(id=2, name='Sam Adams', age=25000), Record(id=3, name='smartypants', age=50000)] And we can delete elements using the .delete() method. table.delete(where=table['id']==3, verbose=True) table.select() DocTable: DELETE FROM _documents_ WHERE _documents_.id = ? /DataDrive/code/doctable/examples/../doctable/doctable.py:509: UserWarning: Method .delete() is depricated. Please use .q.delete() instead. warnings.warn('Method .delete() is depricated. Please use .q.delete() instead.') [Record(id=1, name='Devin Cornell', age=3500), Record(id=2, name='Sam Adams', age=25000)] Notes on DB Interface DocTable2 allows you to access columns through direct subscripting, then relies on the power of sqlalchemy column objects to do most of the work of constructing queries. Here are a few notes on their use. For more demonstration, see the example in examples/dt2_select.ipynb # subscript is used to access underlying sqlalchemy column reference (without querying data) table['id'] Column('id', Integer(), table=<_documents_>, primary_key=True, nullable=False) # conditionals are applied directly to the column objects (as we'll see with \"where\" clause) table['id'] < 3 <sqlalchemy.sql.elements.BinaryExpression object at 0x7f5b4022fc10> # can also access using .col() method table.col('id') Column('id', Integer(), table=<_documents_>, primary_key=True, nullable=False) # to access all column objects (only useful for working directly with sql info) table.columns <sqlalchemy.sql.base.ImmutableColumnCollection at 0x7f5b40388db0> # to access more detailed schema information table.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 name VARCHAR False None auto 0 2 age INTEGER True None auto 0 # If needed, you can also access the sqlalchemy table object using the .table property. table.table Table('_documents_', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', Integer(), table=<_documents_>, primary_key=True, nullable=False), Column('name', String(), table=<_documents_>, nullable=False), Column('age', Integer(), table=<_documents_>), schema=None) # the count method is also an easy way to count rows in the database table.count() /DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead. warnings.warn('Method .count() is depricated. Please use .q.count() instead.') 2 # the print method makes it easy to see the table name and total row count print(table) <DocTable (3 cols)::sqlite:///:memory::_documents_>","title":"DocTable Overview"},{"location":"legacy_documentation/doctable_basics/#doctable-overview","text":"A DocTable acts as an object-oriented interface to a single database table. It combines the utility of dataclasses to create schemas from simple object definitions and sqlalchemy to create connections and execute queries to a database. It should be easy to convert existing data-oriented objects to database schemas, and use those objects when inserting/retrieving data. In this document I'll cover these topics: Creating Schemas Managing Connections Inserting, Deleting, and Selecting Select Queries You may also want to see the vignettes for more examples, the DocTable docs for more information about the class, or the schema guide for more information about creating schemas. I also recommend looking examples for insert, delete , select , and update methods. import random random.seed(0) import pandas as pd import numpy as np from dataclasses import dataclass import sys sys.path.append('..') import doctable","title":"DocTable Overview"},{"location":"legacy_documentation/doctable_basics/#1-creating-a-database-schema","text":"DocTable schemas are created using the doctable.schema decorator on a class that uses doctable.Col for defaulted parameters. Check out the schema guide for more detail about schema classes. Our demonstration class will include three columns: id , name , and age , with an additional .is_old property derived from age for example. Note that the id column uses the default value IDCol() which sets the variable to be the primary key and to auto-increment. Arguments passed to the generic Col() function are passed directly to the sqlalchemy metadata to direct column creation. See more in the schema guide . @doctable.schema class Record: __slots__ = [] id: int = doctable.IDCol() name: str = doctable.Col(nullable=False) age: int = doctable.Col() @property def is_old(self): return self.age >= 30 # lol We can instantiate a DocTable by passing a target and schema ( Record in our example) parameters, and I show the resulting schema using .schema_table() . Note that the type hints were used to describe column types, and id was used as the auto-incremented primary key. table = doctable.DocTable(target=':memory:', schema=Record) table.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 name VARCHAR False None auto 0 2 age INTEGER True None auto 0 Probably a more common use case will be to subclass DocTable to provide some basic definitions. class RecordTable(doctable.DocTable): _tabname_ = 'records' _schema_ = Record _indices_ = ( doctable.Index('ind_age', 'age'), ) _constraints_ = ( doctable.Constraint('check', 'age > 0'), ) table = RecordTable(target=':memory:') table <__main__.RecordTable at 0x7f5b40325940>","title":"1. Creating a Database Schema"},{"location":"legacy_documentation/doctable_basics/#2-maintaining-database-connections","text":"Obviously a big part of working with databases involves managing connections with the database. By default, DocTable instances DO NOT maintain persistent connections to the database - instead, they open a connection as-needed when executing a query. Benchmark comparisons show that the cost of creating a connection is so low relative to an actual insertion that this probably the approach for most applications. Alternatively, there are several ways of working with connections: as a context manager, using the persistent_conn constructor parameter, manually calling open_conn() and close_conn() (not recommended), and manually requesting a connection to execute using your own sqlalchemy or raw sql library queries. As a context manager. Note that the __enter__ method returns the doctable instance itself, so you can access it using with or without the \"as\" keyword. tab = doctable.DocTable(target=':memory:', schema=Record) print(tab._conn) with tab as t: r = Record(name = 'Devin Cornell', age = 32) print(dir(r)) t.insert_single(Record(name = 'Devin Cornell', age = 32)) print(t._conn) # alternatively, no need to use \"as\" with tab: tab.insert_single(Record(name = 'Devin Cornell', age = 32)) print(tab._conn) tab.head() None ['__annotations__', '__class__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__wrapped__', '_doctable__age', '_doctable__id', '_doctable__name', '_doctable_as_dict', '_doctable_from_db', '_doctable_get_val', 'age', 'as_dict', 'get_val', 'id', 'is_old', 'name'] <sqlalchemy.engine.base.Connection object at 0x7f5b40304550> <sqlalchemy.engine.base.Connection object at 0x7f5b403141c0> /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age 0 1 Devin Cornell 32 1 2 Devin Cornell 32 Using the constructor argument persistent_conn tab = doctable.DocTable(target=':memory:', schema=Record, persistent_conn=False) print(tab._conn) tab = doctable.DocTable(target=':memory:', schema=Record, persistent_conn=True) tab._conn None <sqlalchemy.engine.base.Connection at 0x7f5b402cd280> Manually calling .open_conn() and .close_conn() . I recommend using a context manager if you go this route. tab = doctable.DocTable(target=':memory:', schema=Record) print(tab._conn) tab.open_conn() print(tab._conn) tab.close_conn() print(tab._conn) None <sqlalchemy.engine.base.Connection object at 0x7f5b403097c0> None grabbing a connection object to execute your own sqlalchemy queries conn = tab.connect() conn <sqlalchemy.engine.base.Connection at 0x7f5b40309550>","title":"2. Maintaining Database Connections"},{"location":"legacy_documentation/doctable_basics/#3-insert-delete-and-select","text":"The nature of doctable schema definitions means the easiest way to work with database data is often to use the schema class as a normal dataclass. I recommend the schema guide for more detail about the relationship between dataclasses, schema classes, and behavior of the actual database. While the intent behind using dataclasses for database schemas is intuitive and valuable, it can be tricky. NOTE!!!: Unlike ORM-based applications, DocTable instances do not have any connection to instances of the schema class - they are simply used to encapsulate data to be stored and retrieved in the table. This is why the same object can be inserted multiple times in this example. Lets start off by creating some record objects and inserting them into the database with .insert_single() and .insert_many() . In the Record constructor here we do not specify the id value - this is because our database schema dictated that it will be automatically incremented by the database - if we omit the value in the constructor, by default it will simply not pass any value to the database at all (this can be changed later though). See that the results of our call to .head() shows that the rows were given id values upon insertion. table = doctable.DocTable(target=':memory:', schema=Record) o = Record(name='Devin Cornell', age=35) table.insert_single(o, verbose=True) table.insert_single(o) table.insert_many([o, o], verbose=True) table.head() DocTable: INSERT OR FAIL INTO _documents_ (id, name, age) VALUES (?, ?, ?) DocTable: INSERT OR FAIL INTO _documents_ (id, name, age) VALUES (?, ?, ?) /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: .insert_many() is depricated: please use .q.insert_multi() or .q.insert_multi_raw() warnings.warn(f'.insert_many() is depricated: please use .q.insert_multi() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age 0 1 Devin Cornell 35 1 2 Devin Cornell 35 2 3 Devin Cornell 35 3 4 Devin Cornell 35 Now we use .select() to retrieve data from the database. Here we call it with no parameters to simply get all the objects we previously inserted, this time with the id values that the database provided. results = table.select(verbose=True) results DocTable: SELECT _documents_.id, _documents_.name, _documents_.age FROM _documents_ /DataDrive/code/doctable/examples/../doctable/doctable.py:452: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') [Record(id=1, name='Devin Cornell', age=35), Record(id=2, name='Devin Cornell', age=35), Record(id=3, name='Devin Cornell', age=35), Record(id=4, name='Devin Cornell', age=35)]","title":"3. Insert, Delete, and Select"},{"location":"legacy_documentation/doctable_basics/#4-more-complicated-queries","text":"And, of course, the most important part of any database library is to execute queries. To do this, DocTable objects keep track of sqlalchemy core MetaData and Table objects and build queries using the select() , delete() , insert() , and update() methods from sqlalchemy core. First, note that subscripting the table object allows you to access the underlying sqlalchemy Column objects, which, as I will show a bit later, can be used to create where conditionals for select and update queries. You can also access specific column data using the .c property of the doctable. table = doctable.DocTable(target=':memory:', schema=Record) table['id'], table.c.id (Column('id', Integer(), table=<_documents_>, primary_key=True, nullable=False), Column('id', Integer(), table=<_documents_>, primary_key=True, nullable=False)) As we'll show later, these column objects also have some operators defined such that they can be used to construct complex queries and functions. You can read more about this in the sqlalchemy operators documentation . table.c.id > 3, table.c.id.in_([1,2]), table.c.age == 4 (<sqlalchemy.sql.elements.BinaryExpression object at 0x7f5b4025cd00>, <sqlalchemy.sql.elements.BinaryExpression object at 0x7f5b4025cc70>, <sqlalchemy.sql.elements.BinaryExpression object at 0x7f5b4025c8e0>) You can use these expressions as part of select() , update() , and delete() operations by passing them to the where argument. table.insert_single(Record(name='Devin Cornell', age=35)) table.insert_single(Record(name='Sam Adams', age=250)) table.insert_single(Record(name='Rando', age=500)) table.select(where=table.c.id >= 3, verbose=True) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age FROM _documents_ WHERE _documents_.id >= ? /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' [Record(id=3, name='Rando', age=500)] table.select_first(where=table.c.name=='Devin Cornell', verbose=True) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age FROM _documents_ WHERE _documents_.name = ? LIMIT ? OFFSET ? /DataDrive/code/doctable/examples/../doctable/doctable.py:427: UserWarning: Method .select_first() is depricated. Please use .q.select_first() instead. warnings.warn('Method .select_first() is depricated. Please use .q.select_first() instead.') Record(id=1, name='Devin Cornell', age=35)","title":"4. More Complicated Queries"},{"location":"legacy_documentation/doctable_basics/#select-statements","text":"Now we show how to select data from the table. Use the .count() method to check the number of rows. It also accepts some column conditionals to count entries that satisfy a given criteria table.count(verbose=True), table.count(table['age']>=30, verbose=True) DocTable: SELECT count(_documents_.id) AS count_1 FROM _documents_ LIMIT ? OFFSET ? DocTable: SELECT count(_documents_.id) AS count_1 FROM _documents_ WHERE _documents_.age >= ? LIMIT ? OFFSET ? /DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead. warnings.warn('Method .count() is depricated. Please use .q.count() instead.') (3, 3) Use the .select() method with no arguments to retrieve all rows of the table. You can also choose to select one or more columns to select. table.select(verbose=True) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age FROM _documents_ [Record(id=1, name='Devin Cornell', age=35), Record(id=2, name='Sam Adams', age=250), Record(id=3, name='Rando', age=500)] By specifying a column name, you can retrieve a list of column values, or by offering a list of data, you can request only those datas. table.select('name', verbose=True) DocTable: SELECT _documents_.name FROM _documents_ ['Devin Cornell', 'Sam Adams', 'Rando'] # note we have no access to the ID column - just name, but still part of Record type. table.select(['name'], verbose=True) DocTable: SELECT _documents_.name FROM _documents_ [Record(name='Devin Cornell'), Record(name='Sam Adams'), Record(name='Rando')] Accessing a property which was not retrieved from the database will raise an exception. rec = table.select_first(['name']) try: rec.id except doctable.RowDataNotAvailableError as e: print('This exception was raised:', e) This exception was raised: The \"id\" property is not available. This might happen if you did not retrieve the information from a database or if you did not provide a value in the class constructor. You may also use aggregation functions like .sum . table.select_first(table['age'].sum(), verbose=True) DocTable: SELECT sum(_documents_.age) AS sum_1 FROM _documents_ LIMIT ? OFFSET ? 785 The SUM() and COUNT() SQL functions have been mapped to .sum and .count attributes of columns. Use as_dataclass=False if you do retrieve data which does not fit into a Record object. table.select_first([table['age'].sum(),table['age'].count()], verbose=True) DocTable: SELECT sum(_documents_.age) AS sum_1, count(_documents_.age) AS count_1 FROM _documents_ LIMIT ? OFFSET ? DocTable: SELECT sum(_documents_.age) AS sum_1, count(_documents_.age) AS count_1 FROM _documents_ LIMIT ? OFFSET ? /DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Conversion from row to object failed according to the following error. Please use .q.select_first(..,raw_result=True) next time in the future to avoid this issue. e=RowDataConversionFailed(\"Conversion from <class 'sqlalchemy.engine.row.LegacyRow'> to <class '__main__.Record'> failed.\") warnings.warn(f'Conversion from row to object failed according to the following ' (785, 3) Alternatively, to see the results as a pandas dataframe, we can use .select_df() . table.select_df(verbose=True) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age FROM _documents_ /DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age 0 1 Devin Cornell 35 1 2 Sam Adams 250 2 3 Rando 500 Now we can select specific elements of the db using the where argument of the .select() method. table.select(where=table['age'] >= 1, verbose=True) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age FROM _documents_ WHERE _documents_.age >= ? /DataDrive/code/doctable/examples/../doctable/doctable.py:452: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') [Record(id=1, name='Devin Cornell', age=35), Record(id=2, name='Sam Adams', age=250), Record(id=3, name='Rando', age=500)] table.select(where=table['id']==3, verbose=True) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age FROM _documents_ WHERE _documents_.id = ? [Record(id=3, name='Rando', age=500)] We can update the results in a similar way, using the where argument. table.update({'name':'smartypants'}, where=table['id']==3, verbose=True) table.select() DocTable: UPDATE _documents_ SET name=? WHERE _documents_.id = ? /DataDrive/code/doctable/examples/../doctable/doctable.py:504: UserWarning: Method .update() is depricated. Please use .q.update() instead. warnings.warn('Method .update() is depricated. Please use .q.update() instead.') [Record(id=1, name='Devin Cornell', age=35), Record(id=2, name='Sam Adams', age=250), Record(id=3, name='smartypants', age=500)] print(table['age']*100) table.update({'age':table['age']*100}, verbose=True) table.select() _documents_.age * :age_1 DocTable: UPDATE _documents_ SET age=(_documents_.age * ?) [Record(id=1, name='Devin Cornell', age=3500), Record(id=2, name='Sam Adams', age=25000), Record(id=3, name='smartypants', age=50000)] And we can delete elements using the .delete() method. table.delete(where=table['id']==3, verbose=True) table.select() DocTable: DELETE FROM _documents_ WHERE _documents_.id = ? /DataDrive/code/doctable/examples/../doctable/doctable.py:509: UserWarning: Method .delete() is depricated. Please use .q.delete() instead. warnings.warn('Method .delete() is depricated. Please use .q.delete() instead.') [Record(id=1, name='Devin Cornell', age=3500), Record(id=2, name='Sam Adams', age=25000)]","title":"Select Statements"},{"location":"legacy_documentation/doctable_basics/#notes-on-db-interface","text":"DocTable2 allows you to access columns through direct subscripting, then relies on the power of sqlalchemy column objects to do most of the work of constructing queries. Here are a few notes on their use. For more demonstration, see the example in examples/dt2_select.ipynb # subscript is used to access underlying sqlalchemy column reference (without querying data) table['id'] Column('id', Integer(), table=<_documents_>, primary_key=True, nullable=False) # conditionals are applied directly to the column objects (as we'll see with \"where\" clause) table['id'] < 3 <sqlalchemy.sql.elements.BinaryExpression object at 0x7f5b4022fc10> # can also access using .col() method table.col('id') Column('id', Integer(), table=<_documents_>, primary_key=True, nullable=False) # to access all column objects (only useful for working directly with sql info) table.columns <sqlalchemy.sql.base.ImmutableColumnCollection at 0x7f5b40388db0> # to access more detailed schema information table.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 name VARCHAR False None auto 0 2 age INTEGER True None auto 0 # If needed, you can also access the sqlalchemy table object using the .table property. table.table Table('_documents_', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', Integer(), table=<_documents_>, primary_key=True, nullable=False), Column('name', String(), table=<_documents_>, nullable=False), Column('age', Integer(), table=<_documents_>), schema=None) # the count method is also an easy way to count rows in the database table.count() /DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead. warnings.warn('Method .count() is depricated. Please use .q.count() instead.') 2 # the print method makes it easy to see the table name and total row count print(table) <DocTable (3 cols)::sqlite:///:memory::_documents_>","title":"Notes on DB Interface"},{"location":"legacy_documentation/doctable_bootstrap/","text":"Document Bootstrapping Examples When estimating machine learning or statistical models on your corpus, you may need to bootstrap documents (randomly sample with replacement). The .bootstrap() method of DocTable will act like a select statement but return a bootstrap object instead of a direct query result. Here I show how to do some basic bootstrapping using an example doctable. import random import pandas as pd import numpy as np import sys sys.path.append('..') import doctable as dt Create Example DocTable First we define a DocTable that will be used for examples. schema = ( ('integer','id',dict(primary_key=True, autoincrement=True)), ('string','name', dict(nullable=False, unique=True)), ('integer','age'), ('boolean', 'is_old'), ) db = dt.DocTable(target=':memory:', schema=schema) print(db) <DocTable::sqlite:///:memory::_documents_ ct: 0> Then we add several example rows to the doctable. for i in range(10): age = random.random() # number in [0,1] is_old = age > 0.5 row = {'name':'user_'+str(i), 'age':age, 'is_old':is_old} db.insert(row, ifnotunique='replace') for doc in db.select(limit=3): print(doc) (1, 'user_0', 0.16086747483303065, False) (2, 'user_1', 0.14322051505126332, False) (3, 'user_2', 0.22664393988892395, False) Create a Bootstrap We can use the doctable method .bootstrap() to return a bootstrap object using the keyword argument n to set the sample size (will use number of docs by default). This method acts like a select query, so we can specify columns and use the where argument to choose columns and rows to be bootstrapped. The bootsrap object contains the rows in the .doc property. Notice that while our select statement drew three documens, the sample size specified with n is 5. The boostrap object will always return 5 objects, even though the number of docs stays the same. bs = db.bootstrap(['name','age'], where=db['id'] % 3 == 0, n=4) print(type(bs)) print(len(bs.docs)) bs.n <class 'doctable.bootstrap.DocBootstrap'> 3 4 Use the bootstrap object as an iterator to access the bootstrapped docs. The bootstrap object draws a sample upon instantiation, so the same sample is maintained until reset. print('first run:') for doc in bs: print(doc) print('second run:') for doc in bs: print(doc) first run: ('user_5', 0.6473182290263347) ('user_2', 0.22664393988892395) ('user_2', 0.22664393988892395) ('user_5', 0.6473182290263347) second run: ('user_5', 0.6473182290263347) ('user_2', 0.22664393988892395) ('user_2', 0.22664393988892395) ('user_5', 0.6473182290263347) Draw New Sample You can reset the internal sample of the bootstrap object using the .set_new_sample() method. See that we now sample 2 docs and the output is different from previous runs. The sample will still remain the same each time we iterate until we reset the sample. bs.set_new_sample(2) print('first run:') for doc in bs: print(doc) print('second run:') for doc in bs: print(doc) first run: ('user_5', 0.6473182290263347) ('user_8', 0.5270190808172914) second run: ('user_5', 0.6473182290263347) ('user_8', 0.5270190808172914) And we can iterate through a new sample using .new_sample() . Equivalent to calling .set_new_sample() and then iterating through elements. print('drawing new sample:') for doc in bs.new_sample(3): print(doc) print('repeating sample:') for doc in bs: print(doc) drawing new sample: ('user_5', 0.6473182290263347) ('user_5', 0.6473182290263347) ('user_8', 0.5270190808172914) repeating sample: ('user_5', 0.6473182290263347) ('user_5', 0.6473182290263347) ('user_8', 0.5270190808172914) I may add additional functionality in the future if I use this in any projects, but that's it for now.","title":"Document Bootstrapping Examples"},{"location":"legacy_documentation/doctable_bootstrap/#document-bootstrapping-examples","text":"When estimating machine learning or statistical models on your corpus, you may need to bootstrap documents (randomly sample with replacement). The .bootstrap() method of DocTable will act like a select statement but return a bootstrap object instead of a direct query result. Here I show how to do some basic bootstrapping using an example doctable. import random import pandas as pd import numpy as np import sys sys.path.append('..') import doctable as dt","title":"Document Bootstrapping Examples"},{"location":"legacy_documentation/doctable_bootstrap/#create-example-doctable","text":"First we define a DocTable that will be used for examples. schema = ( ('integer','id',dict(primary_key=True, autoincrement=True)), ('string','name', dict(nullable=False, unique=True)), ('integer','age'), ('boolean', 'is_old'), ) db = dt.DocTable(target=':memory:', schema=schema) print(db) <DocTable::sqlite:///:memory::_documents_ ct: 0> Then we add several example rows to the doctable. for i in range(10): age = random.random() # number in [0,1] is_old = age > 0.5 row = {'name':'user_'+str(i), 'age':age, 'is_old':is_old} db.insert(row, ifnotunique='replace') for doc in db.select(limit=3): print(doc) (1, 'user_0', 0.16086747483303065, False) (2, 'user_1', 0.14322051505126332, False) (3, 'user_2', 0.22664393988892395, False)","title":"Create Example DocTable"},{"location":"legacy_documentation/doctable_bootstrap/#create-a-bootstrap","text":"We can use the doctable method .bootstrap() to return a bootstrap object using the keyword argument n to set the sample size (will use number of docs by default). This method acts like a select query, so we can specify columns and use the where argument to choose columns and rows to be bootstrapped. The bootsrap object contains the rows in the .doc property. Notice that while our select statement drew three documens, the sample size specified with n is 5. The boostrap object will always return 5 objects, even though the number of docs stays the same. bs = db.bootstrap(['name','age'], where=db['id'] % 3 == 0, n=4) print(type(bs)) print(len(bs.docs)) bs.n <class 'doctable.bootstrap.DocBootstrap'> 3 4 Use the bootstrap object as an iterator to access the bootstrapped docs. The bootstrap object draws a sample upon instantiation, so the same sample is maintained until reset. print('first run:') for doc in bs: print(doc) print('second run:') for doc in bs: print(doc) first run: ('user_5', 0.6473182290263347) ('user_2', 0.22664393988892395) ('user_2', 0.22664393988892395) ('user_5', 0.6473182290263347) second run: ('user_5', 0.6473182290263347) ('user_2', 0.22664393988892395) ('user_2', 0.22664393988892395) ('user_5', 0.6473182290263347)","title":"Create a Bootstrap"},{"location":"legacy_documentation/doctable_bootstrap/#draw-new-sample","text":"You can reset the internal sample of the bootstrap object using the .set_new_sample() method. See that we now sample 2 docs and the output is different from previous runs. The sample will still remain the same each time we iterate until we reset the sample. bs.set_new_sample(2) print('first run:') for doc in bs: print(doc) print('second run:') for doc in bs: print(doc) first run: ('user_5', 0.6473182290263347) ('user_8', 0.5270190808172914) second run: ('user_5', 0.6473182290263347) ('user_8', 0.5270190808172914) And we can iterate through a new sample using .new_sample() . Equivalent to calling .set_new_sample() and then iterating through elements. print('drawing new sample:') for doc in bs.new_sample(3): print(doc) print('repeating sample:') for doc in bs: print(doc) drawing new sample: ('user_5', 0.6473182290263347) ('user_5', 0.6473182290263347) ('user_8', 0.5270190808172914) repeating sample: ('user_5', 0.6473182290263347) ('user_5', 0.6473182290263347) ('user_8', 0.5270190808172914) I may add additional functionality in the future if I use this in any projects, but that's it for now.","title":"Draw New Sample"},{"location":"legacy_documentation/doctable_concurrency/","text":"Concurrent Database Connections DocTable makes it easy to establish concurrent database connections from different processes. DocTable objects can be copied as-is from one process to another, except that you must call .reopen_engine() to initialize in process thread. This removes now stale database connections (which are not meant to traverse processes) from the engine connection pool. You may also want to use a large timeout using the timeout argument of the doctable constructor (provided in seconds). import sqlalchemy from multiprocessing import Process import os import random import string import dataclasses import time import sys import pathlib sys.path.append('..') import doctable import datetime @doctable.schema class SimpleRow: __slots__ = [] id: int = doctable.IDCol() updated: datetime.datetime = doctable.AddedCol() process: str = doctable.Col() number: int = doctable.Col() #tmp = doctable.TempFolder('exdb') import tempfile tmpf = tempfile.TemporaryDirectory() tmp = pathlib.Path(tmpf.name) db = doctable.DocTable(schema=SimpleRow, target=tmp.joinpath('tmp_concurrent.db'), new_db=True, connect_args={'timeout': 15}) def thread_func(numbers, db): process_id = ''.join(random.choices(string.ascii_uppercase, k=2)) print(f'starting process {process_id}') db.reopen_engine() # create all new connections for num in numbers: db.insert({'process': process_id, 'number': num}) time.sleep(0.01) numbers = list(range(100)) # these numbers are to be inserted into the database db.delete() with doctable.Distribute(5) as d: d.map_chunk(thread_func, numbers, db) db.head(10) /DataDrive/code/doctable/examples/../doctable/doctable.py:506: UserWarning: Method .delete() is depricated. Please use .q.delete() instead. warnings.warn('Method .delete() is depricated. Please use .q.delete() instead.') starting process OC starting process BI /DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' starting process FF /DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' starting process PC /DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' starting process YC /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 10 0 10","title":"Concurrent Database Connections"},{"location":"legacy_documentation/doctable_concurrency/#concurrent-database-connections","text":"DocTable makes it easy to establish concurrent database connections from different processes. DocTable objects can be copied as-is from one process to another, except that you must call .reopen_engine() to initialize in process thread. This removes now stale database connections (which are not meant to traverse processes) from the engine connection pool. You may also want to use a large timeout using the timeout argument of the doctable constructor (provided in seconds). import sqlalchemy from multiprocessing import Process import os import random import string import dataclasses import time import sys import pathlib sys.path.append('..') import doctable import datetime @doctable.schema class SimpleRow: __slots__ = [] id: int = doctable.IDCol() updated: datetime.datetime = doctable.AddedCol() process: str = doctable.Col() number: int = doctable.Col() #tmp = doctable.TempFolder('exdb') import tempfile tmpf = tempfile.TemporaryDirectory() tmp = pathlib.Path(tmpf.name) db = doctable.DocTable(schema=SimpleRow, target=tmp.joinpath('tmp_concurrent.db'), new_db=True, connect_args={'timeout': 15}) def thread_func(numbers, db): process_id = ''.join(random.choices(string.ascii_uppercase, k=2)) print(f'starting process {process_id}') db.reopen_engine() # create all new connections for num in numbers: db.insert({'process': process_id, 'number': num}) time.sleep(0.01) numbers = list(range(100)) # these numbers are to be inserted into the database db.delete() with doctable.Distribute(5) as d: d.map_chunk(thread_func, numbers, db) db.head(10) /DataDrive/code/doctable/examples/../doctable/doctable.py:506: UserWarning: Method .delete() is depricated. Please use .q.delete() instead. warnings.warn('Method .delete() is depricated. Please use .q.delete() instead.') starting process OC starting process BI /DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' starting process FF /DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' starting process PC /DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' starting process YC /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 10 0 10","title":"Concurrent Database Connections"},{"location":"legacy_documentation/doctable_connectengine/","text":"Manage SQL Connections with DocTable This is meant to give a bit more depth describing how doctable works under-the-hood. I won't cover the details of DocTable methods or working with doctable objects, but I will try to give a clearer sense of how connections and tables are managed within a doctable instance. The driving motivator behind doctable is to create an object-oriented interface for working with sql tables by linking schemas described in your code with the structure of the databases you work with. This model is less ideal for the kinds of application-based frameworks where you would define the database schema once and build code around it separately, but works well for data science applications where you will be creating new tables and playing with different schemas regularly as your approach and end-goals change. When you instantiate a DocTable (or inheriting class), the object will convert your provided schema into a set of sqlalchemy objects which are then stored in-memory as part of the doctable instance. If the table does not already exist in the actual database, DocTable can create one that matches the provided schema, and then the schema will be used to work with the underlying database table. I will now discuss the lower-level objects that manage the metadata and connections to the database. import sys sys.path.append('..') import doctable ConnectEngine Class Each doctable maintains a ConnectEngine object to manage database connections and metadata that make all other database operations possible. I'll demonstrate how to instantiate this class manually to show how it works. The constructor takes arguments for dialect (sqlite, mysql, etc) and database target (filename or database server) to create new sqlalchemy engine and metadata objects. The engine object stores information about the target and can generate database connections, the metadata object stores schemas for registered tables. To work with a table, the metadata object must have the table schema registered, although it can be constructed from the database object itself. See here that the constructor requires a target (file or server where the database is located) and a dialect (flavor of database engine). This connection sits above individual table connections, and thus maintains no connections of it's own - only the engine that can create connections. We can, however list the tables in the database and perform other operations on the table. engine = doctable.ConnectEngine(target=':memory:', dialect='sqlite') engine sqlite:///:memory: Working with tables You can also execute connectionless queries directly from this object, although normally you would create a connection object first and then execute queries from the connection. In this example I use a custom sql query to create a new table. As the ConnectEngine sits above the level of tables, we can list and drop tables from here. # see there are no tables here yet. engine.list_tables() [] # run this raw sql query just for example # NOTE: Normally you would NOT create a table this way using doctable. # This is just for example purposes. query = 'CREATE TABLE temp (id INTEGER PRIMARY KEY, number INTEGER NOT NULL)' engine.execute(query) <sqlalchemy.engine.result.ResultProxy at 0x7f62a10e9370> # see that the table is now in the database engine.list_tables() ['temp'] # uses inspect to ask the database directly for the schema engine.schema('temp') [{'name': 'id', 'type': INTEGER(), 'nullable': True, 'default': None, 'autoincrement': 'auto', 'primary_key': 1}, {'name': 'number', 'type': INTEGER(), 'nullable': False, 'default': None, 'autoincrement': 'auto', 'primary_key': 0}] # or as a dataframe engine.schema_df('temp') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER True None auto 1 1 number INTEGER False None auto 0 All of these methods I've shown so far access the database tables directly, but currently our python objects do not have any idea of what the table schema looks like. You can view the sqlalchemy table objects actually registered with the engine by using the .tables property. See that it is currently empty! Our python code is not able to work with the table using objects because it does not have record of the schema. Now we'll show how to register tables with the engine. Creating and accessing tables To create a data structure internally representing the database structure, we can either ask sqlalchemy to read the database and create the schema, or we can provide lists of sqlalchemy column objects. Wee that we can access the registered tables using the .tables property. # see that currently our engine does not have information about the table we created above. engine.tables immutabledict({}) # now I ask doctable to read the database schema and register the table in metadata. engine.add_table('temp') Table('temp', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=<temp>, primary_key=True, nullable=False), Column('number', INTEGER(), table=<temp>, nullable=False), schema=None) # and we can see that the table is registered engine.tables immutabledict({'temp': Table('temp', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=<temp>, primary_key=True, nullable=False), Column('number', INTEGER(), table=<temp>, nullable=False), schema=None)}) When add_table() is called, a new sqlalchemy.Table object is registered in the engine's metadata and returned. If add_table() is called again, it will return the table already registered in the metadata. Because we usually use doctable to manage tables, we'll just show a short example here. # while we can use doctable to do most of this work # usually, I'll just show how sqlalchemy core objects # can be used to create a table in ConnectEngine. from sqlalchemy import Column, Integer, String # create a list of columns columns = ( Column('id', Integer, primary_key = True), Column('name', String), ) # we similarly use the add_table() method to store the schema # in the metadata engine.add_table('temp2', columns=columns) Table('temp2', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', Integer(), table=<temp2>, primary_key=True, nullable=False), Column('name', String(), table=<temp2>), schema=None) # see now that the engine has information about both tables engine.tables immutabledict({'temp': Table('temp', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=<temp>, primary_key=True, nullable=False), Column('number', INTEGER(), table=<temp>, nullable=False), schema=None), 'temp2': Table('temp2', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', Integer(), table=<temp2>, primary_key=True, nullable=False), Column('name', String(), table=<temp2>), schema=None)}) # and see that you can get individual table object references like this engine.tables['temp'] Table('temp', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=<temp>, primary_key=True, nullable=False), Column('number', INTEGER(), table=<temp>, nullable=False), schema=None) Dropping tables Dropping tables is simple enough, but remember that the schema stored in the database and the objects in code mirror each other, so it is best to manipulate them at the same time. Use .drop_table instead of issuing CREATE TABLE query to make sure they stay in sync. The method can also be used on tables that are not in the metadata engine. # by providing the argument as a string engine.drop_table('temp') engine.list_tables() ['temp2'] In cases where an underlying table has been deleted but metadata is retained, the drop_table() method will still work but you may need to call clear_metadata() to flush all metadata and add_all_tables() to re-create the metadata from the actual data. # see this works although the temp3 table is not registered in engine metadata query = 'CREATE TABLE temp3 (id INTEGER PRIMARY KEY, number INTEGER NOT NULL)' engine.execute(query) engine.drop_table('temp3') # this will delete the underlying table even though the metadata information still exists. query = 'CREATE TABLE temp4 (id INTEGER PRIMARY KEY, number INTEGER NOT NULL)' engine.execute(query) engine.execute(f'DROP TABLE IF EXISTS temp4') engine.list_tables() ['temp2'] # see that the table is still registered in the metadata engine.tables immutabledict({'temp': Table('temp', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=<temp>, primary_key=True, nullable=False), Column('number', INTEGER(), table=<temp>, nullable=False), schema=None), 'temp2': Table('temp2', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', Integer(), table=<temp2>, primary_key=True, nullable=False), Column('name', String(), table=<temp2>), schema=None), 'temp3': Table('temp3', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=<temp3>, primary_key=True, nullable=False), Column('number', INTEGER(), table=<temp3>, nullable=False), schema=None)}) # in this case, it might be simplest just to clear all metadata # and re-build according to exising tables engine.clear_metadata() engine.reflect() engine.tables immutabledict({'temp2': Table('temp2', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=<temp2>, primary_key=True, nullable=False), Column('name', VARCHAR(), table=<temp2>), schema=None)}) Managing connections with ConnectEngine ConnectEngine objects are used to create database connections which are maintained by individual doctable objects. Use the get_connection() function to retreive a new connection object which you can use to execute queries. While garbage collecting the connection objects will close the individual connection, sometimes all connections need to be closed simultaneously. This is especially important because garbage-collecting the ConnectEngine object doesn't mean the connections will be garbage-collected if they have references elsewhere in your code. You can close all connections using the close_connections() method. # make new connection conn = engine.connect() conn <sqlalchemy.engine.base.Connection at 0x7f62a10301f0> # see here we just run a select query on the empty table, returning an empty list list(conn.execute('SELECT * FROM temp2')) [] An important use-case of this feature is when you have multiple processes accessing the same database. In general, each process should have separate connections to the database, but both the engine and metadata stored with the ConnectEngine can be copied. Here I'll show a basic multiprocessing case using the Distribute class (it works much like multiprocessing.Pool()). In using the map function we open two processes, and in the thread function we call the close_connections() method to delete existing connections which don't exist in this new memory space. def thread(nums, engine: doctable.ConnectEngine): # close connections that were opened in other thread #engine.close_connections() engine.dispose() # create a new connection for this thread thread_conn = engine.connect() numbers = [1,2] with doctable.Distribute(2) as d: d.map(thread, numbers, engine) engine.list_tables() ['temp2'] DocTable and ConnectEngine Every DocTable object maintains a ConnectEngine to store information about the table they represent, and can be accessed through the engine property. When a target and dialect are provided to doctable, it will automatically initialize a new ConnectEngine and store a new connection object. # create a new doctable and view it's engine schema = (('idcol', 'id'), ('string', 'name')) db = doctable.DocTable(target=':memory:', schema=schema) str(db.engine) '<ConnectEngine::sqlite:///:memory:>' The DocTable constructor can also accept an engine in place of a target and dialect, and thus share ConnectEngines between multiple DocTable objects. In this case, the doctable constructor will use the provided schema to insert the table information into the engine metadata and create the table if doesn't already exist. It will also generate a new connection object from the ConnectEngine. # a w engine.clear_metadata() print(engine.tables.keys()) print(engine.list_tables()) dict_keys([]) ['temp2'] # make a new doctable using the existing engine schema = (('idcol', 'id'), ('string', 'name')) db = doctable.DocTable(engine=engine, schema=schema, tabname='tmp5') db <doctable.doctable.DocTable at 0x7f62a1096b20> # make another doctable using existing engine schema2 = (('idcol', 'id'), ('string', 'name')) db2 = doctable.DocTable(engine=engine, schema=schema2, tabname='tmp6') db2 <doctable.doctable.DocTable at 0x7f62a0fd4cd0> # we can see that both tables have been created in the database engine.list_tables() ['temp2', 'tmp5', 'tmp6'] # and that both are registered in the metadata engine.tables.keys() dict_keys(['tmp5', 'tmp6']) Some ConnectEngine methods are also accessable through the DocTable instances. db.list_tables() ['temp2', 'tmp5', 'tmp6'] db.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 name VARCHAR True None auto 0 # and this is equivalent to calling the engine method reopen(), which clears # metadata and closes connection pool db.reopen_engine()","title":"Manage SQL Connections with DocTable"},{"location":"legacy_documentation/doctable_connectengine/#manage-sql-connections-with-doctable","text":"This is meant to give a bit more depth describing how doctable works under-the-hood. I won't cover the details of DocTable methods or working with doctable objects, but I will try to give a clearer sense of how connections and tables are managed within a doctable instance. The driving motivator behind doctable is to create an object-oriented interface for working with sql tables by linking schemas described in your code with the structure of the databases you work with. This model is less ideal for the kinds of application-based frameworks where you would define the database schema once and build code around it separately, but works well for data science applications where you will be creating new tables and playing with different schemas regularly as your approach and end-goals change. When you instantiate a DocTable (or inheriting class), the object will convert your provided schema into a set of sqlalchemy objects which are then stored in-memory as part of the doctable instance. If the table does not already exist in the actual database, DocTable can create one that matches the provided schema, and then the schema will be used to work with the underlying database table. I will now discuss the lower-level objects that manage the metadata and connections to the database. import sys sys.path.append('..') import doctable","title":"Manage SQL Connections with DocTable"},{"location":"legacy_documentation/doctable_connectengine/#connectengine-class","text":"Each doctable maintains a ConnectEngine object to manage database connections and metadata that make all other database operations possible. I'll demonstrate how to instantiate this class manually to show how it works. The constructor takes arguments for dialect (sqlite, mysql, etc) and database target (filename or database server) to create new sqlalchemy engine and metadata objects. The engine object stores information about the target and can generate database connections, the metadata object stores schemas for registered tables. To work with a table, the metadata object must have the table schema registered, although it can be constructed from the database object itself. See here that the constructor requires a target (file or server where the database is located) and a dialect (flavor of database engine). This connection sits above individual table connections, and thus maintains no connections of it's own - only the engine that can create connections. We can, however list the tables in the database and perform other operations on the table. engine = doctable.ConnectEngine(target=':memory:', dialect='sqlite') engine sqlite:///:memory:","title":"ConnectEngine Class"},{"location":"legacy_documentation/doctable_connectengine/#working-with-tables","text":"You can also execute connectionless queries directly from this object, although normally you would create a connection object first and then execute queries from the connection. In this example I use a custom sql query to create a new table. As the ConnectEngine sits above the level of tables, we can list and drop tables from here. # see there are no tables here yet. engine.list_tables() [] # run this raw sql query just for example # NOTE: Normally you would NOT create a table this way using doctable. # This is just for example purposes. query = 'CREATE TABLE temp (id INTEGER PRIMARY KEY, number INTEGER NOT NULL)' engine.execute(query) <sqlalchemy.engine.result.ResultProxy at 0x7f62a10e9370> # see that the table is now in the database engine.list_tables() ['temp'] # uses inspect to ask the database directly for the schema engine.schema('temp') [{'name': 'id', 'type': INTEGER(), 'nullable': True, 'default': None, 'autoincrement': 'auto', 'primary_key': 1}, {'name': 'number', 'type': INTEGER(), 'nullable': False, 'default': None, 'autoincrement': 'auto', 'primary_key': 0}] # or as a dataframe engine.schema_df('temp') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER True None auto 1 1 number INTEGER False None auto 0 All of these methods I've shown so far access the database tables directly, but currently our python objects do not have any idea of what the table schema looks like. You can view the sqlalchemy table objects actually registered with the engine by using the .tables property. See that it is currently empty! Our python code is not able to work with the table using objects because it does not have record of the schema. Now we'll show how to register tables with the engine.","title":"Working with tables"},{"location":"legacy_documentation/doctable_connectengine/#creating-and-accessing-tables","text":"To create a data structure internally representing the database structure, we can either ask sqlalchemy to read the database and create the schema, or we can provide lists of sqlalchemy column objects. Wee that we can access the registered tables using the .tables property. # see that currently our engine does not have information about the table we created above. engine.tables immutabledict({}) # now I ask doctable to read the database schema and register the table in metadata. engine.add_table('temp') Table('temp', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=<temp>, primary_key=True, nullable=False), Column('number', INTEGER(), table=<temp>, nullable=False), schema=None) # and we can see that the table is registered engine.tables immutabledict({'temp': Table('temp', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=<temp>, primary_key=True, nullable=False), Column('number', INTEGER(), table=<temp>, nullable=False), schema=None)}) When add_table() is called, a new sqlalchemy.Table object is registered in the engine's metadata and returned. If add_table() is called again, it will return the table already registered in the metadata. Because we usually use doctable to manage tables, we'll just show a short example here. # while we can use doctable to do most of this work # usually, I'll just show how sqlalchemy core objects # can be used to create a table in ConnectEngine. from sqlalchemy import Column, Integer, String # create a list of columns columns = ( Column('id', Integer, primary_key = True), Column('name', String), ) # we similarly use the add_table() method to store the schema # in the metadata engine.add_table('temp2', columns=columns) Table('temp2', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', Integer(), table=<temp2>, primary_key=True, nullable=False), Column('name', String(), table=<temp2>), schema=None) # see now that the engine has information about both tables engine.tables immutabledict({'temp': Table('temp', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=<temp>, primary_key=True, nullable=False), Column('number', INTEGER(), table=<temp>, nullable=False), schema=None), 'temp2': Table('temp2', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', Integer(), table=<temp2>, primary_key=True, nullable=False), Column('name', String(), table=<temp2>), schema=None)}) # and see that you can get individual table object references like this engine.tables['temp'] Table('temp', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=<temp>, primary_key=True, nullable=False), Column('number', INTEGER(), table=<temp>, nullable=False), schema=None)","title":"Creating and accessing tables"},{"location":"legacy_documentation/doctable_connectengine/#dropping-tables","text":"Dropping tables is simple enough, but remember that the schema stored in the database and the objects in code mirror each other, so it is best to manipulate them at the same time. Use .drop_table instead of issuing CREATE TABLE query to make sure they stay in sync. The method can also be used on tables that are not in the metadata engine. # by providing the argument as a string engine.drop_table('temp') engine.list_tables() ['temp2'] In cases where an underlying table has been deleted but metadata is retained, the drop_table() method will still work but you may need to call clear_metadata() to flush all metadata and add_all_tables() to re-create the metadata from the actual data. # see this works although the temp3 table is not registered in engine metadata query = 'CREATE TABLE temp3 (id INTEGER PRIMARY KEY, number INTEGER NOT NULL)' engine.execute(query) engine.drop_table('temp3') # this will delete the underlying table even though the metadata information still exists. query = 'CREATE TABLE temp4 (id INTEGER PRIMARY KEY, number INTEGER NOT NULL)' engine.execute(query) engine.execute(f'DROP TABLE IF EXISTS temp4') engine.list_tables() ['temp2'] # see that the table is still registered in the metadata engine.tables immutabledict({'temp': Table('temp', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=<temp>, primary_key=True, nullable=False), Column('number', INTEGER(), table=<temp>, nullable=False), schema=None), 'temp2': Table('temp2', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', Integer(), table=<temp2>, primary_key=True, nullable=False), Column('name', String(), table=<temp2>), schema=None), 'temp3': Table('temp3', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=<temp3>, primary_key=True, nullable=False), Column('number', INTEGER(), table=<temp3>, nullable=False), schema=None)}) # in this case, it might be simplest just to clear all metadata # and re-build according to exising tables engine.clear_metadata() engine.reflect() engine.tables immutabledict({'temp2': Table('temp2', MetaData(bind=Engine(sqlite:///:memory:)), Column('id', INTEGER(), table=<temp2>, primary_key=True, nullable=False), Column('name', VARCHAR(), table=<temp2>), schema=None)})","title":"Dropping tables"},{"location":"legacy_documentation/doctable_connectengine/#managing-connections-with-connectengine","text":"ConnectEngine objects are used to create database connections which are maintained by individual doctable objects. Use the get_connection() function to retreive a new connection object which you can use to execute queries. While garbage collecting the connection objects will close the individual connection, sometimes all connections need to be closed simultaneously. This is especially important because garbage-collecting the ConnectEngine object doesn't mean the connections will be garbage-collected if they have references elsewhere in your code. You can close all connections using the close_connections() method. # make new connection conn = engine.connect() conn <sqlalchemy.engine.base.Connection at 0x7f62a10301f0> # see here we just run a select query on the empty table, returning an empty list list(conn.execute('SELECT * FROM temp2')) [] An important use-case of this feature is when you have multiple processes accessing the same database. In general, each process should have separate connections to the database, but both the engine and metadata stored with the ConnectEngine can be copied. Here I'll show a basic multiprocessing case using the Distribute class (it works much like multiprocessing.Pool()). In using the map function we open two processes, and in the thread function we call the close_connections() method to delete existing connections which don't exist in this new memory space. def thread(nums, engine: doctable.ConnectEngine): # close connections that were opened in other thread #engine.close_connections() engine.dispose() # create a new connection for this thread thread_conn = engine.connect() numbers = [1,2] with doctable.Distribute(2) as d: d.map(thread, numbers, engine) engine.list_tables() ['temp2']","title":"Managing connections with ConnectEngine"},{"location":"legacy_documentation/doctable_connectengine/#doctable-and-connectengine","text":"Every DocTable object maintains a ConnectEngine to store information about the table they represent, and can be accessed through the engine property. When a target and dialect are provided to doctable, it will automatically initialize a new ConnectEngine and store a new connection object. # create a new doctable and view it's engine schema = (('idcol', 'id'), ('string', 'name')) db = doctable.DocTable(target=':memory:', schema=schema) str(db.engine) '<ConnectEngine::sqlite:///:memory:>' The DocTable constructor can also accept an engine in place of a target and dialect, and thus share ConnectEngines between multiple DocTable objects. In this case, the doctable constructor will use the provided schema to insert the table information into the engine metadata and create the table if doesn't already exist. It will also generate a new connection object from the ConnectEngine. # a w engine.clear_metadata() print(engine.tables.keys()) print(engine.list_tables()) dict_keys([]) ['temp2'] # make a new doctable using the existing engine schema = (('idcol', 'id'), ('string', 'name')) db = doctable.DocTable(engine=engine, schema=schema, tabname='tmp5') db <doctable.doctable.DocTable at 0x7f62a1096b20> # make another doctable using existing engine schema2 = (('idcol', 'id'), ('string', 'name')) db2 = doctable.DocTable(engine=engine, schema=schema2, tabname='tmp6') db2 <doctable.doctable.DocTable at 0x7f62a0fd4cd0> # we can see that both tables have been created in the database engine.list_tables() ['temp2', 'tmp5', 'tmp6'] # and that both are registered in the metadata engine.tables.keys() dict_keys(['tmp5', 'tmp6']) Some ConnectEngine methods are also accessable through the DocTable instances. db.list_tables() ['temp2', 'tmp5', 'tmp6'] db.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 name VARCHAR True None auto 0 # and this is equivalent to calling the engine method reopen(), which clears # metadata and closes connection pool db.reopen_engine()","title":"DocTable and ConnectEngine"},{"location":"legacy_documentation/doctable_file_column_types/","text":"DocTable File Column Types It is often good advice to avoid storing large binary data in an SQL table because it will significantly impact the read performance of the entire table. I find, however, that it can be extremely useful in text analysis applications as a way to keep track of a large number of models with associated metadata. As an alternative to storing binary data in the table directly, DocTable includes a number of custom column types that can transparently store data into the filesystem and keep track of it using the schema definitions. I provide two file storage column types: (1) TextFileCol for storing text data, and (2) PickleFileCol for storing any python data that requires pickling. import numpy as np from pathlib import Path import sys sys.path.append('..') import doctable # automatically clean up temp folder after python ends import tempfile tempdir = tempfile.TemporaryDirectory() tmpfolder = tempdir.name tmpfolder = Path(tmpfolder) tmpfolder PosixPath('/tmp/tmpkoe57pma') Now I create a new table representing a matrix. Notice that I use the PickleFileCol column shortcut to create the column. This column is equivalent to Col(None, coltype='picklefile', type_args=dict(folder=folder)) . See that to SQLite, this column simply looks like a text column. import dataclasses @doctable.schema(require_slots=False) class MatrixRow: id: int = doctable.IDCol() array: np.ndarray = doctable.PickleFileCol(f'{tmpfolder}/matrix_pickle_files') # will store files in the tmp directory db = doctable.DocTable(target=f'{tmpfolder}/test.db', schema=MatrixRow, new_db=True) db.schema_info() [{'name': 'id', 'type': INTEGER(), 'nullable': False, 'default': None, 'autoincrement': 'auto', 'primary_key': 1}, {'name': 'array', 'type': VARCHAR(), 'nullable': True, 'default': None, 'autoincrement': 'auto', 'primary_key': 0}] Now we insert a new array. It appears to be inserted the same as any other object. db.insert({'array': np.random.rand(10,10)}) db.insert({'array': np.random.rand(10,10)}) print(db.count()) db.select_df(limit=3) 2 /DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead. warnings.warn('Method .count() is depricated. Please use .q.count() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') /DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: TypeDecorator PickleFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True. This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions. Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf) return self._engine.execute(query, *args, **kwargs) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id array 0 1 [[0.5329174823769329, 0.45399901667272, 0.4110... 1 2 [[0.6837499182333924, 0.40540705856582326, 0.6... But when we actually look at the filesystem, we see that files have been created to store the array. for fpath in tmpfolder.rglob('*.pic'): print(str(fpath)) /tmp/tmpkoe57pma/matrix_pickle_files/838448859815.pic /tmp/tmpkoe57pma/matrix_pickle_files/241946168596.pic If we want to see the raw data stored in the table, we can create a new doctable without a defined schema. See that the raw filenames have been stored in the database. Recall that the directory indicating where to find these files was provided in the schema itself. vdb = doctable.DocTable(f'{tmpfolder}/test.db') print(vdb.count()) vdb.head() 2 /DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead. warnings.warn('Method .count() is depricated. Please use .q.count() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id array 0 1 838448859815.pic 1 2 241946168596.pic Data Folder Consistency Now we try to delete a row from the database. We can see that it was deleted as expected. db.delete(where=db['id']==1) print(db.count()) db.head() 1 /DataDrive/code/doctable/examples/../doctable/doctable.py:495: UserWarning: Method .delete() is depricated. Please use .q.delete() instead. warnings.warn('Method .delete() is depricated. Please use .q.delete() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead. warnings.warn('Method .count() is depricated. Please use .q.count() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') /DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: TypeDecorator PickleFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True. This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions. Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf) return self._engine.execute(query, *args, **kwargs) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id array 0 2 [[0.6837499182333924, 0.40540705856582326, 0.6... However, when we check the folder where the data was stored, we find that the file was, in fact, not deleted. This is the case for technical reasons. for fpath in tmpfolder.rglob('*.pic'): print(str(fpath)) /tmp/tmpkoe57pma/matrix_pickle_files/838448859815.pic /tmp/tmpkoe57pma/matrix_pickle_files/241946168596.pic We can clean up the unused files using clean_col_files() though. Note that the specific column to clean must be provided. db.clean_col_files('array') for fpath in tmpfolder.rglob('*.pic'): print(str(fpath)) /tmp/tmpkoe57pma/matrix_pickle_files/241946168596.pic /DataDrive/code/doctable/examples/../doctable/doctable.py:444: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') /DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: TypeDecorator PickleFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True. This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions. Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf) return self._engine.execute(query, *args, **kwargs) There may be a situation where doctable cannot find the folder associated with an existing row. We can also use clean_col_files() to check for missing data. This might most frequently occur when the wrong folder is specified in the schema after moving the data file folder. For example, we delete all the pickle files in the directory and then run clean_col_files() . [fp.unlink() for fp in tmpfolder.rglob('*.pic')] for fpath in tmpfolder.rglob('*.pic'): print(str(fpath)) # see that the exception was raised try: db.clean_col_files('array') except FileNotFoundError as e: print(e) These files were not found while cleaning: {'/tmp/tmpkoe57pma/matrix_pickle_files/241946168596.pic'} Text File Types We can also store text files in a similar way. For this, use TextFileCol in the folder specification. @doctable.schema(require_slots=False) class TextFileRow: id: int = doctable.IDCol() text: str = doctable.TextFileCol(f'{tmpfolder}/my_text_files') # will store files in the tmp directory tdb = doctable.DocTable(target=f'{tmpfolder}/test_textfiles.db', schema=TextFileRow, new_db=True) tdb.insert({'text': 'Hello world. DocTable is the most useful python package of all time.'}) tdb.insert({'text': 'Star Wars is my favorite movie.'}) tdb.head() /DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') /DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: TypeDecorator TextFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True. This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions. Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf) return self._engine.execute(query, *args, **kwargs) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id text 0 1 Hello world. DocTable is the most useful pytho... 1 2 Star Wars is my favorite movie. # and they look like text files vdb = doctable.DocTable(f'{tmpfolder}/test_textfiles.db') print(vdb.count()) vdb.head() 2 /DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead. warnings.warn('Method .count() is depricated. Please use .q.count() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id text 0 1 509620359442.txt 1 2 409663648614.txt See that the text files were created, and they look like normal text files so we can read them normally. for fpath in tmpfolder.rglob('*.txt'): print(f\"{fpath}: {fpath.read_text()}\") /tmp/tmpkoe57pma/my_text_files/409663648614.txt: Star Wars is my favorite movie. /tmp/tmpkoe57pma/my_text_files/509620359442.txt: Hello world. DocTable is the most useful python package of all time.","title":"DocTable File Column Types"},{"location":"legacy_documentation/doctable_file_column_types/#doctable-file-column-types","text":"It is often good advice to avoid storing large binary data in an SQL table because it will significantly impact the read performance of the entire table. I find, however, that it can be extremely useful in text analysis applications as a way to keep track of a large number of models with associated metadata. As an alternative to storing binary data in the table directly, DocTable includes a number of custom column types that can transparently store data into the filesystem and keep track of it using the schema definitions. I provide two file storage column types: (1) TextFileCol for storing text data, and (2) PickleFileCol for storing any python data that requires pickling. import numpy as np from pathlib import Path import sys sys.path.append('..') import doctable # automatically clean up temp folder after python ends import tempfile tempdir = tempfile.TemporaryDirectory() tmpfolder = tempdir.name tmpfolder = Path(tmpfolder) tmpfolder PosixPath('/tmp/tmpkoe57pma') Now I create a new table representing a matrix. Notice that I use the PickleFileCol column shortcut to create the column. This column is equivalent to Col(None, coltype='picklefile', type_args=dict(folder=folder)) . See that to SQLite, this column simply looks like a text column. import dataclasses @doctable.schema(require_slots=False) class MatrixRow: id: int = doctable.IDCol() array: np.ndarray = doctable.PickleFileCol(f'{tmpfolder}/matrix_pickle_files') # will store files in the tmp directory db = doctable.DocTable(target=f'{tmpfolder}/test.db', schema=MatrixRow, new_db=True) db.schema_info() [{'name': 'id', 'type': INTEGER(), 'nullable': False, 'default': None, 'autoincrement': 'auto', 'primary_key': 1}, {'name': 'array', 'type': VARCHAR(), 'nullable': True, 'default': None, 'autoincrement': 'auto', 'primary_key': 0}] Now we insert a new array. It appears to be inserted the same as any other object. db.insert({'array': np.random.rand(10,10)}) db.insert({'array': np.random.rand(10,10)}) print(db.count()) db.select_df(limit=3) 2 /DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead. warnings.warn('Method .count() is depricated. Please use .q.count() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') /DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: TypeDecorator PickleFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True. This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions. Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf) return self._engine.execute(query, *args, **kwargs) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id array 0 1 [[0.5329174823769329, 0.45399901667272, 0.4110... 1 2 [[0.6837499182333924, 0.40540705856582326, 0.6... But when we actually look at the filesystem, we see that files have been created to store the array. for fpath in tmpfolder.rglob('*.pic'): print(str(fpath)) /tmp/tmpkoe57pma/matrix_pickle_files/838448859815.pic /tmp/tmpkoe57pma/matrix_pickle_files/241946168596.pic If we want to see the raw data stored in the table, we can create a new doctable without a defined schema. See that the raw filenames have been stored in the database. Recall that the directory indicating where to find these files was provided in the schema itself. vdb = doctable.DocTable(f'{tmpfolder}/test.db') print(vdb.count()) vdb.head() 2 /DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead. warnings.warn('Method .count() is depricated. Please use .q.count() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id array 0 1 838448859815.pic 1 2 241946168596.pic","title":"DocTable File Column Types"},{"location":"legacy_documentation/doctable_file_column_types/#data-folder-consistency","text":"Now we try to delete a row from the database. We can see that it was deleted as expected. db.delete(where=db['id']==1) print(db.count()) db.head() 1 /DataDrive/code/doctable/examples/../doctable/doctable.py:495: UserWarning: Method .delete() is depricated. Please use .q.delete() instead. warnings.warn('Method .delete() is depricated. Please use .q.delete() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead. warnings.warn('Method .count() is depricated. Please use .q.count() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') /DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: TypeDecorator PickleFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True. This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions. Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf) return self._engine.execute(query, *args, **kwargs) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id array 0 2 [[0.6837499182333924, 0.40540705856582326, 0.6... However, when we check the folder where the data was stored, we find that the file was, in fact, not deleted. This is the case for technical reasons. for fpath in tmpfolder.rglob('*.pic'): print(str(fpath)) /tmp/tmpkoe57pma/matrix_pickle_files/838448859815.pic /tmp/tmpkoe57pma/matrix_pickle_files/241946168596.pic We can clean up the unused files using clean_col_files() though. Note that the specific column to clean must be provided. db.clean_col_files('array') for fpath in tmpfolder.rglob('*.pic'): print(str(fpath)) /tmp/tmpkoe57pma/matrix_pickle_files/241946168596.pic /DataDrive/code/doctable/examples/../doctable/doctable.py:444: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') /DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: TypeDecorator PickleFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True. This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions. Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf) return self._engine.execute(query, *args, **kwargs) There may be a situation where doctable cannot find the folder associated with an existing row. We can also use clean_col_files() to check for missing data. This might most frequently occur when the wrong folder is specified in the schema after moving the data file folder. For example, we delete all the pickle files in the directory and then run clean_col_files() . [fp.unlink() for fp in tmpfolder.rglob('*.pic')] for fpath in tmpfolder.rglob('*.pic'): print(str(fpath)) # see that the exception was raised try: db.clean_col_files('array') except FileNotFoundError as e: print(e) These files were not found while cleaning: {'/tmp/tmpkoe57pma/matrix_pickle_files/241946168596.pic'}","title":"Data Folder Consistency"},{"location":"legacy_documentation/doctable_file_column_types/#text-file-types","text":"We can also store text files in a similar way. For this, use TextFileCol in the folder specification. @doctable.schema(require_slots=False) class TextFileRow: id: int = doctable.IDCol() text: str = doctable.TextFileCol(f'{tmpfolder}/my_text_files') # will store files in the tmp directory tdb = doctable.DocTable(target=f'{tmpfolder}/test_textfiles.db', schema=TextFileRow, new_db=True) tdb.insert({'text': 'Hello world. DocTable is the most useful python package of all time.'}) tdb.insert({'text': 'Star Wars is my favorite movie.'}) tdb.head() /DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') /DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: TypeDecorator TextFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True. This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions. Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf) return self._engine.execute(query, *args, **kwargs) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id text 0 1 Hello world. DocTable is the most useful pytho... 1 2 Star Wars is my favorite movie. # and they look like text files vdb = doctable.DocTable(f'{tmpfolder}/test_textfiles.db') print(vdb.count()) vdb.head() 2 /DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead. warnings.warn('Method .count() is depricated. Please use .q.count() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id text 0 1 509620359442.txt 1 2 409663648614.txt See that the text files were created, and they look like normal text files so we can read them normally. for fpath in tmpfolder.rglob('*.txt'): print(f\"{fpath}: {fpath.read_text()}\") /tmp/tmpkoe57pma/my_text_files/409663648614.txt: Star Wars is my favorite movie. /tmp/tmpkoe57pma/my_text_files/509620359442.txt: Hello world. DocTable is the most useful python package of all time.","title":"Text File Types"},{"location":"legacy_documentation/doctable_insert_delete/","text":"DocTable Examples: Insert and Delete Here we show basics of inserting and deleting data into a doctable. import random import pandas as pd import numpy as np import sys sys.path.append('..') import doctable import dataclasses @doctable.schema class Record: __slots__ = [] id: int = doctable.IDCol() name: str = doctable.Col(nullable=False) age: int = None is_old: bool = None def make_rows(N=3): rows = list() for i in range(N): age = random.random() # number in [0,1] is_old = age > 0.5 yield {'name':'user_'+str(i), 'age':age, 'is_old':is_old} return rows Basic Inserts There are only two ways to insert: one at a time (pass single dict), or multiple at a time (pass sequence of dicts). table = doctable.DocTable(target=':memory:', schema=Record, verbose=True) for row in make_rows(): table.insert(row) table.select_df() DocTable: INSERT OR FAIL INTO _documents_ (name, age, is_old) VALUES (?, ?, ?) DocTable: INSERT OR FAIL INTO _documents_ (name, age, is_old) VALUES (?, ?, ?) DocTable: INSERT OR FAIL INTO _documents_ (name, age, is_old) VALUES (?, ?, ?) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 1 user_0 0.485860 False 1 2 user_1 0.661900 True 2 3 user_2 0.082627 False newrows = list(make_rows()) table.insert(newrows) table.select_df(verbose=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 1 user_0 0.485860 False 1 2 user_1 0.661900 True 2 3 user_2 0.082627 False 3 4 user_0 0.936185 True 4 5 user_1 0.082005 False 5 6 user_2 0.567260 True Deletes # delete all entries where is_old is false table.delete(where=~table['is_old']) table.select_df(verbose=False) DocTable: DELETE FROM _documents_ WHERE _documents_.is_old = 0 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 2 user_1 0.661900 True 1 4 user_0 0.936185 True 2 6 user_2 0.567260 True # use vacuum to free unused space now table.delete(where=~table['is_old'], vacuum=True) table.select_df(verbose=False) DocTable: DELETE FROM _documents_ WHERE _documents_.is_old = 0 DocTable: VACUUM .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 2 user_1 0.661900 True 1 4 user_0 0.936185 True 2 6 user_2 0.567260 True # delete everything table.delete() table.count() DocTable: DELETE FROM _documents_ DocTable: SELECT count() AS count_1 FROM _documents_ LIMIT ? OFFSET ? 0","title":"DocTable Examples: Insert and Delete"},{"location":"legacy_documentation/doctable_insert_delete/#doctable-examples-insert-and-delete","text":"Here we show basics of inserting and deleting data into a doctable. import random import pandas as pd import numpy as np import sys sys.path.append('..') import doctable import dataclasses @doctable.schema class Record: __slots__ = [] id: int = doctable.IDCol() name: str = doctable.Col(nullable=False) age: int = None is_old: bool = None def make_rows(N=3): rows = list() for i in range(N): age = random.random() # number in [0,1] is_old = age > 0.5 yield {'name':'user_'+str(i), 'age':age, 'is_old':is_old} return rows","title":"DocTable Examples: Insert and Delete"},{"location":"legacy_documentation/doctable_insert_delete/#basic-inserts","text":"There are only two ways to insert: one at a time (pass single dict), or multiple at a time (pass sequence of dicts). table = doctable.DocTable(target=':memory:', schema=Record, verbose=True) for row in make_rows(): table.insert(row) table.select_df() DocTable: INSERT OR FAIL INTO _documents_ (name, age, is_old) VALUES (?, ?, ?) DocTable: INSERT OR FAIL INTO _documents_ (name, age, is_old) VALUES (?, ?, ?) DocTable: INSERT OR FAIL INTO _documents_ (name, age, is_old) VALUES (?, ?, ?) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 1 user_0 0.485860 False 1 2 user_1 0.661900 True 2 3 user_2 0.082627 False newrows = list(make_rows()) table.insert(newrows) table.select_df(verbose=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 1 user_0 0.485860 False 1 2 user_1 0.661900 True 2 3 user_2 0.082627 False 3 4 user_0 0.936185 True 4 5 user_1 0.082005 False 5 6 user_2 0.567260 True","title":"Basic Inserts"},{"location":"legacy_documentation/doctable_insert_delete/#deletes","text":"# delete all entries where is_old is false table.delete(where=~table['is_old']) table.select_df(verbose=False) DocTable: DELETE FROM _documents_ WHERE _documents_.is_old = 0 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 2 user_1 0.661900 True 1 4 user_0 0.936185 True 2 6 user_2 0.567260 True # use vacuum to free unused space now table.delete(where=~table['is_old'], vacuum=True) table.select_df(verbose=False) DocTable: DELETE FROM _documents_ WHERE _documents_.is_old = 0 DocTable: VACUUM .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 2 user_1 0.661900 True 1 4 user_0 0.936185 True 2 6 user_2 0.567260 True # delete everything table.delete() table.count() DocTable: DELETE FROM _documents_ DocTable: SELECT count() AS count_1 FROM _documents_ LIMIT ? OFFSET ? 0","title":"Deletes"},{"location":"legacy_documentation/doctable_multitable/","text":"Example: Multiple Tables In this example, I show how doctable can be used with multiple relational tables to perform queries which automatically merge different aspects of your dataset when you use .select() . By integrating these relations into the schema, your database can automatically maintain consistency between tables by deleting irrelevant elements when their relations disappear. There are two important features of any multi-table schema using doctable: (1) Set the foreign_keys=True in the DocTable or ConnectEngine constructor. It is enabled by default. Otherwise sqlalchemy will not enable. (2) Use the \"foreignkey\" column type to set the constraint, probably with the onupdate and ondelete keywords specifiied. I will show two examples here: many-to-many relations, and many-to-one relations. import datetime import dataclasses import tempfile import sys sys.path.append('..') import doctable tmp = tempfile.TemporaryDirectory() Many-to-Many Relationships The premise is that we have an imaginary API where we can get newly released books along with the libraries they are associted with (although they man, in some cases, not have library information). We want to keep track of the set of books with unique titles, and have book information exist on its own (i.e. we can insert book information if it does not have library information). We would also like to keep track of the libraries they belong to. We need this schema to be fast for selection, but it can be slow for insertion. Primary accesses methods: insert a book query books by year of publication insert a single library and associated books query books associated with libraries in certain zips In this example, we are going to use two tables with a many-to-many relationships and a table to handle relationships between them (required for a many-to-many relationship): BookTable : keeps title and publication year of each book. Should exist independently of LibraryTable, because we may not want to use LibraryTable at all. LibraryTable : keeps name of library, makes it easy to query by Library. BookLibraryRelationsTable : keeps track of relationships between BookTable and LibraryTable. First we define the BookTable table. Because we are primarily interested in books, we will create a separate Book object for working with them. @doctable.schema(frozen=True, eq=True) class Book: __slots__ = [] _id: int = doctable.IDCol() isbn: str = doctable.Col(unique=True) title: str = doctable.Col() year: int = doctable.Col() author: str = doctable.Col() date_updated: datetime.datetime = doctable.UpdatedCol() class BookTable(doctable.DocTable): _tabname_ = 'books' _schema_ = Book _indices_ = [doctable.Index('isbn_index', 'isbn')] book_table = BookTable(target=f'{tmp.name}/1.db', new_db=True) We are not planning to work with author data outside of the schema definition, so we include it as part of the table definition. @doctable.schema(frozen=True, eq=True) class Library: __slots__ = [] _id: int = doctable.IDCol() name: str = doctable.Col() zip: int = doctable.Col() class LibraryTable(doctable.DocTable): _tabname_ = 'libraries' _schema_ = Library _constraints_ = [doctable.Constraint('unique', 'name', 'zip')] library_table = LibraryTable(engine=book_table.engine) class BookLibraryRelationsTable(doctable.DocTable): '''Link between books and libraries.''' _tabname_ = 'book_library_relations' @doctable.schema class _schema_: __slots__ = [] _id: int = doctable.IDCol() book_isbn: int = doctable.Col(nullable=False) library_id: int = doctable.Col(nullable=False) _constraints_ = ( doctable.Constraint('foreignkey', ('book_isbn',), ('books.isbn',)), doctable.Constraint('foreignkey', ('library_id',), ('libraries._id',)), doctable.Constraint('unique', 'book_isbn', 'library_id'), ) relations_table = BookLibraryRelationsTable(engine=book_table.engine) relations_table.list_tables() ['book_library_relations', 'books', 'libraries'] Now we create some random books that are not at libraries and add them into our database. newly_published_books = [ Book(isbn='A', title='A', year=2020, author='Pierre Bourdieu'), Book(isbn='E', title='E', year=2018, author='Jean-Luc Picard'), ] for book in newly_published_books: print(book) Book(isbn='A', title='A', year=2020, author='Pierre Bourdieu') Book(isbn='E', title='E', year=2018, author='Jean-Luc Picard') Now we insert the list of books that were published. It works as expected. book_table.insert(newly_published_books, ifnotunique='replace') book_table.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } _id isbn title year author date_updated 0 1 A A 2020 Pierre Bourdieu 2022-07-26 21:30:30.364805 1 2 E E 2018 Jean-Luc Picard 2022-07-26 21:30:30.364812 And now lets add a bunch of books that are associated with library objects. new_library_books = { Library(name='Library1', zip=12345): [ Book(isbn='A', title='A', year=2020, author='Pierre Bourdieu'), Book(isbn='B', title='B', year=2020, author='Pierre Bourdieu'), ], Library(name='Library2', zip=12345): [ Book(isbn='A', title='A', year=2020, author='Devin Cornell'), Book(isbn='C', title='C', year=2021, author='Jean-Luc Picard'), ], Library(name='Library3', zip=67890): [ Book(isbn='A', title='A', year=2020, author='Pierre Bourdieu'), Book(isbn='B', title='B', year=2020, author='Jean-Luc Picard'), Book(isbn='D', title='D', year=2019, author='Devin Cornell'), ], } for library, books in new_library_books.items(): r = library_table.insert(library, ifnotunique='ignore') book_table.insert(books, ifnotunique='replace') relations_table.insert([{'book_isbn':b.isbn, 'library_id': r.lastrowid} for b in books], ifnotunique='ignore') book_table.select_df() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } _id isbn title year author date_updated 0 2 E E 2018 Jean-Luc Picard 2022-07-26 21:30:30.364812 1 6 C C 2021 Jean-Luc Picard 2022-07-26 21:30:30.482867 2 7 A A 2020 Pierre Bourdieu 2022-07-26 21:30:30.494686 3 8 B B 2020 Jean-Luc Picard 2022-07-26 21:30:30.494692 4 9 D D 2019 Devin Cornell 2022-07-26 21:30:30.494694 library_table.select_df() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } _id name zip 0 1 Library1 12345 1 2 Library2 12345 2 3 Library3 67890 relations_table.select_df() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } _id book_isbn library_id 0 1 A 1 1 2 B 1 2 3 A 2 3 4 C 2 4 5 A 3 5 6 B 3 6 7 D 3 Select Queries That Join Tables Similar to sqlalchemy, DocTable joins are doen simply by replacing the where conditional. While not technically nessecary, typically you will be joining tables on foreign key columns because it is much faster. bt, lt, rt = book_table, library_table, relations_table For the first example, say we want to get the isbn numbers of books associated with each library in zip code 12345. We implement the join using a simple conditional equating the associated keys in each table. Our database schema already knows that the foreign keys are in place, so this expression will give us the join we want. lt.select([lt['name'], rt['book_isbn']], where=(lt['_id']==rt['library_id']) & (lt['zip']==12345), as_dataclass=False) [('Library1', 'A'), ('Library1', 'B'), ('Library2', 'A'), ('Library2', 'C')] Now say we want to characterize each library according to the age distribution of it's books. We use two conditionals for the join: one connecting library table to relations table, and another connecting relations table to books table. We also include the condition to get only libraries associated with the given zip. conditions = (bt['isbn']==rt['book_isbn']) & (rt['library_id']==lt['_id']) & (lt['zip']==12345) bt.select([bt['title'], bt['year'], lt['name']], where=conditions, as_dataclass=False) [('C', 2021, 'Library2'), ('A', 2020, 'Library1'), ('A', 2020, 'Library2'), ('B', 2020, 'Library1')] Alternatively we can use the .join method of doctable (although I recommend just using select statements). jt = lt.join(rt, (lt['zip']==12345) & (lt['_id']==rt['library_id']), isouter=False) bt.select(where=bt['isbn']==jt.c['book_library_relations_book_isbn'], as_dataclass=True) bt.select([bt['title'], jt.c['book_library_relations_library_id']], where=bt['isbn']==jt.c['book_library_relations_book_isbn'], as_dataclass=False) bt.select([bt['title'], jt.c['libraries_name']], where=bt['isbn']==jt.c['book_library_relations_book_isbn'], as_dataclass=False, limit=3) [('C', 'Library1'), ('C', 'Library2'), ('C', 'Library3')] Many-to-One Relationships Now we create an author class and table to demonstrate a many-to-one relationship. @doctable.schema(frozen=True, eq=True) class Author: __slots__ = [] #_id: int = doctable.IDCol() name: str = doctable.Col(primary_key=True, unique=True) age: int = doctable.Col() class AuthorTable(doctable.DocTable): _tabname_ = 'authors' _schema_ = Author _constraints_ = [doctable.Constraint('foreignkey', ('name',), ('books.author',))] #book_table_auth = BookTable(target=f'{tmp.name}/16.db', new_db=True) #author_table = AuthorTable(engine=book_table_auth.engine) author_table = AuthorTable(engine=book_table.engine) author_table.delete() author_table.insert([ Author(name='Devin Cornell', age=30), Author(name='Pierre Bourdieu', age=99), Author(name='Jean-Luc Picard', age=1000), ]) author_table.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name age 0 Devin Cornell 30 1 Pierre Bourdieu 99 2 Jean-Luc Picard 1000 book_table.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } _id isbn title year author date_updated 0 2 E E 2018 Jean-Luc Picard 2022-07-26 21:30:30.364812 1 6 C C 2021 Jean-Luc Picard 2022-07-26 21:30:30.482867 2 7 A A 2020 Pierre Bourdieu 2022-07-26 21:30:30.494686 3 8 B B 2020 Jean-Luc Picard 2022-07-26 21:30:30.494692 4 9 D D 2019 Devin Cornell 2022-07-26 21:30:30.494694 columns = [book_table['year'], author_table['age'], author_table['name']] where = (book_table['author']==author_table['name']) & (book_table['author'] > 30) book_table.select_df(columns, where=where) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year age name 0 2018 1000 Jean-Luc Picard 1 2021 1000 Jean-Luc Picard 2 2020 99 Pierre Bourdieu 3 2020 1000 Jean-Luc Picard 4 2019 30 Devin Cornell","title":"Example: Multiple Tables"},{"location":"legacy_documentation/doctable_multitable/#example-multiple-tables","text":"In this example, I show how doctable can be used with multiple relational tables to perform queries which automatically merge different aspects of your dataset when you use .select() . By integrating these relations into the schema, your database can automatically maintain consistency between tables by deleting irrelevant elements when their relations disappear. There are two important features of any multi-table schema using doctable: (1) Set the foreign_keys=True in the DocTable or ConnectEngine constructor. It is enabled by default. Otherwise sqlalchemy will not enable. (2) Use the \"foreignkey\" column type to set the constraint, probably with the onupdate and ondelete keywords specifiied. I will show two examples here: many-to-many relations, and many-to-one relations. import datetime import dataclasses import tempfile import sys sys.path.append('..') import doctable tmp = tempfile.TemporaryDirectory()","title":"Example: Multiple Tables"},{"location":"legacy_documentation/doctable_multitable/#many-to-many-relationships","text":"The premise is that we have an imaginary API where we can get newly released books along with the libraries they are associted with (although they man, in some cases, not have library information). We want to keep track of the set of books with unique titles, and have book information exist on its own (i.e. we can insert book information if it does not have library information). We would also like to keep track of the libraries they belong to. We need this schema to be fast for selection, but it can be slow for insertion. Primary accesses methods: insert a book query books by year of publication insert a single library and associated books query books associated with libraries in certain zips In this example, we are going to use two tables with a many-to-many relationships and a table to handle relationships between them (required for a many-to-many relationship): BookTable : keeps title and publication year of each book. Should exist independently of LibraryTable, because we may not want to use LibraryTable at all. LibraryTable : keeps name of library, makes it easy to query by Library. BookLibraryRelationsTable : keeps track of relationships between BookTable and LibraryTable. First we define the BookTable table. Because we are primarily interested in books, we will create a separate Book object for working with them. @doctable.schema(frozen=True, eq=True) class Book: __slots__ = [] _id: int = doctable.IDCol() isbn: str = doctable.Col(unique=True) title: str = doctable.Col() year: int = doctable.Col() author: str = doctable.Col() date_updated: datetime.datetime = doctable.UpdatedCol() class BookTable(doctable.DocTable): _tabname_ = 'books' _schema_ = Book _indices_ = [doctable.Index('isbn_index', 'isbn')] book_table = BookTable(target=f'{tmp.name}/1.db', new_db=True) We are not planning to work with author data outside of the schema definition, so we include it as part of the table definition. @doctable.schema(frozen=True, eq=True) class Library: __slots__ = [] _id: int = doctable.IDCol() name: str = doctable.Col() zip: int = doctable.Col() class LibraryTable(doctable.DocTable): _tabname_ = 'libraries' _schema_ = Library _constraints_ = [doctable.Constraint('unique', 'name', 'zip')] library_table = LibraryTable(engine=book_table.engine) class BookLibraryRelationsTable(doctable.DocTable): '''Link between books and libraries.''' _tabname_ = 'book_library_relations' @doctable.schema class _schema_: __slots__ = [] _id: int = doctable.IDCol() book_isbn: int = doctable.Col(nullable=False) library_id: int = doctable.Col(nullable=False) _constraints_ = ( doctable.Constraint('foreignkey', ('book_isbn',), ('books.isbn',)), doctable.Constraint('foreignkey', ('library_id',), ('libraries._id',)), doctable.Constraint('unique', 'book_isbn', 'library_id'), ) relations_table = BookLibraryRelationsTable(engine=book_table.engine) relations_table.list_tables() ['book_library_relations', 'books', 'libraries'] Now we create some random books that are not at libraries and add them into our database. newly_published_books = [ Book(isbn='A', title='A', year=2020, author='Pierre Bourdieu'), Book(isbn='E', title='E', year=2018, author='Jean-Luc Picard'), ] for book in newly_published_books: print(book) Book(isbn='A', title='A', year=2020, author='Pierre Bourdieu') Book(isbn='E', title='E', year=2018, author='Jean-Luc Picard') Now we insert the list of books that were published. It works as expected. book_table.insert(newly_published_books, ifnotunique='replace') book_table.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } _id isbn title year author date_updated 0 1 A A 2020 Pierre Bourdieu 2022-07-26 21:30:30.364805 1 2 E E 2018 Jean-Luc Picard 2022-07-26 21:30:30.364812 And now lets add a bunch of books that are associated with library objects. new_library_books = { Library(name='Library1', zip=12345): [ Book(isbn='A', title='A', year=2020, author='Pierre Bourdieu'), Book(isbn='B', title='B', year=2020, author='Pierre Bourdieu'), ], Library(name='Library2', zip=12345): [ Book(isbn='A', title='A', year=2020, author='Devin Cornell'), Book(isbn='C', title='C', year=2021, author='Jean-Luc Picard'), ], Library(name='Library3', zip=67890): [ Book(isbn='A', title='A', year=2020, author='Pierre Bourdieu'), Book(isbn='B', title='B', year=2020, author='Jean-Luc Picard'), Book(isbn='D', title='D', year=2019, author='Devin Cornell'), ], } for library, books in new_library_books.items(): r = library_table.insert(library, ifnotunique='ignore') book_table.insert(books, ifnotunique='replace') relations_table.insert([{'book_isbn':b.isbn, 'library_id': r.lastrowid} for b in books], ifnotunique='ignore') book_table.select_df() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } _id isbn title year author date_updated 0 2 E E 2018 Jean-Luc Picard 2022-07-26 21:30:30.364812 1 6 C C 2021 Jean-Luc Picard 2022-07-26 21:30:30.482867 2 7 A A 2020 Pierre Bourdieu 2022-07-26 21:30:30.494686 3 8 B B 2020 Jean-Luc Picard 2022-07-26 21:30:30.494692 4 9 D D 2019 Devin Cornell 2022-07-26 21:30:30.494694 library_table.select_df() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } _id name zip 0 1 Library1 12345 1 2 Library2 12345 2 3 Library3 67890 relations_table.select_df() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } _id book_isbn library_id 0 1 A 1 1 2 B 1 2 3 A 2 3 4 C 2 4 5 A 3 5 6 B 3 6 7 D 3","title":"Many-to-Many Relationships"},{"location":"legacy_documentation/doctable_multitable/#select-queries-that-join-tables","text":"Similar to sqlalchemy, DocTable joins are doen simply by replacing the where conditional. While not technically nessecary, typically you will be joining tables on foreign key columns because it is much faster. bt, lt, rt = book_table, library_table, relations_table For the first example, say we want to get the isbn numbers of books associated with each library in zip code 12345. We implement the join using a simple conditional equating the associated keys in each table. Our database schema already knows that the foreign keys are in place, so this expression will give us the join we want. lt.select([lt['name'], rt['book_isbn']], where=(lt['_id']==rt['library_id']) & (lt['zip']==12345), as_dataclass=False) [('Library1', 'A'), ('Library1', 'B'), ('Library2', 'A'), ('Library2', 'C')] Now say we want to characterize each library according to the age distribution of it's books. We use two conditionals for the join: one connecting library table to relations table, and another connecting relations table to books table. We also include the condition to get only libraries associated with the given zip. conditions = (bt['isbn']==rt['book_isbn']) & (rt['library_id']==lt['_id']) & (lt['zip']==12345) bt.select([bt['title'], bt['year'], lt['name']], where=conditions, as_dataclass=False) [('C', 2021, 'Library2'), ('A', 2020, 'Library1'), ('A', 2020, 'Library2'), ('B', 2020, 'Library1')] Alternatively we can use the .join method of doctable (although I recommend just using select statements). jt = lt.join(rt, (lt['zip']==12345) & (lt['_id']==rt['library_id']), isouter=False) bt.select(where=bt['isbn']==jt.c['book_library_relations_book_isbn'], as_dataclass=True) bt.select([bt['title'], jt.c['book_library_relations_library_id']], where=bt['isbn']==jt.c['book_library_relations_book_isbn'], as_dataclass=False) bt.select([bt['title'], jt.c['libraries_name']], where=bt['isbn']==jt.c['book_library_relations_book_isbn'], as_dataclass=False, limit=3) [('C', 'Library1'), ('C', 'Library2'), ('C', 'Library3')]","title":"Select Queries That Join Tables"},{"location":"legacy_documentation/doctable_multitable/#many-to-one-relationships","text":"Now we create an author class and table to demonstrate a many-to-one relationship. @doctable.schema(frozen=True, eq=True) class Author: __slots__ = [] #_id: int = doctable.IDCol() name: str = doctable.Col(primary_key=True, unique=True) age: int = doctable.Col() class AuthorTable(doctable.DocTable): _tabname_ = 'authors' _schema_ = Author _constraints_ = [doctable.Constraint('foreignkey', ('name',), ('books.author',))] #book_table_auth = BookTable(target=f'{tmp.name}/16.db', new_db=True) #author_table = AuthorTable(engine=book_table_auth.engine) author_table = AuthorTable(engine=book_table.engine) author_table.delete() author_table.insert([ Author(name='Devin Cornell', age=30), Author(name='Pierre Bourdieu', age=99), Author(name='Jean-Luc Picard', age=1000), ]) author_table.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name age 0 Devin Cornell 30 1 Pierre Bourdieu 99 2 Jean-Luc Picard 1000 book_table.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } _id isbn title year author date_updated 0 2 E E 2018 Jean-Luc Picard 2022-07-26 21:30:30.364812 1 6 C C 2021 Jean-Luc Picard 2022-07-26 21:30:30.482867 2 7 A A 2020 Pierre Bourdieu 2022-07-26 21:30:30.494686 3 8 B B 2020 Jean-Luc Picard 2022-07-26 21:30:30.494692 4 9 D D 2019 Devin Cornell 2022-07-26 21:30:30.494694 columns = [book_table['year'], author_table['age'], author_table['name']] where = (book_table['author']==author_table['name']) & (book_table['author'] > 30) book_table.select_df(columns, where=where) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year age name 0 2018 1000 Jean-Luc Picard 1 2021 1000 Jean-Luc Picard 2 2020 99 Pierre Bourdieu 3 2020 1000 Jean-Luc Picard 4 2019 30 Devin Cornell","title":"Many-to-One Relationships"},{"location":"legacy_documentation/doctable_parsetreedoc_column/","text":"ParseTreeDoc Column Types Creating parsetrees with spacy can be a computationally expensive task, so we may often want to store them in a database for better use. Because they may be large binary files, we will store them as pickle file column types, but with an additional serialization step. import spacy nlp = spacy.load('en_core_web_sm') import numpy as np from pathlib import Path import sys sys.path.append('..') import doctable # automatically clean up temp folder after python ends import tempfile tempdir = tempfile.TemporaryDirectory() tmpfolder = tempdir.name tmpfolder import tempfile with tempfile.TemporaryDirectory() as tmp: print(tmp) /tmp/tmpnmvhkw9t Create some test data and make a new ParseTreeDoc object. texts = [ 'Help me Obi-Wan Kenobi. You\u2019re my only hope. ', 'I find your lack of faith disturbing. ', 'Do, or do not. There is no try. ' ] parser = doctable.ParsePipeline([nlp, doctable.Comp('get_parsetrees')]) docs = parser.parsemany(texts) for doc in docs: print(len(doc)) 2 1 2 Now we create a schema that includes the doc column and the ParseTreeFileCol default value. Notice that using the type hint ParseTreeDoc and giving a generic Col default value is also sufficient. import dataclasses @doctable.schema(require_slots=False) class DocRow: id: int = doctable.IDCol() doc: doctable.ParseTreeDoc = doctable.ParseTreeFileCol(f'{tmpfolder}/parsetree_pickle_files') # could also use this: #doc: doctable.ParseTreeDoc = doctable.Col(type_args=dict(folder=f'{tmp}/parsetree_pickle_files')) db = doctable.DocTable(target=f'{tmpfolder}/test_ptrees.db', schema=DocRow, new_db=True) db.schema_info() [{'name': 'id', 'type': INTEGER(), 'nullable': False, 'default': None, 'autoincrement': 'auto', 'primary_key': 1}, {'name': 'doc', 'type': VARCHAR(), 'nullable': True, 'default': None, 'autoincrement': 'auto', 'primary_key': 0}] #db.insert([{'doc':doc} for doc in docs]) for doc in docs: db.insert({'doc': doc}) db.head(3) /DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 3 0 3 for idx, doc in db.q.select_raw(): print(f\"doc id {idx}:\") for i, sent in enumerate(doc): print(f\"\\tsent {i}: {[t.text for t in sent]}\") doc id 1: sent 0: ['Help', 'me', 'Obi', '-', 'Wan', 'Kenobi', '.'] sent 1: ['You', '\u2019re', 'my', 'only', 'hope', '.'] doc id 2: sent 0: ['I', 'find', 'your', 'lack', 'of', 'faith', 'disturbing', '.'] doc id 3: sent 0: ['Do', ',', 'or', 'do', 'not', '.'] sent 1: ['There', 'is', 'no', 'try', '.'] /DataDrive/code/doctable/examples/../doctable/connectengine.py:70: SAWarning: TypeDecorator ParseTreeDocFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True. This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions. Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf) return self._engine.execute(query, *args, **kwargs) See that the files exist, and we can remove/clean them just as any other file column type. for fpath in Path(tmpfolder).rglob('*.pic'): print(str(fpath)) /tmp/tmpvmthfr7t/parsetree_pickle_files/347692105083_parsetreedoc.pic /tmp/tmpvmthfr7t/parsetree_pickle_files/98072534351_parsetreedoc.pic /tmp/tmpvmthfr7t/parsetree_pickle_files/689952128879_parsetreedoc.pic db.delete(db['id']==1) for fpath in Path(tmpfolder).rglob('*.pic'): print(str(fpath)) db.head() /tmp/tmpvmthfr7t/parsetree_pickle_files/347692105083_parsetreedoc.pic /tmp/tmpvmthfr7t/parsetree_pickle_files/98072534351_parsetreedoc.pic /tmp/tmpvmthfr7t/parsetree_pickle_files/689952128879_parsetreedoc.pic /DataDrive/code/doctable/examples/../doctable/doctable.py:506: UserWarning: Method .delete() is depricated. Please use .q.delete() instead. warnings.warn('Method .delete() is depricated. Please use .q.delete() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id doc 0 2 [(I, find, your, lack, of, faith, disturbing, .)] 1 3 [(Do, ,, or, do, not, .), (There, is, no, try,... db.clean_col_files('doc') for fpath in Path(tmpfolder).rglob('*.pic'): print(str(fpath)) /tmp/tmpvmthfr7t/parsetree_pickle_files/347692105083_parsetreedoc.pic /tmp/tmpvmthfr7t/parsetree_pickle_files/98072534351_parsetreedoc.pic /DataDrive/code/doctable/examples/../doctable/doctable.py:449: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') /DataDrive/code/doctable/examples/../doctable/connectengine.py:70: SAWarning: TypeDecorator ParseTreeDocFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True. This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions. Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf) return self._engine.execute(query, *args, **kwargs)","title":"ParseTreeDoc Column Types"},{"location":"legacy_documentation/doctable_parsetreedoc_column/#parsetreedoc-column-types","text":"Creating parsetrees with spacy can be a computationally expensive task, so we may often want to store them in a database for better use. Because they may be large binary files, we will store them as pickle file column types, but with an additional serialization step. import spacy nlp = spacy.load('en_core_web_sm') import numpy as np from pathlib import Path import sys sys.path.append('..') import doctable # automatically clean up temp folder after python ends import tempfile tempdir = tempfile.TemporaryDirectory() tmpfolder = tempdir.name tmpfolder import tempfile with tempfile.TemporaryDirectory() as tmp: print(tmp) /tmp/tmpnmvhkw9t Create some test data and make a new ParseTreeDoc object. texts = [ 'Help me Obi-Wan Kenobi. You\u2019re my only hope. ', 'I find your lack of faith disturbing. ', 'Do, or do not. There is no try. ' ] parser = doctable.ParsePipeline([nlp, doctable.Comp('get_parsetrees')]) docs = parser.parsemany(texts) for doc in docs: print(len(doc)) 2 1 2 Now we create a schema that includes the doc column and the ParseTreeFileCol default value. Notice that using the type hint ParseTreeDoc and giving a generic Col default value is also sufficient. import dataclasses @doctable.schema(require_slots=False) class DocRow: id: int = doctable.IDCol() doc: doctable.ParseTreeDoc = doctable.ParseTreeFileCol(f'{tmpfolder}/parsetree_pickle_files') # could also use this: #doc: doctable.ParseTreeDoc = doctable.Col(type_args=dict(folder=f'{tmp}/parsetree_pickle_files')) db = doctable.DocTable(target=f'{tmpfolder}/test_ptrees.db', schema=DocRow, new_db=True) db.schema_info() [{'name': 'id', 'type': INTEGER(), 'nullable': False, 'default': None, 'autoincrement': 'auto', 'primary_key': 1}, {'name': 'doc', 'type': VARCHAR(), 'nullable': True, 'default': None, 'autoincrement': 'auto', 'primary_key': 0}] #db.insert([{'doc':doc} for doc in docs]) for doc in docs: db.insert({'doc': doc}) db.head(3) /DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 3 0 3 for idx, doc in db.q.select_raw(): print(f\"doc id {idx}:\") for i, sent in enumerate(doc): print(f\"\\tsent {i}: {[t.text for t in sent]}\") doc id 1: sent 0: ['Help', 'me', 'Obi', '-', 'Wan', 'Kenobi', '.'] sent 1: ['You', '\u2019re', 'my', 'only', 'hope', '.'] doc id 2: sent 0: ['I', 'find', 'your', 'lack', 'of', 'faith', 'disturbing', '.'] doc id 3: sent 0: ['Do', ',', 'or', 'do', 'not', '.'] sent 1: ['There', 'is', 'no', 'try', '.'] /DataDrive/code/doctable/examples/../doctable/connectengine.py:70: SAWarning: TypeDecorator ParseTreeDocFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True. This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions. Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf) return self._engine.execute(query, *args, **kwargs) See that the files exist, and we can remove/clean them just as any other file column type. for fpath in Path(tmpfolder).rglob('*.pic'): print(str(fpath)) /tmp/tmpvmthfr7t/parsetree_pickle_files/347692105083_parsetreedoc.pic /tmp/tmpvmthfr7t/parsetree_pickle_files/98072534351_parsetreedoc.pic /tmp/tmpvmthfr7t/parsetree_pickle_files/689952128879_parsetreedoc.pic db.delete(db['id']==1) for fpath in Path(tmpfolder).rglob('*.pic'): print(str(fpath)) db.head() /tmp/tmpvmthfr7t/parsetree_pickle_files/347692105083_parsetreedoc.pic /tmp/tmpvmthfr7t/parsetree_pickle_files/98072534351_parsetreedoc.pic /tmp/tmpvmthfr7t/parsetree_pickle_files/689952128879_parsetreedoc.pic /DataDrive/code/doctable/examples/../doctable/doctable.py:506: UserWarning: Method .delete() is depricated. Please use .q.delete() instead. warnings.warn('Method .delete() is depricated. Please use .q.delete() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:408: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id doc 0 2 [(I, find, your, lack, of, faith, disturbing, .)] 1 3 [(Do, ,, or, do, not, .), (There, is, no, try,... db.clean_col_files('doc') for fpath in Path(tmpfolder).rglob('*.pic'): print(str(fpath)) /tmp/tmpvmthfr7t/parsetree_pickle_files/347692105083_parsetreedoc.pic /tmp/tmpvmthfr7t/parsetree_pickle_files/98072534351_parsetreedoc.pic /DataDrive/code/doctable/examples/../doctable/doctable.py:449: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') /DataDrive/code/doctable/examples/../doctable/connectengine.py:70: SAWarning: TypeDecorator ParseTreeDocFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True. This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions. Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf) return self._engine.execute(query, *args, **kwargs)","title":"ParseTreeDoc Column Types"},{"location":"legacy_documentation/doctable_picklefile/","text":"DocTable Example: Pickle and Text Files Here I show a bit about how to use picklefile and textfile column types. DocTable transparently handles saving and reading column data as separate files when data is large to improve performance of select queries. It will automatically create a folder in the same directory as your sqlite database and save or read file data as if you were working with a regular table entry. import os import sys sys.path.append('..') import doctable #tmp = doctable.TempFolder('./tmp') # will delete folder upon destruction import tempfile import pathlib fkasdfjlaj = tempfile.TemporaryDirectory() tmp = fkasdfjlaj.name # create column schema: each row corresponds to a pickle import dataclasses @doctable.schema(require_slots=False) class FileEntry: pic: list = doctable.Col(column_type='picklefile', type_kwargs=dict(folder=tmp)) idx: int = doctable.IDCol() db = doctable.DocTable(schema=FileEntry, target=':memory:') First we try inserting a basic object, where the data will be stored in a pickle file. We can see from the select statement that the data read/write is handled transparently by doctable. a = [1, 2, 3, 4, 5] db.insert(FileEntry(a)) db.select() # regular select using the picklefile datatype BINDING MF PARAMSSSSSSS PROCESSING MF PARAMSSSSSSS [FileEntry(pic=[1, 2, 3, 4, 5], idx=1)] We can also try turning off the transparent conversion, and instead retrieve the regular directory. with db['pic'].type.control: r = db.select() r PROCESSING MF PARAMSSSSSSS [FileEntry(pic=f'{tmp}/564814847383.pic', idx=1)] For performance reasons, DocTable never deletes stored file data unless you call the .clean_col_files() method directly. It will raise an exception if a referenced file is missing, and delete all files which are not referenced in the table. This is a costly function call, but a good way to make sure your database is 1-1 matched with your filesystem. # deletes files not in db and raise error if some db files not in filesystem db.clean_col_files('pic') PROCESSING MF PARAMSSSSSSS Now I create another DocTable with a changed fpath argument. Because the argument changed, DocTable will raise an exception when selecting or calling .clean_col_files() . Be wary of this!","title":"DocTable Example: Pickle and Text Files"},{"location":"legacy_documentation/doctable_picklefile/#doctable-example-pickle-and-text-files","text":"Here I show a bit about how to use picklefile and textfile column types. DocTable transparently handles saving and reading column data as separate files when data is large to improve performance of select queries. It will automatically create a folder in the same directory as your sqlite database and save or read file data as if you were working with a regular table entry. import os import sys sys.path.append('..') import doctable #tmp = doctable.TempFolder('./tmp') # will delete folder upon destruction import tempfile import pathlib fkasdfjlaj = tempfile.TemporaryDirectory() tmp = fkasdfjlaj.name # create column schema: each row corresponds to a pickle import dataclasses @doctable.schema(require_slots=False) class FileEntry: pic: list = doctable.Col(column_type='picklefile', type_kwargs=dict(folder=tmp)) idx: int = doctable.IDCol() db = doctable.DocTable(schema=FileEntry, target=':memory:') First we try inserting a basic object, where the data will be stored in a pickle file. We can see from the select statement that the data read/write is handled transparently by doctable. a = [1, 2, 3, 4, 5] db.insert(FileEntry(a)) db.select() # regular select using the picklefile datatype BINDING MF PARAMSSSSSSS PROCESSING MF PARAMSSSSSSS [FileEntry(pic=[1, 2, 3, 4, 5], idx=1)] We can also try turning off the transparent conversion, and instead retrieve the regular directory. with db['pic'].type.control: r = db.select() r PROCESSING MF PARAMSSSSSSS [FileEntry(pic=f'{tmp}/564814847383.pic', idx=1)] For performance reasons, DocTable never deletes stored file data unless you call the .clean_col_files() method directly. It will raise an exception if a referenced file is missing, and delete all files which are not referenced in the table. This is a costly function call, but a good way to make sure your database is 1-1 matched with your filesystem. # deletes files not in db and raise error if some db files not in filesystem db.clean_col_files('pic') PROCESSING MF PARAMSSSSSSS Now I create another DocTable with a changed fpath argument. Because the argument changed, DocTable will raise an exception when selecting or calling .clean_col_files() . Be wary of this!","title":"DocTable Example: Pickle and Text Files"},{"location":"legacy_documentation/doctable_schema/","text":"DocTable Schemas Your database table column names and types come from a schema class defined using the @doctable.schema decorator. In addition to providing a schema definition, this class can be used to encapsulate data when inserting or retrieving from the database. At its most basic, your schema class operates like a dataclass that uses slots for efficiency and allows for custom methods that will not affect the database schema. from datetime import datetime from pprint import pprint import pandas as pd import sys sys.path.append('..') import doctable Introduction This is an example of a basic doctable schema. Note the use of the decorator @doctable.schema , the inclusion of __slots__ = [] , and the type hints of the member variables - I will explain each of these later in this document. This class represents a database schema that includes two columns: name (an int ) and age (a str ). @doctable.schema class Record: __slots__ = [] name: str age: int The schema class definition is then provided to the doctable constructor to create the database table. Here we create an in-memory sqlite table and show the schema resulting from our custom class. Note that doctable automatically inferred that name should be a VARCHAR and age should be an INTEGER based on the provided type hints. # the schema that would result from this dataclass: table = doctable.DocTable(target=':memory:', schema=Record) table.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 name VARCHAR True None auto 0 1 age INTEGER True None auto 0 We can also use the schema class to insert data into our DocTable . We simply create a new Record and pass it to the DocTable.insert() method. Using .head() , we see the contents of the database so far. Note that you may also pass a dictionary to insert data - this is just one way of inserting data. new_record = Record(name='Devin Cornell', age=30) print(new_record) table.insert(new_record) table.head() Record(name='Devin Cornell', age=30) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name age 0 Devin Cornell 30 And perhaps more usefully, we can use it to encapsulate results from .select() queries. Note that the returned object is exactly the same as the one we put in. Slot classes are more memory-efficient than dictionaries for storing data, but there is cpu time overhead from inserting that data into the slots. first_record = table.select_first() print(first_record) Record(name='Devin Cornell', age=30) But, of course, the data can be returned in its raw format by passing the parameter as_dataclass=False . first_record = table.select_first(as_dataclass=False) print(first_record) ('Devin Cornell', 30) The doctable.schema Decorator The @doctable.schema decorator does the work to convert your custom class into a schema class. It transforms your schema class in three ways: create slots : First, slot variable names will be added to __slots__ automatically based on the fields in your class definition. This is why the default functionality requires you to add __slots__ = [] with no variable names. You may also turn slots off by passing require_slots=False to the decorator (i.e. @doctable.schema(require_slots=False) ), otherwise an exception will be raised. convert to dataclass : Second, your schema class will be converted to a dataclass that generates __init__ , __repr__ , and other boilerplate methods meant for classes that primarily store data. Any keyword arguments passed to the schema decorator, with the exception of require_slots , will be passed directly to the @dataclasses.dataclass decorator so you have control over the dataclass definition. inherit from DocTableSchema : Lastly, your schema class will inherit from doctable.DocTableSchema , which provides additional accessors that are used for storage in a DocTable and fine-grained control over retreived data. More on this later. Column names and types will be inferred from the type hints in your schema class definition. Because DocTable is built on sqlalchemy core , all fields will eventually be converted to sqlalchemy column objects and added to the DocTable metadata. This table shows the type mappings implemented in doctable: doctable.python_to_slqlchemy_type {int: sqlalchemy.sql.sqltypes.Integer, float: sqlalchemy.sql.sqltypes.Float, str: sqlalchemy.sql.sqltypes.String, bool: sqlalchemy.sql.sqltypes.Boolean, datetime.datetime: sqlalchemy.sql.sqltypes.DateTime, datetime.time: sqlalchemy.sql.sqltypes.Time, datetime.date: sqlalchemy.sql.sqltypes.Date, doctable.textmodels.parsetreedoc.ParseTreeDoc: doctable.schemas.custom_coltypes.ParseTreeDocFileType} For example, see this example of the most basic possible schema class that can be used to create a doctable. We use static defaulted parameters and type hints including str , int , datetime , and Any , which you can see are converted to VARCHAR , INTEGER , DATETIME , and BLOB column types, respectively. BLOB was used because the provided type hint Any has no entry in the above table. from typing import Any import datetime @doctable.schema class Record: __slots__ = [] name: str = None age: int = None time: datetime.datetime = None friends: Any = None # the schema that would result from this dataclass: doctable.DocTable(target=':memory:', schema=Record).schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 name VARCHAR True None auto 0 1 age INTEGER True None auto 0 2 time DATETIME True None auto 0 3 friends BLOB True None auto 0 You can see that this class operates much like a regular dataclass with slots. Thus, these defaulted parameters are applied in the constructor of the schema class, and NOT as the default value in the database schema. Record('Devin Cornell', 30) Record(name='Devin Cornell', age=30, time=None, friends=None) Use doctable.Col For More Control Over Schema Creation Using doctable.Col() as a default value in the schema class definition can give you more control over schema definitions. Firstly, this function returns a dataclass field object that can be used to set parameters like default_factory or compare as used by the dataclass. Pass arguments meant for field through the Col parameter field_kwargs=dict(..) . Other data passed to Col will be used to create the DocTable schema, which is stored as metadata inside the field . This example shows how Col can be used to set some parameters meant for field . These will affect your schema class behavior without affecting the produced DocTable schema. @doctable.schema class Record: __slots__ = [] name: str = doctable.Col() age: int = doctable.Col(field_kwargs=dict(default_factory=list, compare=True)) Record() Record(age=[]) Col also allows you to explicitly specify a column type using a string, sqlalchemy type definition, or sqlalchemy instance passed to column_type . You can then pass arguments meant for the sqlalchemy type constructor through type_kwargs . You may also use type_kwargs with the column type inferred from the type hint. import sqlalchemy @doctable.schema class Record: __slots__ = [] # providing only the type as first argument age: int = doctable.Col(sqlalchemy.BigInteger) # these are all quivalent name1: str = doctable.Col(type_kwargs=dict(length=100)) # infers type from type hint name2: str = doctable.Col(sqlalchemy.String, type_kwargs=dict(length=100)) # accepts provided type sqlalchemy.String, pass parameters through type_kwargs name3: str = doctable.Col(sqlalchemy.String(length=100)) # accepts type instance (no need for type_kwargs this way) name4: str = doctable.Col('string', type_kwargs=dict(length=100)) # the schema that would result from this dataclass: doctable.DocTable(target=':memory:', schema=Record).schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 age BIGINT True None auto 0 1 name1 VARCHAR(100) True None auto 0 2 name2 VARCHAR(100) True None auto 0 3 name3 VARCHAR(100) True None auto 0 4 name4 VARCHAR(100) True None auto 0 A full list of string -> sqlalchemy type mappings is shown below: doctable.string_to_sqlalchemy_type {'biginteger': sqlalchemy.sql.sqltypes.BigInteger, 'boolean': sqlalchemy.sql.sqltypes.Boolean, 'date': sqlalchemy.sql.sqltypes.Date, 'datetime': sqlalchemy.sql.sqltypes.DateTime, 'enum': sqlalchemy.sql.sqltypes.Enum, 'float': sqlalchemy.sql.sqltypes.Float, 'integer': sqlalchemy.sql.sqltypes.Integer, 'interval': sqlalchemy.sql.sqltypes.Interval, 'largebinary': sqlalchemy.sql.sqltypes.LargeBinary, 'numeric': sqlalchemy.sql.sqltypes.Numeric, 'smallinteger': sqlalchemy.sql.sqltypes.SmallInteger, 'string': sqlalchemy.sql.sqltypes.String, 'text': sqlalchemy.sql.sqltypes.Text, 'time': sqlalchemy.sql.sqltypes.Time, 'unicode': sqlalchemy.sql.sqltypes.Unicode, 'unicodetext': sqlalchemy.sql.sqltypes.UnicodeText, 'json': doctable.schemas.custom_coltypes.JSONType, 'pickle': doctable.schemas.custom_coltypes.CpickleType, 'parsetree': doctable.schemas.custom_coltypes.ParseTreeDocFileType, 'picklefile': doctable.schemas.custom_coltypes.PickleFileType, 'textfile': doctable.schemas.custom_coltypes.TextFileType} Finally, Col allows you to pass keyword arguments directly to the sqlalchemy Column constructor. This includes flags like primary_key or default , which are both used to construct the database schema but do not affect the python dataclass. Note that I recreated the classic id column below. @doctable.schema class Record: __slots__ = [] id: int = doctable.Col(primary_key=True, autoincrement=True) age: int = doctable.Col(nullable=False) name: str = doctable.Col(default='MISSING_NAME') # the schema that would result from this dataclass: doctable.DocTable(target=':memory:', schema=Record).schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 age INTEGER False None auto 0 2 name VARCHAR True None auto 0 I also included some shortcut Col functions like IDCol , AddedCol , and UpdatedCol - see below. import datetime @doctable.schema class Record: __slots__ = [] id: int = doctable.IDCol() # auto-increment primary key added: datetime.datetime = doctable.AddedCol() # record when row was added updated: datetime.datetime = doctable.UpdatedCol() # record when row was updated doctable.DocTable(target=':memory:', schema=Record).schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 added DATETIME True None auto 0 2 updated DATETIME True None auto 0 In this way, Col allows you to give fine-grained control to both the schema class behavior and the sql schema definition. Working With Schema Objects Using Col default parameters also has some additional side effects, primarily due to the inherited class DocTableSchema . Among other things, the Col method defines the default dataclass value to be a doctable.EmptyValue() object, which is essentially a placeholder for data that was not inserted into the class upon construction. The __repr__ defined in DocTableSchema dictates that member objects containing this value not appear when printing the class, and furthermore, member variables with the value EmptyValue() will not be provided in the database insertion. This means that the database schema is allowed to use its own default value - an effect which is most obviously useful when inserting an object that does not have an id or other automatically provided values. The example below shows the new_record.id contains EmptyValue() as a default, and that the id column is not included in the insert query - only name . @doctable.schema class Record: __slots__ = [] id: int = doctable.IDCol() name: str = doctable.Col() new_record = Record(name='Devin Cornell') print(new_record) try: print(new_record.id) except doctable.DataNotAvailableError: print(f'exception was raised') table = doctable.DocTable(target=':memory:', schema=Record, verbose=True) table.insert(new_record) table.head() Record(name='Devin Cornell') exception was raised DocTable: INSERT OR FAIL INTO _documents_ (name) VALUES (?) DocTable: SELECT _documents_.id, _documents_.name FROM _documents_ LIMIT ? OFFSET ? .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name 0 1 Devin Cornell Yet when we go to retrieve the inserted data, we can see that the value has been replaced by the defaulted value in the database. This is a useful feature if your pipeline involves the insertion of schema objects directly (as opposed to inserting dictionaries for each row). table.select_first(verbose=False) Record(id=1, name='Devin Cornell') The EmptyValue() feature is also useful when issuing select queries involving only a subset of columns. See here we run a select query where we just retrieve the name data, yet the result is still stored in a Record object. returned_record = table.select_first(['name'], verbose=False) print(returned_record) Record(name='Devin Cornell') To avoid working with EmptyValue() objects directly, it is recommended that you use the __getitem__ string subscripting to access column data. When using this subscript, the schema object will raise an exception if the returned value is an EmptyValue() . try: returned_record.id except doctable.DataNotAvailableError as e: print(e) The \"id\" property is not available. This might happen if you did not retrieve the information from a database or if you did not provide a value in the class constructor. Indices and Constraints Indices and constraints are provided to the DocTable constructor or definition, as it is not part of the schema class. Here I create custom schema and table definitions where the table has some defined indices and constraints. doctable.Index is really just a direct reference to sqlalchemy.Index , and doctable.Constraint is a mapping to an sqlalchemy constraint type, with the first argument indicating which one. @doctable.schema class Record: __slots__ = [] id: int = doctable.IDCol() name: str = doctable.Col() age: int = doctable.Col() class RecordTable(doctable.DocTable): _tabname_ = 'records' _schema_ = Record # table indices _indices_ = ( doctable.Index('name_index', 'name'), doctable.Index('name_age_index', 'name', 'age', unique=True), ) # table constraints _constraints_ = ( doctable.Constraint('unique', 'name', 'age', name='name_age_constraint'), doctable.Constraint('check', 'age > 0', name='check_age'), ) table = RecordTable(target=':memory:') And we can see that the constraints are working when we try to insert a record where age is less than 1. try: table.insert(Record(age=-1)) except sqlalchemy.exc.IntegrityError as e: print(e) (sqlite3.IntegrityError) CHECK constraint failed: check_age [SQL: INSERT OR FAIL INTO records (age) VALUES (?)] [parameters: (-1,)] (Background on this error at: http://sqlalche.me/e/13/gkpj) This is a full list of the mappings between constraint names and the associated sqlalchemy objects. doctable.constraint_lookup {'check': sqlalchemy.sql.schema.CheckConstraint, 'unique': sqlalchemy.sql.schema.UniqueConstraint, 'primarykey': sqlalchemy.sql.schema.PrimaryKeyConstraint, 'foreignkey': sqlalchemy.sql.schema.ForeignKeyConstraint} Conclusions In this guide, I tried to show some exmaples and give explanations for the ways that schema classes can be used to create doctables. The design is fairly efficent and flexible, and brings a more object-focused approach compared to raw sql queries without the overhead of ORM.","title":"DocTable Schemas"},{"location":"legacy_documentation/doctable_schema/#doctable-schemas","text":"Your database table column names and types come from a schema class defined using the @doctable.schema decorator. In addition to providing a schema definition, this class can be used to encapsulate data when inserting or retrieving from the database. At its most basic, your schema class operates like a dataclass that uses slots for efficiency and allows for custom methods that will not affect the database schema. from datetime import datetime from pprint import pprint import pandas as pd import sys sys.path.append('..') import doctable","title":"DocTable Schemas"},{"location":"legacy_documentation/doctable_schema/#introduction","text":"This is an example of a basic doctable schema. Note the use of the decorator @doctable.schema , the inclusion of __slots__ = [] , and the type hints of the member variables - I will explain each of these later in this document. This class represents a database schema that includes two columns: name (an int ) and age (a str ). @doctable.schema class Record: __slots__ = [] name: str age: int The schema class definition is then provided to the doctable constructor to create the database table. Here we create an in-memory sqlite table and show the schema resulting from our custom class. Note that doctable automatically inferred that name should be a VARCHAR and age should be an INTEGER based on the provided type hints. # the schema that would result from this dataclass: table = doctable.DocTable(target=':memory:', schema=Record) table.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 name VARCHAR True None auto 0 1 age INTEGER True None auto 0 We can also use the schema class to insert data into our DocTable . We simply create a new Record and pass it to the DocTable.insert() method. Using .head() , we see the contents of the database so far. Note that you may also pass a dictionary to insert data - this is just one way of inserting data. new_record = Record(name='Devin Cornell', age=30) print(new_record) table.insert(new_record) table.head() Record(name='Devin Cornell', age=30) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name age 0 Devin Cornell 30 And perhaps more usefully, we can use it to encapsulate results from .select() queries. Note that the returned object is exactly the same as the one we put in. Slot classes are more memory-efficient than dictionaries for storing data, but there is cpu time overhead from inserting that data into the slots. first_record = table.select_first() print(first_record) Record(name='Devin Cornell', age=30) But, of course, the data can be returned in its raw format by passing the parameter as_dataclass=False . first_record = table.select_first(as_dataclass=False) print(first_record) ('Devin Cornell', 30)","title":"Introduction"},{"location":"legacy_documentation/doctable_schema/#the-doctableschema-decorator","text":"The @doctable.schema decorator does the work to convert your custom class into a schema class. It transforms your schema class in three ways: create slots : First, slot variable names will be added to __slots__ automatically based on the fields in your class definition. This is why the default functionality requires you to add __slots__ = [] with no variable names. You may also turn slots off by passing require_slots=False to the decorator (i.e. @doctable.schema(require_slots=False) ), otherwise an exception will be raised. convert to dataclass : Second, your schema class will be converted to a dataclass that generates __init__ , __repr__ , and other boilerplate methods meant for classes that primarily store data. Any keyword arguments passed to the schema decorator, with the exception of require_slots , will be passed directly to the @dataclasses.dataclass decorator so you have control over the dataclass definition. inherit from DocTableSchema : Lastly, your schema class will inherit from doctable.DocTableSchema , which provides additional accessors that are used for storage in a DocTable and fine-grained control over retreived data. More on this later. Column names and types will be inferred from the type hints in your schema class definition. Because DocTable is built on sqlalchemy core , all fields will eventually be converted to sqlalchemy column objects and added to the DocTable metadata. This table shows the type mappings implemented in doctable: doctable.python_to_slqlchemy_type {int: sqlalchemy.sql.sqltypes.Integer, float: sqlalchemy.sql.sqltypes.Float, str: sqlalchemy.sql.sqltypes.String, bool: sqlalchemy.sql.sqltypes.Boolean, datetime.datetime: sqlalchemy.sql.sqltypes.DateTime, datetime.time: sqlalchemy.sql.sqltypes.Time, datetime.date: sqlalchemy.sql.sqltypes.Date, doctable.textmodels.parsetreedoc.ParseTreeDoc: doctable.schemas.custom_coltypes.ParseTreeDocFileType} For example, see this example of the most basic possible schema class that can be used to create a doctable. We use static defaulted parameters and type hints including str , int , datetime , and Any , which you can see are converted to VARCHAR , INTEGER , DATETIME , and BLOB column types, respectively. BLOB was used because the provided type hint Any has no entry in the above table. from typing import Any import datetime @doctable.schema class Record: __slots__ = [] name: str = None age: int = None time: datetime.datetime = None friends: Any = None # the schema that would result from this dataclass: doctable.DocTable(target=':memory:', schema=Record).schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 name VARCHAR True None auto 0 1 age INTEGER True None auto 0 2 time DATETIME True None auto 0 3 friends BLOB True None auto 0 You can see that this class operates much like a regular dataclass with slots. Thus, these defaulted parameters are applied in the constructor of the schema class, and NOT as the default value in the database schema. Record('Devin Cornell', 30) Record(name='Devin Cornell', age=30, time=None, friends=None)","title":"The doctable.schema Decorator"},{"location":"legacy_documentation/doctable_schema/#use-doctablecol-for-more-control-over-schema-creation","text":"Using doctable.Col() as a default value in the schema class definition can give you more control over schema definitions. Firstly, this function returns a dataclass field object that can be used to set parameters like default_factory or compare as used by the dataclass. Pass arguments meant for field through the Col parameter field_kwargs=dict(..) . Other data passed to Col will be used to create the DocTable schema, which is stored as metadata inside the field . This example shows how Col can be used to set some parameters meant for field . These will affect your schema class behavior without affecting the produced DocTable schema. @doctable.schema class Record: __slots__ = [] name: str = doctable.Col() age: int = doctable.Col(field_kwargs=dict(default_factory=list, compare=True)) Record() Record(age=[]) Col also allows you to explicitly specify a column type using a string, sqlalchemy type definition, or sqlalchemy instance passed to column_type . You can then pass arguments meant for the sqlalchemy type constructor through type_kwargs . You may also use type_kwargs with the column type inferred from the type hint. import sqlalchemy @doctable.schema class Record: __slots__ = [] # providing only the type as first argument age: int = doctable.Col(sqlalchemy.BigInteger) # these are all quivalent name1: str = doctable.Col(type_kwargs=dict(length=100)) # infers type from type hint name2: str = doctable.Col(sqlalchemy.String, type_kwargs=dict(length=100)) # accepts provided type sqlalchemy.String, pass parameters through type_kwargs name3: str = doctable.Col(sqlalchemy.String(length=100)) # accepts type instance (no need for type_kwargs this way) name4: str = doctable.Col('string', type_kwargs=dict(length=100)) # the schema that would result from this dataclass: doctable.DocTable(target=':memory:', schema=Record).schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 age BIGINT True None auto 0 1 name1 VARCHAR(100) True None auto 0 2 name2 VARCHAR(100) True None auto 0 3 name3 VARCHAR(100) True None auto 0 4 name4 VARCHAR(100) True None auto 0 A full list of string -> sqlalchemy type mappings is shown below: doctable.string_to_sqlalchemy_type {'biginteger': sqlalchemy.sql.sqltypes.BigInteger, 'boolean': sqlalchemy.sql.sqltypes.Boolean, 'date': sqlalchemy.sql.sqltypes.Date, 'datetime': sqlalchemy.sql.sqltypes.DateTime, 'enum': sqlalchemy.sql.sqltypes.Enum, 'float': sqlalchemy.sql.sqltypes.Float, 'integer': sqlalchemy.sql.sqltypes.Integer, 'interval': sqlalchemy.sql.sqltypes.Interval, 'largebinary': sqlalchemy.sql.sqltypes.LargeBinary, 'numeric': sqlalchemy.sql.sqltypes.Numeric, 'smallinteger': sqlalchemy.sql.sqltypes.SmallInteger, 'string': sqlalchemy.sql.sqltypes.String, 'text': sqlalchemy.sql.sqltypes.Text, 'time': sqlalchemy.sql.sqltypes.Time, 'unicode': sqlalchemy.sql.sqltypes.Unicode, 'unicodetext': sqlalchemy.sql.sqltypes.UnicodeText, 'json': doctable.schemas.custom_coltypes.JSONType, 'pickle': doctable.schemas.custom_coltypes.CpickleType, 'parsetree': doctable.schemas.custom_coltypes.ParseTreeDocFileType, 'picklefile': doctable.schemas.custom_coltypes.PickleFileType, 'textfile': doctable.schemas.custom_coltypes.TextFileType} Finally, Col allows you to pass keyword arguments directly to the sqlalchemy Column constructor. This includes flags like primary_key or default , which are both used to construct the database schema but do not affect the python dataclass. Note that I recreated the classic id column below. @doctable.schema class Record: __slots__ = [] id: int = doctable.Col(primary_key=True, autoincrement=True) age: int = doctable.Col(nullable=False) name: str = doctable.Col(default='MISSING_NAME') # the schema that would result from this dataclass: doctable.DocTable(target=':memory:', schema=Record).schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 age INTEGER False None auto 0 2 name VARCHAR True None auto 0 I also included some shortcut Col functions like IDCol , AddedCol , and UpdatedCol - see below. import datetime @doctable.schema class Record: __slots__ = [] id: int = doctable.IDCol() # auto-increment primary key added: datetime.datetime = doctable.AddedCol() # record when row was added updated: datetime.datetime = doctable.UpdatedCol() # record when row was updated doctable.DocTable(target=':memory:', schema=Record).schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 added DATETIME True None auto 0 2 updated DATETIME True None auto 0 In this way, Col allows you to give fine-grained control to both the schema class behavior and the sql schema definition.","title":"Use doctable.Col For More Control Over Schema Creation"},{"location":"legacy_documentation/doctable_schema/#working-with-schema-objects","text":"Using Col default parameters also has some additional side effects, primarily due to the inherited class DocTableSchema . Among other things, the Col method defines the default dataclass value to be a doctable.EmptyValue() object, which is essentially a placeholder for data that was not inserted into the class upon construction. The __repr__ defined in DocTableSchema dictates that member objects containing this value not appear when printing the class, and furthermore, member variables with the value EmptyValue() will not be provided in the database insertion. This means that the database schema is allowed to use its own default value - an effect which is most obviously useful when inserting an object that does not have an id or other automatically provided values. The example below shows the new_record.id contains EmptyValue() as a default, and that the id column is not included in the insert query - only name . @doctable.schema class Record: __slots__ = [] id: int = doctable.IDCol() name: str = doctable.Col() new_record = Record(name='Devin Cornell') print(new_record) try: print(new_record.id) except doctable.DataNotAvailableError: print(f'exception was raised') table = doctable.DocTable(target=':memory:', schema=Record, verbose=True) table.insert(new_record) table.head() Record(name='Devin Cornell') exception was raised DocTable: INSERT OR FAIL INTO _documents_ (name) VALUES (?) DocTable: SELECT _documents_.id, _documents_.name FROM _documents_ LIMIT ? OFFSET ? .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name 0 1 Devin Cornell Yet when we go to retrieve the inserted data, we can see that the value has been replaced by the defaulted value in the database. This is a useful feature if your pipeline involves the insertion of schema objects directly (as opposed to inserting dictionaries for each row). table.select_first(verbose=False) Record(id=1, name='Devin Cornell') The EmptyValue() feature is also useful when issuing select queries involving only a subset of columns. See here we run a select query where we just retrieve the name data, yet the result is still stored in a Record object. returned_record = table.select_first(['name'], verbose=False) print(returned_record) Record(name='Devin Cornell') To avoid working with EmptyValue() objects directly, it is recommended that you use the __getitem__ string subscripting to access column data. When using this subscript, the schema object will raise an exception if the returned value is an EmptyValue() . try: returned_record.id except doctable.DataNotAvailableError as e: print(e) The \"id\" property is not available. This might happen if you did not retrieve the information from a database or if you did not provide a value in the class constructor.","title":"Working With Schema Objects"},{"location":"legacy_documentation/doctable_schema/#indices-and-constraints","text":"Indices and constraints are provided to the DocTable constructor or definition, as it is not part of the schema class. Here I create custom schema and table definitions where the table has some defined indices and constraints. doctable.Index is really just a direct reference to sqlalchemy.Index , and doctable.Constraint is a mapping to an sqlalchemy constraint type, with the first argument indicating which one. @doctable.schema class Record: __slots__ = [] id: int = doctable.IDCol() name: str = doctable.Col() age: int = doctable.Col() class RecordTable(doctable.DocTable): _tabname_ = 'records' _schema_ = Record # table indices _indices_ = ( doctable.Index('name_index', 'name'), doctable.Index('name_age_index', 'name', 'age', unique=True), ) # table constraints _constraints_ = ( doctable.Constraint('unique', 'name', 'age', name='name_age_constraint'), doctable.Constraint('check', 'age > 0', name='check_age'), ) table = RecordTable(target=':memory:') And we can see that the constraints are working when we try to insert a record where age is less than 1. try: table.insert(Record(age=-1)) except sqlalchemy.exc.IntegrityError as e: print(e) (sqlite3.IntegrityError) CHECK constraint failed: check_age [SQL: INSERT OR FAIL INTO records (age) VALUES (?)] [parameters: (-1,)] (Background on this error at: http://sqlalche.me/e/13/gkpj) This is a full list of the mappings between constraint names and the associated sqlalchemy objects. doctable.constraint_lookup {'check': sqlalchemy.sql.schema.CheckConstraint, 'unique': sqlalchemy.sql.schema.UniqueConstraint, 'primarykey': sqlalchemy.sql.schema.PrimaryKeyConstraint, 'foreignkey': sqlalchemy.sql.schema.ForeignKeyConstraint}","title":"Indices and Constraints"},{"location":"legacy_documentation/doctable_schema/#conclusions","text":"In this guide, I tried to show some exmaples and give explanations for the ways that schema classes can be used to create doctables. The design is fairly efficent and flexible, and brings a more object-focused approach compared to raw sql queries without the overhead of ORM.","title":"Conclusions"},{"location":"legacy_documentation/doctable_schema_dataclass/","text":"DocTable Example: Schemas In this example, we show column specifications for each available type, as well as the sqlalchemy equivalents on which they were based. Note that . Each column in the schema passed to doctable is a 2+ tuple containing, in order, the column type, name, and arguments, and optionally the sqlalchemy type arguemnts. from datetime import datetime from pprint import pprint import pandas as pd import typing import sys sys.path.append('..') import doctable @doctable.schema class MyClass: __slots__ = [] # builtin column types idx: int = doctable.IDCol() # unique name name: str = doctable.Col(unique=True) # want to be the first ordered argument # special columns for added and updated updated: datetime = doctable.UpdatedCol() added: datetime = doctable.AddedCol() # custom column types lon: float = doctable.Col() lat: float = doctable.Col() # use Col to use factory to construct emtpy list # will be stored as binary/pickle type, since no other available elements: doctable.JSONType = doctable.Col(field_kwargs=dict(default_factory=list)) class MyTable(doctable.DocTable): _schema_ = MyClass # indices and constraints _indices = ( doctable.Index('lonlat_index', 'lon', 'lat', unique=True), doctable.Index('name_index', 'name'), ) _constraints_ = ( doctable.Constraint('check', 'lon > 0', name='check_lon'), doctable.Constraint('check', 'lat > 0'), ) md = MyTable(target=':memory:', verbose=True) #pprint(md.schemainfo) md.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 idx INTEGER False None auto 1 1 name VARCHAR True None auto 0 2 updated DATETIME True None auto 0 3 added DATETIME True None auto 0 4 lon FLOAT True None auto 0 5 lat FLOAT True None auto 0 6 elements VARCHAR True None auto 0","title":"DocTable Example: Schemas"},{"location":"legacy_documentation/doctable_schema_dataclass/#doctable-example-schemas","text":"In this example, we show column specifications for each available type, as well as the sqlalchemy equivalents on which they were based. Note that . Each column in the schema passed to doctable is a 2+ tuple containing, in order, the column type, name, and arguments, and optionally the sqlalchemy type arguemnts. from datetime import datetime from pprint import pprint import pandas as pd import typing import sys sys.path.append('..') import doctable @doctable.schema class MyClass: __slots__ = [] # builtin column types idx: int = doctable.IDCol() # unique name name: str = doctable.Col(unique=True) # want to be the first ordered argument # special columns for added and updated updated: datetime = doctable.UpdatedCol() added: datetime = doctable.AddedCol() # custom column types lon: float = doctable.Col() lat: float = doctable.Col() # use Col to use factory to construct emtpy list # will be stored as binary/pickle type, since no other available elements: doctable.JSONType = doctable.Col(field_kwargs=dict(default_factory=list)) class MyTable(doctable.DocTable): _schema_ = MyClass # indices and constraints _indices = ( doctable.Index('lonlat_index', 'lon', 'lat', unique=True), doctable.Index('name_index', 'name'), ) _constraints_ = ( doctable.Constraint('check', 'lon > 0', name='check_lon'), doctable.Constraint('check', 'lat > 0'), ) md = MyTable(target=':memory:', verbose=True) #pprint(md.schemainfo) md.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 idx INTEGER False None auto 1 1 name VARCHAR True None auto 0 2 updated DATETIME True None auto 0 3 added DATETIME True None auto 0 4 lon FLOAT True None auto 0 5 lat FLOAT True None auto 0 6 elements VARCHAR True None auto 0","title":"DocTable Example: Schemas"},{"location":"legacy_documentation/doctable_schema_legacy/","text":"DocTable Schemas There are two ways to define schemas for a DocTable: dataclass schema : column names and types come from a class created using the @doctable.schema decorator. This class represents a single row, and is returned by default when a select query is executed. doctable provides a thin layer over dataclasses with slots to reduce memory overhead from returned results. Custom methods can also be defined on the class that will not affect the database schema. When using this method, constraints and indices must be provided at the time of DocTable instantiation (or in the definition of an inheriting DocTable ). list schema : column names and types come from sequences of strings according to a custom doctable format. This method requires less knowledge of doctable objects but otherwise has no advantages over dataclass schemas. The doctable package builds on sqlalchemy, so both types of schema specifications ultimately result in a sequence of sqlalchemy column types that will be used to construct (or interface with) the database table. from datetime import datetime from pprint import pprint import pandas as pd import sys sys.path.append('..') import doctable Schema Type Mappings There are two lookup tables used to relate to sqlalchemy column types. The first is a map from Python datatypes to the sqlalchemy types. This is sufficient for the simplest possible dataclass schema specification. The second is a string lookup that is provided for the list schema format. You can see that this offers a larger number of types compared to the Python type conversion. There are several other custom column types I included for convenience. doctable.string_to_sqlalchemy_type {'biginteger': sqlalchemy.sql.sqltypes.BigInteger, 'boolean': sqlalchemy.sql.sqltypes.Boolean, 'date': sqlalchemy.sql.sqltypes.Date, 'datetime': sqlalchemy.sql.sqltypes.DateTime, 'enum': sqlalchemy.sql.sqltypes.Enum, 'float': sqlalchemy.sql.sqltypes.Float, 'integer': sqlalchemy.sql.sqltypes.Integer, 'interval': sqlalchemy.sql.sqltypes.Interval, 'largebinary': sqlalchemy.sql.sqltypes.LargeBinary, 'numeric': sqlalchemy.sql.sqltypes.Numeric, 'smallinteger': sqlalchemy.sql.sqltypes.SmallInteger, 'string': sqlalchemy.sql.sqltypes.String, 'text': sqlalchemy.sql.sqltypes.Text, 'time': sqlalchemy.sql.sqltypes.Time, 'unicode': sqlalchemy.sql.sqltypes.Unicode, 'unicodetext': sqlalchemy.sql.sqltypes.UnicodeText, 'json': doctable.schemas.custom_coltypes.JSONType, 'pickle': doctable.schemas.custom_coltypes.CpickleType, 'parsetree': doctable.schemas.custom_coltypes.ParseTreeDocFileType, 'picklefile': doctable.schemas.custom_coltypes.PickleFileType, 'textfile': doctable.schemas.custom_coltypes.TextFileType} List Schemas And this is another example showing the list schema format. schema = ( # standard id column #SQLAlchemy: Column('id', Integer, primary_key = True, autoincrement=True), ('integer', 'id', dict(primary_key=True, autoincrement=True)), # short form (can't provide any additional args though): ('idcol', 'id') # make a category column with two options: \"FICTION\" and \"NONFICTION\" #SQLAlchemy: Column('title', String,) ('string', 'category', dict(nullable=False)), # make a non-null title column #SQLAlchemy: Column('title', String,) ('string', 'title', dict(nullable=False)), # make an abstract where the default is an empty string instead of null #SQLAlchemy: Column('abstract', String, default='') ('string', 'abstract',dict(default='')), # make an age column where age must be greater than zero #SQLAlchemy: Column('abstract', Integer) ('integer', 'age'), # make a column that keeps track of column updates #SQLAlchemy: Column('updated_on', DateTime(), default=datetime.now, onupdate=datetime.now) ('datetime', 'updated_on', dict(default=datetime.now, onupdate=datetime.now)), # short form to auto-record update date: ('date_updated', 'updated_on') #SQLAlchemy: Column('updated_on', DateTime(), default=datetime.now) ('datetime', 'updated_on', dict(default=datetime.now)), # short form to auto-record insertion date: ('date_added', 'added_on') # make a string column with max of 500 characters #SQLAlchemy: Column('abstract', String, default='') ('string', 'text',dict(),dict(length=500)), ##### Custom DocTable Column Types ##### # uses json.dump to convert python object to json when storing and # json.load to convert json back to python when querying ('json','json_data'), # stores pickled python object directly in table as BLOB # TokensType and ParagraphsType are defined in doctable/coltypes.py # SQLAlchemy: Column('tokenized', TokensType), Column('sentencized', ParagraphsType) ('pickle','tokenized'), # store pickled data into a separate file, recording only filename directly in table # the 'fpath' argument can specify where the files should be placed, but by # default they are stored in <dbname>_<tablename>_<columnname> #('picklefile', 'pickle_obj', dict(), dict(fpath='folder_for_picklefiles')), # very similar to above, but use only when storing text data #('textfile', 'text_file'), # similar to above ##### Constraints ##### #SQLAlchemy: CheckConstraint('category in (\"FICTION\",\"NONFICTION\")', name='salary_check') ('check_constraint', 'category in (\"FICTION\",\"NONFICTION\")', dict(name='salary_check')), #SQLAlchemy: CheckConstraint('age > 0') ('check_constraint', 'age > 0'), # make sure each category/title entry is unique #SQLAlchemy: UniqueConstraint('category', 'title', name='work_key') ('unique_constraint', ['category','title'], dict(name='work_key')), # makes a foreign key from the 'subkey' column of this table to the 'id' # column of ANOTHERDOCTABLE, setting the SQL onupdate and ondelete foreign key constraints #('foreignkey_constraint', [['subkey'], [ANOTHERDOCTABLE['id']]], {}, dict(onupdate=\"CASCADE\", ondelete=\"CASCADE\")), #NOTE: Can't show here because we didn't make ANOTHERDOCTABLE ##### Indexes ###### # make index table # SQLAlchemy: Index('ind0', 'category', 'title', unique=True) ('index', 'ind0', ('category','title'),dict(unique=True)), ) md = doctable.DocTable(target=':memory:', schema=schema, verbose=True) md.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 category VARCHAR False None auto 0 2 title VARCHAR False None auto 0 3 abstract VARCHAR True None auto 0 4 age INTEGER True None auto 0 5 updated_on DATETIME True None auto 0 6 text VARCHAR(500) True None auto 0 7 json_data VARCHAR True None auto 0 8 tokenized BLOB True None auto 0","title":"DocTable Schemas"},{"location":"legacy_documentation/doctable_schema_legacy/#doctable-schemas","text":"There are two ways to define schemas for a DocTable: dataclass schema : column names and types come from a class created using the @doctable.schema decorator. This class represents a single row, and is returned by default when a select query is executed. doctable provides a thin layer over dataclasses with slots to reduce memory overhead from returned results. Custom methods can also be defined on the class that will not affect the database schema. When using this method, constraints and indices must be provided at the time of DocTable instantiation (or in the definition of an inheriting DocTable ). list schema : column names and types come from sequences of strings according to a custom doctable format. This method requires less knowledge of doctable objects but otherwise has no advantages over dataclass schemas. The doctable package builds on sqlalchemy, so both types of schema specifications ultimately result in a sequence of sqlalchemy column types that will be used to construct (or interface with) the database table. from datetime import datetime from pprint import pprint import pandas as pd import sys sys.path.append('..') import doctable","title":"DocTable Schemas"},{"location":"legacy_documentation/doctable_schema_legacy/#schema-type-mappings","text":"There are two lookup tables used to relate to sqlalchemy column types. The first is a map from Python datatypes to the sqlalchemy types. This is sufficient for the simplest possible dataclass schema specification. The second is a string lookup that is provided for the list schema format. You can see that this offers a larger number of types compared to the Python type conversion. There are several other custom column types I included for convenience. doctable.string_to_sqlalchemy_type {'biginteger': sqlalchemy.sql.sqltypes.BigInteger, 'boolean': sqlalchemy.sql.sqltypes.Boolean, 'date': sqlalchemy.sql.sqltypes.Date, 'datetime': sqlalchemy.sql.sqltypes.DateTime, 'enum': sqlalchemy.sql.sqltypes.Enum, 'float': sqlalchemy.sql.sqltypes.Float, 'integer': sqlalchemy.sql.sqltypes.Integer, 'interval': sqlalchemy.sql.sqltypes.Interval, 'largebinary': sqlalchemy.sql.sqltypes.LargeBinary, 'numeric': sqlalchemy.sql.sqltypes.Numeric, 'smallinteger': sqlalchemy.sql.sqltypes.SmallInteger, 'string': sqlalchemy.sql.sqltypes.String, 'text': sqlalchemy.sql.sqltypes.Text, 'time': sqlalchemy.sql.sqltypes.Time, 'unicode': sqlalchemy.sql.sqltypes.Unicode, 'unicodetext': sqlalchemy.sql.sqltypes.UnicodeText, 'json': doctable.schemas.custom_coltypes.JSONType, 'pickle': doctable.schemas.custom_coltypes.CpickleType, 'parsetree': doctable.schemas.custom_coltypes.ParseTreeDocFileType, 'picklefile': doctable.schemas.custom_coltypes.PickleFileType, 'textfile': doctable.schemas.custom_coltypes.TextFileType}","title":"Schema Type Mappings"},{"location":"legacy_documentation/doctable_schema_legacy/#list-schemas","text":"And this is another example showing the list schema format. schema = ( # standard id column #SQLAlchemy: Column('id', Integer, primary_key = True, autoincrement=True), ('integer', 'id', dict(primary_key=True, autoincrement=True)), # short form (can't provide any additional args though): ('idcol', 'id') # make a category column with two options: \"FICTION\" and \"NONFICTION\" #SQLAlchemy: Column('title', String,) ('string', 'category', dict(nullable=False)), # make a non-null title column #SQLAlchemy: Column('title', String,) ('string', 'title', dict(nullable=False)), # make an abstract where the default is an empty string instead of null #SQLAlchemy: Column('abstract', String, default='') ('string', 'abstract',dict(default='')), # make an age column where age must be greater than zero #SQLAlchemy: Column('abstract', Integer) ('integer', 'age'), # make a column that keeps track of column updates #SQLAlchemy: Column('updated_on', DateTime(), default=datetime.now, onupdate=datetime.now) ('datetime', 'updated_on', dict(default=datetime.now, onupdate=datetime.now)), # short form to auto-record update date: ('date_updated', 'updated_on') #SQLAlchemy: Column('updated_on', DateTime(), default=datetime.now) ('datetime', 'updated_on', dict(default=datetime.now)), # short form to auto-record insertion date: ('date_added', 'added_on') # make a string column with max of 500 characters #SQLAlchemy: Column('abstract', String, default='') ('string', 'text',dict(),dict(length=500)), ##### Custom DocTable Column Types ##### # uses json.dump to convert python object to json when storing and # json.load to convert json back to python when querying ('json','json_data'), # stores pickled python object directly in table as BLOB # TokensType and ParagraphsType are defined in doctable/coltypes.py # SQLAlchemy: Column('tokenized', TokensType), Column('sentencized', ParagraphsType) ('pickle','tokenized'), # store pickled data into a separate file, recording only filename directly in table # the 'fpath' argument can specify where the files should be placed, but by # default they are stored in <dbname>_<tablename>_<columnname> #('picklefile', 'pickle_obj', dict(), dict(fpath='folder_for_picklefiles')), # very similar to above, but use only when storing text data #('textfile', 'text_file'), # similar to above ##### Constraints ##### #SQLAlchemy: CheckConstraint('category in (\"FICTION\",\"NONFICTION\")', name='salary_check') ('check_constraint', 'category in (\"FICTION\",\"NONFICTION\")', dict(name='salary_check')), #SQLAlchemy: CheckConstraint('age > 0') ('check_constraint', 'age > 0'), # make sure each category/title entry is unique #SQLAlchemy: UniqueConstraint('category', 'title', name='work_key') ('unique_constraint', ['category','title'], dict(name='work_key')), # makes a foreign key from the 'subkey' column of this table to the 'id' # column of ANOTHERDOCTABLE, setting the SQL onupdate and ondelete foreign key constraints #('foreignkey_constraint', [['subkey'], [ANOTHERDOCTABLE['id']]], {}, dict(onupdate=\"CASCADE\", ondelete=\"CASCADE\")), #NOTE: Can't show here because we didn't make ANOTHERDOCTABLE ##### Indexes ###### # make index table # SQLAlchemy: Index('ind0', 'category', 'title', unique=True) ('index', 'ind0', ('category','title'),dict(unique=True)), ) md = doctable.DocTable(target=':memory:', schema=schema, verbose=True) md.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 category VARCHAR False None auto 0 2 title VARCHAR False None auto 0 3 abstract VARCHAR True None auto 0 4 age INTEGER True None auto 0 5 updated_on DATETIME True None auto 0 6 text VARCHAR(500) True None auto 0 7 json_data VARCHAR True None auto 0 8 tokenized BLOB True None auto 0","title":"List Schemas"},{"location":"legacy_documentation/doctable_select/","text":"DocTable Examples: Select Here I show how to select data from a DocTable. We cover object-oriented conditional selects emulating the WHERE SQL clause, as well as some reduce functions. import random import pandas as pd import numpy as np import sys sys.path.append('..') import doctable import dataclasses @doctable.schema class Record: __slots__ = [] id: int = doctable.IDCol() name: str = doctable.Col(nullable=False) age: int = None is_old: bool = None table = doctable.DocTable(target=':memory:', schema=Record, verbose=True) print(table) <DocTable (4 cols)::sqlite:///:memory::_documents_> N = 10 for i in range(N): age = random.random() # number in [0,1] is_old = age > 0.5 table.insert({'name':'user_'+str(i), 'age':age, 'is_old':is_old}, verbose=False) print(table) <DocTable (4 cols)::sqlite:///:memory::_documents_> /DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' Regular Selects These functions all return lists of ResultProxy objects. As such, they can be accessed using numerical indices or keyword indices. For instance, if one select output row is row=(1, 'user_0') (after selecting \"id\" and \"user\"), it can be accessed such that row[0]==row['id'] and row[1]==row['user'] . # the limit argument means the result will only return some rows. # I'll use it for convenience in these examples. # this selects all rows table.select(limit=2) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ LIMIT ? OFFSET ? /DataDrive/code/doctable/examples/../doctable/doctable.py:449: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') [Record(id=1, name='user_0', age=0.30111935823671676, is_old=False), Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)] table.select(['id','name'], limit=1) DocTable: SELECT _documents_.id, _documents_.name FROM _documents_ LIMIT ? OFFSET ? [Record(id=1, name='user_0', age=None, is_old=None)] # can also select by accessing the column object (db['id']) itself # this will be useful later with more complex queries table.select([table['id'],table['name']], limit=1) DocTable: SELECT _documents_.id, _documents_.name FROM _documents_ LIMIT ? OFFSET ? [Record(id=1, name='user_0', age=None, is_old=None)] table.select_first() DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ LIMIT ? OFFSET ? /DataDrive/code/doctable/examples/../doctable/doctable.py:427: UserWarning: Method .select_first() is depricated. Please use .q.select_first() instead. warnings.warn('Method .select_first() is depricated. Please use .q.select_first() instead.') Record(id=1, name='user_0', age=0.30111935823671676, is_old=False) table.select('name',limit=5) DocTable: SELECT _documents_.name FROM _documents_ LIMIT ? OFFSET ? ['user_0', 'user_1', 'user_2', 'user_3', 'user_4'] table.select_first('age') DocTable: SELECT _documents_.age FROM _documents_ LIMIT ? OFFSET ? Record(age=0.30111935823671676, is_old=None) Conditional Selects table.select(where=table['id']==2) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE _documents_.id = ? [Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)] table.select(where=table['id']<3) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE _documents_.id < ? [Record(id=1, name='user_0', age=0.30111935823671676, is_old=False), Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)] # mod operator works too table.select(where=(table['id']%2)==0, limit=2) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE _documents_.id % ? = ? LIMIT ? OFFSET ? [Record(id=2, name='user_1', age=0.7524872495613466, is_old=True), Record(id=4, name='user_3', age=0.9011039173289395, is_old=True)] # note parantheses to handle order of ops with overloaded bitwise ops table.select(where= (table['id']>=2) & (table['id']<=4) & (table['name']!='user_2')) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE _documents_.id >= ? AND _documents_.id <= ? AND _documents_.name != ? [Record(id=2, name='user_1', age=0.7524872495613466, is_old=True), Record(id=4, name='user_3', age=0.9011039173289395, is_old=True)] table.select(where=table['name'].in_(('user_2','user_3'))) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE _documents_.name IN (__[POSTCOMPILE_name_1]) [Record(id=3, name='user_2', age=0.33272186856831554, is_old=False), Record(id=4, name='user_3', age=0.9011039173289395, is_old=True)] table.select(where=table['id'].between(2,4)) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE _documents_.id BETWEEN ? AND ? [Record(id=2, name='user_1', age=0.7524872495613466, is_old=True), Record(id=3, name='user_2', age=0.33272186856831554, is_old=False), Record(id=4, name='user_3', age=0.9011039173289395, is_old=True)] # use of logical not operator \"~\" table.select(where= ~(table['name'].in_(('user_2','user_3'))) & (table['id'] < 4)) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE (_documents_.name NOT IN (__[POSTCOMPILE_name_1])) AND _documents_.id < ? [Record(id=1, name='user_0', age=0.30111935823671676, is_old=False), Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)] # more verbose operators .and_, .or_, and .not_ are bound to the doctable package table.select(where= doctable.f.or_(doctable.f.not_(table['id']==4)) & (table['id'] <= 2)) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE _documents_.id != ? AND _documents_.id <= ? [Record(id=1, name='user_0', age=0.30111935823671676, is_old=False), Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)] # now with simple computation ages = table.select(table['age']) mean_age = sum(ages)/len(ages) table.select(table['name'], where=table['age']>mean_age, limit=2) DocTable: SELECT _documents_.age FROM _documents_ DocTable: SELECT _documents_.name FROM _documents_ WHERE _documents_.age > ? LIMIT ? OFFSET ? ['user_1', 'user_3'] # apply .label() method to columns dict(table.select_first([table['age'].label('myage'), table['name'].label('myname')], as_dataclass=False)) DocTable: SELECT _documents_.age AS myage, _documents_.name AS myname FROM _documents_ LIMIT ? OFFSET ? /DataDrive/code/doctable/examples/../doctable/doctable.py:429: UserWarning: The \"as_dataclass\" parameter has been depricated: please set get_raw=True or select_raw to specify that you would like to retrieve a raw RowProxy pobject. warnings.warn(f'The \"as_dataclass\" parameter has been depricated: please set get_raw=True or ' {'myage': 0.30111935823671676, 'myname': 'user_0'} Column Operators I bind the .min, .max, .count, .sum, and .mode methods to the column objects. Additionally, I move the .count method to a separate DocTable2 method. # with labels now dict(table.select_first([table['age'].sum().label('sum'), table['age'].count().label('ct')], as_dataclass=False)) DocTable: SELECT sum(_documents_.age) AS sum, count(_documents_.age) AS ct FROM _documents_ LIMIT ? OFFSET ? {'sum': 4.99992719426638, 'ct': 10} table.select_first([table['age'].sum(), table['age'].count(), table['age']], as_dataclass=False) DocTable: SELECT sum(_documents_.age) AS sum_1, count(_documents_.age) AS count_1, _documents_.age FROM _documents_ LIMIT ? OFFSET ? (4.99992719426638, 10, 0.30111935823671676) ORDER BY, GROUP BY, LIMIT These additional arguments have also been provided. # the limit is obvious - it has been used throughout these examples table.select(limit=2) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ LIMIT ? OFFSET ? [Record(id=1, name='user_0', age=0.30111935823671676, is_old=False), Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)] table.select([table['is_old'], doctable.f.count()], groupby=table['is_old']) DocTable: SELECT _documents_.is_old, count(*) AS count_1 FROM _documents_ GROUP BY _documents_.is_old DocTable: SELECT _documents_.is_old, count(*) AS count_1 FROM _documents_ GROUP BY _documents_.is_old /DataDrive/code/doctable/examples/../doctable/doctable.py:464: UserWarning: Conversion from row to object failed according to the following error. Please use .q.select_raw() when requesting non-object formatted data such as counts or sums in the future. For now it is automatically converted. e=RowDataConversionFailed(\"Conversion from <class 'sqlalchemy.engine.row.LegacyRow'> to <class '__main__.Record'> failed.\") warnings.warn(f'Conversion from row to object failed according to the following ' [(False, 5), (True, 5)] # orderby clause table.select(orderby=table['age'].desc(), limit=2) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ ORDER BY _documents_.age DESC LIMIT ? OFFSET ? [Record(id=4, name='user_3', age=0.9011039173289395, is_old=True), Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)] # compound orderby table.select(orderby=(table['age'].desc(),table['is_old'].asc()), limit=2) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ ORDER BY _documents_.age DESC, _documents_.is_old ASC LIMIT ? OFFSET ? [Record(id=4, name='user_3', age=0.9011039173289395, is_old=True), Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)] f = doctable.f cols = [table['is_old'], f.count().label('ct')] table.q.select_raw(cols, groupby=table['is_old'], orderby=f.asc('ct')) DocTable: SELECT _documents_.is_old, count(*) AS ct FROM _documents_ GROUP BY _documents_.is_old ORDER BY ct ASC [(False, 5), (True, 5)] # can also use column name directly # can only use ascending and can use only one col table.select(orderby='age', limit=2) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ ORDER BY _documents_.age LIMIT ? OFFSET ? [Record(id=7, name='user_6', age=0.04850236983248746, is_old=False), Record(id=6, name='user_5', age=0.300309388680601, is_old=False)] # groupby clause # returns first row of each group without any aggregation functions table.select(groupby=table['is_old']) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ GROUP BY _documents_.is_old [Record(id=1, name='user_0', age=0.30111935823671676, is_old=False), Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)] # compound groupby (weird example bc name is unique - have only one cat var in this demo) table.select(groupby=(table['is_old'],table['name']), limit=3) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ GROUP BY _documents_.is_old, _documents_.name LIMIT ? OFFSET ? [Record(id=1, name='user_0', age=0.30111935823671676, is_old=False), Record(id=3, name='user_2', age=0.33272186856831554, is_old=False), Record(id=6, name='user_5', age=0.300309388680601, is_old=False)] # groupby clause using max aggregation function # gets match age for both old and young groups table.select(table['age'].max(), groupby=table['is_old']) DocTable: SELECT max(_documents_.age) AS max_1 FROM _documents_ GROUP BY _documents_.is_old [0.46166274965800924, 0.9011039173289395] SQL String Commands and Additional Clauses For cases where DocTable2 does not provide a convenient interface, you may submit raw SQL commands. These may be a bit more unwieldly, but they offer maximum flexibility. They may be used either as simply an addition to the WHERE or arbitrary end clauses, or accessed in totality. qstr = 'SELECT age,name FROM {} WHERE id==\"{}\"'.format(table.tabname, 1) results = table.execute(qstr) dict(list(results)[0]) DocTable: SELECT age,name FROM _documents_ WHERE id==\"1\" {'age': 0.30111935823671676, 'name': 'user_0'} wherestr = 'is_old==\"{}\"'.format('1') table.select(wherestr=wherestr, limit=2) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE (is_old==\"1\") LIMIT ? OFFSET ? [Record(id=2, name='user_1', age=0.7524872495613466, is_old=True), Record(id=4, name='user_3', age=0.9011039173289395, is_old=True)] # combine whrstr with structured query where clause wherestr = 'is_old==\"{}\"'.format('1') table.select(where=table['id']<=5, wherestr=wherestr) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE _documents_.id <= ? AND (is_old==\"1\") [Record(id=2, name='user_1', age=0.7524872495613466, is_old=True), Record(id=4, name='user_3', age=0.9011039173289395, is_old=True), Record(id=5, name='user_4', age=0.6092744222076869, is_old=True)] # combine whrstr with structured query where clause wherestr = 'is_old==\"{}\"'.format('1') table.select(where=table['id']<=5, wherestr=wherestr) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE _documents_.id <= ? AND (is_old==\"1\") [Record(id=2, name='user_1', age=0.7524872495613466, is_old=True), Record(id=4, name='user_3', age=0.9011039173289395, is_old=True), Record(id=5, name='user_4', age=0.6092744222076869, is_old=True)] Count Method and Get Next ID .count() is a convenience method. Mostly the same could be accomplished by db.select_first(db['id'].count()) , but this requires no reference to a specific column. .next_id() is especially useful if one hopes to enter the id (or any primary key column) into new rows manually. Especially useful because SQL engines don't provide new ids except when a single insert is performed. table.count() DocTable: SELECT count(_documents_.id) AS count_1 FROM _documents_ LIMIT ? OFFSET ? /DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead. warnings.warn('Method .count() is depricated. Please use .q.count() instead.') 10 table.count(table['age'] < 0.5) DocTable: SELECT count(_documents_.id) AS count_1 FROM _documents_ WHERE _documents_.age < ? LIMIT ? OFFSET ? 5 Select as Pandas Series and DataFrame These are especially useful when working with metadata because Pandas provides robust descriptive and plotting features than SQL alone. Good for generating sample information. # must provide only a single column table.select_series(table['age']).head(2) DocTable: SELECT _documents_.age FROM _documents_ 0 0.301119 1 0.752487 dtype: float64 table.select_series(table['age']).quantile([0.025, 0.985]) DocTable: SELECT _documents_.age FROM _documents_ 0.025 0.105159 0.985 0.881041 dtype: float64 table.select_df(['id','age']).head(2) DocTable: SELECT _documents_.id, _documents_.age FROM _documents_ /DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age 0 1 0.301119 1 2 0.752487 table.select_df('age').head(2) DocTable: SELECT _documents_.age FROM _documents_ /DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age 0 0.301119 1 0.752487 # must provide list of cols (even for one col) table.select_df([table['id'],table['age']]).corr() DocTable: SELECT _documents_.id, _documents_.age FROM _documents_ /DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age id 1.000000 0.006293 age 0.006293 1.000000 table.select_df([table['id'],table['age']]).describe().T DocTable: SELECT _documents_.id, _documents_.age FROM _documents_ /DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count mean std min 25% 50% 75% max id 10.0 5.500000 3.027650 1.000000 3.25000 5.500000 7.750000 10.000000 age 10.0 0.499993 0.256825 0.048502 0.30902 0.535469 0.659958 0.901104 mean_age = table.select_series(table['age']).mean() df = table.select_df([table['id'],table['age']]) df['old_grp'] = df['age'] > mean_age df.groupby('old_grp').describe() DocTable: SELECT _documents_.age FROM _documents_ DocTable: SELECT _documents_.id, _documents_.age FROM _documents_ /DataDrive/code/doctable/examples/../doctable/doctable.py:415: UserWarning: Method .select_series() is depricated. Please use .q.select_series() instead. warnings.warn('Method .select_series() is depricated. Please use .q.select_series() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } id age count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max old_grp False 5.0 5.0 2.915476 1.0 3.0 6.0 7.0 8.0 5.0 0.288863 0.149865 0.048502 0.300309 0.301119 0.332722 0.461663 True 5.0 6.0 3.391165 2.0 4.0 5.0 9.0 10.0 5.0 0.711122 0.120456 0.609274 0.619203 0.673543 0.752487 0.901104 # more complicated groupby aggregation. # calculates the variance both for entries above and below average age mean_age = table.select_series(table['age']).mean() df = table.select_df([table['name'],table['age']]) df['old_grp'] = df['age']>mean_age df.groupby('old_grp').agg(**{ 'first_name':pd.NamedAgg(column='name', aggfunc='first'), 'var_age':pd.NamedAgg(column='age', aggfunc=np.var), }) DocTable: SELECT _documents_.age FROM _documents_ DocTable: SELECT _documents_.name, _documents_.age FROM _documents_ /DataDrive/code/doctable/examples/../doctable/doctable.py:415: UserWarning: Method .select_series() is depricated. Please use .q.select_series() instead. warnings.warn('Method .select_series() is depricated. Please use .q.select_series() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name var_age old_grp False user_0 0.022459 True user_1 0.014510 Select with Buffer In cases where you have many rows or each row contains a lot of data, you may want to perform a select query which makes requests in chunks. This is performed using the SQL OFFSET command, and querying up to buffsize while yielding each returned row. This system is designed this way because the underlying sql engine buffers all rows retreived from a query, and thus there is no way to stream data into memory without this system. NOTE: The limit keyword is incompatible with this method - it will return all results. A workaround is to use the approx_max_rows param, which will return at minimum this number of rows, at max the specified number of rows plus buffsize. for row_chunk in table.select_chunks(chunksize=2, where=(table['id']%2)==0, verbose=False): print(row_chunk) [Record(id=2, name='user_1', age=0.7524872495613466, is_old=True), Record(id=4, name='user_3', age=0.9011039173289395, is_old=True)] [Record(id=6, name='user_5', age=0.300309388680601, is_old=False), Record(id=8, name='user_7', age=0.46166274965800924, is_old=False)] [Record(id=10, name='user_9', age=0.6192026652607745, is_old=True)] []","title":"DocTable Examples: Select"},{"location":"legacy_documentation/doctable_select/#doctable-examples-select","text":"Here I show how to select data from a DocTable. We cover object-oriented conditional selects emulating the WHERE SQL clause, as well as some reduce functions. import random import pandas as pd import numpy as np import sys sys.path.append('..') import doctable import dataclasses @doctable.schema class Record: __slots__ = [] id: int = doctable.IDCol() name: str = doctable.Col(nullable=False) age: int = None is_old: bool = None table = doctable.DocTable(target=':memory:', schema=Record, verbose=True) print(table) <DocTable (4 cols)::sqlite:///:memory::_documents_> N = 10 for i in range(N): age = random.random() # number in [0,1] is_old = age > 0.5 table.insert({'name':'user_'+str(i), 'age':age, 'is_old':is_old}, verbose=False) print(table) <DocTable (4 cols)::sqlite:///:memory::_documents_> /DataDrive/code/doctable/examples/../doctable/doctable.py:365: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:391: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or '","title":"DocTable Examples: Select"},{"location":"legacy_documentation/doctable_select/#regular-selects","text":"These functions all return lists of ResultProxy objects. As such, they can be accessed using numerical indices or keyword indices. For instance, if one select output row is row=(1, 'user_0') (after selecting \"id\" and \"user\"), it can be accessed such that row[0]==row['id'] and row[1]==row['user'] . # the limit argument means the result will only return some rows. # I'll use it for convenience in these examples. # this selects all rows table.select(limit=2) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ LIMIT ? OFFSET ? /DataDrive/code/doctable/examples/../doctable/doctable.py:449: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') [Record(id=1, name='user_0', age=0.30111935823671676, is_old=False), Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)] table.select(['id','name'], limit=1) DocTable: SELECT _documents_.id, _documents_.name FROM _documents_ LIMIT ? OFFSET ? [Record(id=1, name='user_0', age=None, is_old=None)] # can also select by accessing the column object (db['id']) itself # this will be useful later with more complex queries table.select([table['id'],table['name']], limit=1) DocTable: SELECT _documents_.id, _documents_.name FROM _documents_ LIMIT ? OFFSET ? [Record(id=1, name='user_0', age=None, is_old=None)] table.select_first() DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ LIMIT ? OFFSET ? /DataDrive/code/doctable/examples/../doctable/doctable.py:427: UserWarning: Method .select_first() is depricated. Please use .q.select_first() instead. warnings.warn('Method .select_first() is depricated. Please use .q.select_first() instead.') Record(id=1, name='user_0', age=0.30111935823671676, is_old=False) table.select('name',limit=5) DocTable: SELECT _documents_.name FROM _documents_ LIMIT ? OFFSET ? ['user_0', 'user_1', 'user_2', 'user_3', 'user_4'] table.select_first('age') DocTable: SELECT _documents_.age FROM _documents_ LIMIT ? OFFSET ? Record(age=0.30111935823671676, is_old=None)","title":"Regular Selects"},{"location":"legacy_documentation/doctable_select/#conditional-selects","text":"table.select(where=table['id']==2) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE _documents_.id = ? [Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)] table.select(where=table['id']<3) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE _documents_.id < ? [Record(id=1, name='user_0', age=0.30111935823671676, is_old=False), Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)] # mod operator works too table.select(where=(table['id']%2)==0, limit=2) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE _documents_.id % ? = ? LIMIT ? OFFSET ? [Record(id=2, name='user_1', age=0.7524872495613466, is_old=True), Record(id=4, name='user_3', age=0.9011039173289395, is_old=True)] # note parantheses to handle order of ops with overloaded bitwise ops table.select(where= (table['id']>=2) & (table['id']<=4) & (table['name']!='user_2')) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE _documents_.id >= ? AND _documents_.id <= ? AND _documents_.name != ? [Record(id=2, name='user_1', age=0.7524872495613466, is_old=True), Record(id=4, name='user_3', age=0.9011039173289395, is_old=True)] table.select(where=table['name'].in_(('user_2','user_3'))) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE _documents_.name IN (__[POSTCOMPILE_name_1]) [Record(id=3, name='user_2', age=0.33272186856831554, is_old=False), Record(id=4, name='user_3', age=0.9011039173289395, is_old=True)] table.select(where=table['id'].between(2,4)) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE _documents_.id BETWEEN ? AND ? [Record(id=2, name='user_1', age=0.7524872495613466, is_old=True), Record(id=3, name='user_2', age=0.33272186856831554, is_old=False), Record(id=4, name='user_3', age=0.9011039173289395, is_old=True)] # use of logical not operator \"~\" table.select(where= ~(table['name'].in_(('user_2','user_3'))) & (table['id'] < 4)) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE (_documents_.name NOT IN (__[POSTCOMPILE_name_1])) AND _documents_.id < ? [Record(id=1, name='user_0', age=0.30111935823671676, is_old=False), Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)] # more verbose operators .and_, .or_, and .not_ are bound to the doctable package table.select(where= doctable.f.or_(doctable.f.not_(table['id']==4)) & (table['id'] <= 2)) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE _documents_.id != ? AND _documents_.id <= ? [Record(id=1, name='user_0', age=0.30111935823671676, is_old=False), Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)] # now with simple computation ages = table.select(table['age']) mean_age = sum(ages)/len(ages) table.select(table['name'], where=table['age']>mean_age, limit=2) DocTable: SELECT _documents_.age FROM _documents_ DocTable: SELECT _documents_.name FROM _documents_ WHERE _documents_.age > ? LIMIT ? OFFSET ? ['user_1', 'user_3'] # apply .label() method to columns dict(table.select_first([table['age'].label('myage'), table['name'].label('myname')], as_dataclass=False)) DocTable: SELECT _documents_.age AS myage, _documents_.name AS myname FROM _documents_ LIMIT ? OFFSET ? /DataDrive/code/doctable/examples/../doctable/doctable.py:429: UserWarning: The \"as_dataclass\" parameter has been depricated: please set get_raw=True or select_raw to specify that you would like to retrieve a raw RowProxy pobject. warnings.warn(f'The \"as_dataclass\" parameter has been depricated: please set get_raw=True or ' {'myage': 0.30111935823671676, 'myname': 'user_0'}","title":"Conditional Selects"},{"location":"legacy_documentation/doctable_select/#column-operators","text":"I bind the .min, .max, .count, .sum, and .mode methods to the column objects. Additionally, I move the .count method to a separate DocTable2 method. # with labels now dict(table.select_first([table['age'].sum().label('sum'), table['age'].count().label('ct')], as_dataclass=False)) DocTable: SELECT sum(_documents_.age) AS sum, count(_documents_.age) AS ct FROM _documents_ LIMIT ? OFFSET ? {'sum': 4.99992719426638, 'ct': 10} table.select_first([table['age'].sum(), table['age'].count(), table['age']], as_dataclass=False) DocTable: SELECT sum(_documents_.age) AS sum_1, count(_documents_.age) AS count_1, _documents_.age FROM _documents_ LIMIT ? OFFSET ? (4.99992719426638, 10, 0.30111935823671676)","title":"Column Operators"},{"location":"legacy_documentation/doctable_select/#order-by-group-by-limit","text":"These additional arguments have also been provided. # the limit is obvious - it has been used throughout these examples table.select(limit=2) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ LIMIT ? OFFSET ? [Record(id=1, name='user_0', age=0.30111935823671676, is_old=False), Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)] table.select([table['is_old'], doctable.f.count()], groupby=table['is_old']) DocTable: SELECT _documents_.is_old, count(*) AS count_1 FROM _documents_ GROUP BY _documents_.is_old DocTable: SELECT _documents_.is_old, count(*) AS count_1 FROM _documents_ GROUP BY _documents_.is_old /DataDrive/code/doctable/examples/../doctable/doctable.py:464: UserWarning: Conversion from row to object failed according to the following error. Please use .q.select_raw() when requesting non-object formatted data such as counts or sums in the future. For now it is automatically converted. e=RowDataConversionFailed(\"Conversion from <class 'sqlalchemy.engine.row.LegacyRow'> to <class '__main__.Record'> failed.\") warnings.warn(f'Conversion from row to object failed according to the following ' [(False, 5), (True, 5)] # orderby clause table.select(orderby=table['age'].desc(), limit=2) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ ORDER BY _documents_.age DESC LIMIT ? OFFSET ? [Record(id=4, name='user_3', age=0.9011039173289395, is_old=True), Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)] # compound orderby table.select(orderby=(table['age'].desc(),table['is_old'].asc()), limit=2) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ ORDER BY _documents_.age DESC, _documents_.is_old ASC LIMIT ? OFFSET ? [Record(id=4, name='user_3', age=0.9011039173289395, is_old=True), Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)] f = doctable.f cols = [table['is_old'], f.count().label('ct')] table.q.select_raw(cols, groupby=table['is_old'], orderby=f.asc('ct')) DocTable: SELECT _documents_.is_old, count(*) AS ct FROM _documents_ GROUP BY _documents_.is_old ORDER BY ct ASC [(False, 5), (True, 5)] # can also use column name directly # can only use ascending and can use only one col table.select(orderby='age', limit=2) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ ORDER BY _documents_.age LIMIT ? OFFSET ? [Record(id=7, name='user_6', age=0.04850236983248746, is_old=False), Record(id=6, name='user_5', age=0.300309388680601, is_old=False)] # groupby clause # returns first row of each group without any aggregation functions table.select(groupby=table['is_old']) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ GROUP BY _documents_.is_old [Record(id=1, name='user_0', age=0.30111935823671676, is_old=False), Record(id=2, name='user_1', age=0.7524872495613466, is_old=True)] # compound groupby (weird example bc name is unique - have only one cat var in this demo) table.select(groupby=(table['is_old'],table['name']), limit=3) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ GROUP BY _documents_.is_old, _documents_.name LIMIT ? OFFSET ? [Record(id=1, name='user_0', age=0.30111935823671676, is_old=False), Record(id=3, name='user_2', age=0.33272186856831554, is_old=False), Record(id=6, name='user_5', age=0.300309388680601, is_old=False)] # groupby clause using max aggregation function # gets match age for both old and young groups table.select(table['age'].max(), groupby=table['is_old']) DocTable: SELECT max(_documents_.age) AS max_1 FROM _documents_ GROUP BY _documents_.is_old [0.46166274965800924, 0.9011039173289395]","title":"ORDER BY, GROUP BY, LIMIT"},{"location":"legacy_documentation/doctable_select/#sql-string-commands-and-additional-clauses","text":"For cases where DocTable2 does not provide a convenient interface, you may submit raw SQL commands. These may be a bit more unwieldly, but they offer maximum flexibility. They may be used either as simply an addition to the WHERE or arbitrary end clauses, or accessed in totality. qstr = 'SELECT age,name FROM {} WHERE id==\"{}\"'.format(table.tabname, 1) results = table.execute(qstr) dict(list(results)[0]) DocTable: SELECT age,name FROM _documents_ WHERE id==\"1\" {'age': 0.30111935823671676, 'name': 'user_0'} wherestr = 'is_old==\"{}\"'.format('1') table.select(wherestr=wherestr, limit=2) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE (is_old==\"1\") LIMIT ? OFFSET ? [Record(id=2, name='user_1', age=0.7524872495613466, is_old=True), Record(id=4, name='user_3', age=0.9011039173289395, is_old=True)] # combine whrstr with structured query where clause wherestr = 'is_old==\"{}\"'.format('1') table.select(where=table['id']<=5, wherestr=wherestr) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE _documents_.id <= ? AND (is_old==\"1\") [Record(id=2, name='user_1', age=0.7524872495613466, is_old=True), Record(id=4, name='user_3', age=0.9011039173289395, is_old=True), Record(id=5, name='user_4', age=0.6092744222076869, is_old=True)] # combine whrstr with structured query where clause wherestr = 'is_old==\"{}\"'.format('1') table.select(where=table['id']<=5, wherestr=wherestr) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ WHERE _documents_.id <= ? AND (is_old==\"1\") [Record(id=2, name='user_1', age=0.7524872495613466, is_old=True), Record(id=4, name='user_3', age=0.9011039173289395, is_old=True), Record(id=5, name='user_4', age=0.6092744222076869, is_old=True)]","title":"SQL String Commands and Additional Clauses"},{"location":"legacy_documentation/doctable_select/#count-method-and-get-next-id","text":".count() is a convenience method. Mostly the same could be accomplished by db.select_first(db['id'].count()) , but this requires no reference to a specific column. .next_id() is especially useful if one hopes to enter the id (or any primary key column) into new rows manually. Especially useful because SQL engines don't provide new ids except when a single insert is performed. table.count() DocTable: SELECT count(_documents_.id) AS count_1 FROM _documents_ LIMIT ? OFFSET ? /DataDrive/code/doctable/examples/../doctable/doctable.py:403: UserWarning: Method .count() is depricated. Please use .q.count() instead. warnings.warn('Method .count() is depricated. Please use .q.count() instead.') 10 table.count(table['age'] < 0.5) DocTable: SELECT count(_documents_.id) AS count_1 FROM _documents_ WHERE _documents_.age < ? LIMIT ? OFFSET ? 5","title":"Count Method and Get Next ID"},{"location":"legacy_documentation/doctable_select/#select-as-pandas-series-and-dataframe","text":"These are especially useful when working with metadata because Pandas provides robust descriptive and plotting features than SQL alone. Good for generating sample information. # must provide only a single column table.select_series(table['age']).head(2) DocTable: SELECT _documents_.age FROM _documents_ 0 0.301119 1 0.752487 dtype: float64 table.select_series(table['age']).quantile([0.025, 0.985]) DocTable: SELECT _documents_.age FROM _documents_ 0.025 0.105159 0.985 0.881041 dtype: float64 table.select_df(['id','age']).head(2) DocTable: SELECT _documents_.id, _documents_.age FROM _documents_ /DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age 0 1 0.301119 1 2 0.752487 table.select_df('age').head(2) DocTable: SELECT _documents_.age FROM _documents_ /DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age 0 0.301119 1 0.752487 # must provide list of cols (even for one col) table.select_df([table['id'],table['age']]).corr() DocTable: SELECT _documents_.id, _documents_.age FROM _documents_ /DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id age id 1.000000 0.006293 age 0.006293 1.000000 table.select_df([table['id'],table['age']]).describe().T DocTable: SELECT _documents_.id, _documents_.age FROM _documents_ /DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count mean std min 25% 50% 75% max id 10.0 5.500000 3.027650 1.000000 3.25000 5.500000 7.750000 10.000000 age 10.0 0.499993 0.256825 0.048502 0.30902 0.535469 0.659958 0.901104 mean_age = table.select_series(table['age']).mean() df = table.select_df([table['id'],table['age']]) df['old_grp'] = df['age'] > mean_age df.groupby('old_grp').describe() DocTable: SELECT _documents_.age FROM _documents_ DocTable: SELECT _documents_.id, _documents_.age FROM _documents_ /DataDrive/code/doctable/examples/../doctable/doctable.py:415: UserWarning: Method .select_series() is depricated. Please use .q.select_series() instead. warnings.warn('Method .select_series() is depricated. Please use .q.select_series() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } id age count mean std min 25% 50% 75% max count mean std min 25% 50% 75% max old_grp False 5.0 5.0 2.915476 1.0 3.0 6.0 7.0 8.0 5.0 0.288863 0.149865 0.048502 0.300309 0.301119 0.332722 0.461663 True 5.0 6.0 3.391165 2.0 4.0 5.0 9.0 10.0 5.0 0.711122 0.120456 0.609274 0.619203 0.673543 0.752487 0.901104 # more complicated groupby aggregation. # calculates the variance both for entries above and below average age mean_age = table.select_series(table['age']).mean() df = table.select_df([table['name'],table['age']]) df['old_grp'] = df['age']>mean_age df.groupby('old_grp').agg(**{ 'first_name':pd.NamedAgg(column='name', aggfunc='first'), 'var_age':pd.NamedAgg(column='age', aggfunc=np.var), }) DocTable: SELECT _documents_.age FROM _documents_ DocTable: SELECT _documents_.name, _documents_.age FROM _documents_ /DataDrive/code/doctable/examples/../doctable/doctable.py:415: UserWarning: Method .select_series() is depricated. Please use .q.select_series() instead. warnings.warn('Method .select_series() is depricated. Please use .q.select_series() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:420: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name var_age old_grp False user_0 0.022459 True user_1 0.014510","title":"Select as Pandas Series and DataFrame"},{"location":"legacy_documentation/doctable_select/#select-with-buffer","text":"In cases where you have many rows or each row contains a lot of data, you may want to perform a select query which makes requests in chunks. This is performed using the SQL OFFSET command, and querying up to buffsize while yielding each returned row. This system is designed this way because the underlying sql engine buffers all rows retreived from a query, and thus there is no way to stream data into memory without this system. NOTE: The limit keyword is incompatible with this method - it will return all results. A workaround is to use the approx_max_rows param, which will return at minimum this number of rows, at max the specified number of rows plus buffsize. for row_chunk in table.select_chunks(chunksize=2, where=(table['id']%2)==0, verbose=False): print(row_chunk) [Record(id=2, name='user_1', age=0.7524872495613466, is_old=True), Record(id=4, name='user_3', age=0.9011039173289395, is_old=True)] [Record(id=6, name='user_5', age=0.300309388680601, is_old=False), Record(id=8, name='user_7', age=0.46166274965800924, is_old=False)] [Record(id=10, name='user_9', age=0.6192026652607745, is_old=True)] []","title":"Select with Buffer"},{"location":"legacy_documentation/doctable_update/","text":"DocTable Examples: Update Here I show how to update data into a DocTable. In addition to providing updated values, DocTable also allows you to create map functions to transform existing data. import random import pandas as pd import numpy as np import sys sys.path.append('..') import doctable import dataclasses @doctable.schema class Record: __slots__ = [] id: int = doctable.IDCol() name: str = doctable.Col(nullable=False) age: int = None is_old: bool = None def new_db(): table = doctable.DocTable(schema=Record, target=':memory:', verbose=True) N = 10 for i in range(N): age = random.random() # number in [0,1] is_old = age > 0.5 table.insert({'name':'user_'+str(i), 'age':age, 'is_old':is_old}, verbose=False) return table table = new_db() print(table) <DocTable (4 cols)::sqlite:///:memory::_documents_> /DataDrive/code/doctable/examples/../doctable/doctable.py:324: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:350: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' table.select_df(limit=3) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ LIMIT ? OFFSET ? /DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 1 user_0 0.998030 True 1 2 user_1 0.210891 False 2 3 user_2 0.431233 False Single Update Update multiple (or single) rows with same values. table = new_db() table.select_df(where=table['is_old']==True, limit=3, verbose=False) /DataDrive/code/doctable/examples/../doctable/doctable.py:324: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:350: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 1 user_0 0.670833 True 1 2 user_1 0.895172 True 2 5 user_4 0.688209 True table = new_db() table.update({'age':1},where=table['is_old']==True) table.update({'age':0},where=table['is_old']==False) table.select_df(limit=3, verbose=False) DocTable: UPDATE _documents_ SET age=? WHERE _documents_.is_old = 1 DocTable: UPDATE _documents_ SET age=? WHERE _documents_.is_old = 0 /DataDrive/code/doctable/examples/../doctable/doctable.py:324: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:350: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:440: UserWarning: Method .update() is depricated. Please use .q.update() instead. warnings.warn('Method .update() is depricated. Please use .q.update() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 1 user_0 1 True 1 2 user_1 0 False 2 3 user_2 1 True Apply as Map Function This feature allows you to update columns based on the values of old columns. table = new_db() values = {table['name']:table['name']+'th', table['age']:table['age']+1, table['is_old']:True} table.update(values) table.select_df(limit=3, verbose=False) DocTable: UPDATE _documents_ SET name=(_documents_.name || ?), age=(_documents_.age + ?), is_old=? /DataDrive/code/doctable/examples/../doctable/doctable.py:324: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:350: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:440: UserWarning: Method .update() is depricated. Please use .q.update() instead. warnings.warn('Method .update() is depricated. Please use .q.update() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 1 user_0th 1.566417 True 1 2 user_1th 1.434875 True 2 3 user_2th 1.422777 True Apply as Set of Ordered Map Functions This is useful for when the updating of one column might change the value of another, depending on the order in which it was applied. table = new_db() values = [(table['name'],table['age']-1), (table['age'],table['age']+1),] table.update(values) table.select_df(limit=3, verbose=False) DocTable: UPDATE _documents_ SET name=(_documents_.age - ?), age=(_documents_.age + ?) /DataDrive/code/doctable/examples/../doctable/doctable.py:324: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:350: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:440: UserWarning: Method .update() is depricated. Please use .q.update() instead. warnings.warn('Method .update() is depricated. Please use .q.update() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 1 -0.823513491706054 1.176487 False 1 2 -0.567734080088791 1.432266 False 2 3 -0.838314843815808 1.161685 False Update Using SQL WHERE String table = new_db() table.update({'age':1.00}, wherestr='is_old==true') table.select_df(limit=5, verbose=False) DocTable: UPDATE _documents_ SET age=? WHERE is_old==true /DataDrive/code/doctable/examples/../doctable/doctable.py:324: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:350: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:440: UserWarning: Method .update() is depricated. Please use .q.update() instead. warnings.warn('Method .update() is depricated. Please use .q.update() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 1 user_0 0.488699 False 1 2 user_1 0.391556 False 2 3 user_2 1.000000 True 3 4 user_3 0.472176 False 4 5 user_4 0.154501 False","title":"DocTable Examples: Update"},{"location":"legacy_documentation/doctable_update/#doctable-examples-update","text":"Here I show how to update data into a DocTable. In addition to providing updated values, DocTable also allows you to create map functions to transform existing data. import random import pandas as pd import numpy as np import sys sys.path.append('..') import doctable import dataclasses @doctable.schema class Record: __slots__ = [] id: int = doctable.IDCol() name: str = doctable.Col(nullable=False) age: int = None is_old: bool = None def new_db(): table = doctable.DocTable(schema=Record, target=':memory:', verbose=True) N = 10 for i in range(N): age = random.random() # number in [0,1] is_old = age > 0.5 table.insert({'name':'user_'+str(i), 'age':age, 'is_old':is_old}, verbose=False) return table table = new_db() print(table) <DocTable (4 cols)::sqlite:///:memory::_documents_> /DataDrive/code/doctable/examples/../doctable/doctable.py:324: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:350: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' table.select_df(limit=3) DocTable: SELECT _documents_.id, _documents_.name, _documents_.age, _documents_.is_old FROM _documents_ LIMIT ? OFFSET ? /DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 1 user_0 0.998030 True 1 2 user_1 0.210891 False 2 3 user_2 0.431233 False","title":"DocTable Examples: Update"},{"location":"legacy_documentation/doctable_update/#single-update","text":"Update multiple (or single) rows with same values. table = new_db() table.select_df(where=table['is_old']==True, limit=3, verbose=False) /DataDrive/code/doctable/examples/../doctable/doctable.py:324: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:350: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 1 user_0 0.670833 True 1 2 user_1 0.895172 True 2 5 user_4 0.688209 True table = new_db() table.update({'age':1},where=table['is_old']==True) table.update({'age':0},where=table['is_old']==False) table.select_df(limit=3, verbose=False) DocTable: UPDATE _documents_ SET age=? WHERE _documents_.is_old = 1 DocTable: UPDATE _documents_ SET age=? WHERE _documents_.is_old = 0 /DataDrive/code/doctable/examples/../doctable/doctable.py:324: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:350: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:440: UserWarning: Method .update() is depricated. Please use .q.update() instead. warnings.warn('Method .update() is depricated. Please use .q.update() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 1 user_0 1 True 1 2 user_1 0 False 2 3 user_2 1 True","title":"Single Update"},{"location":"legacy_documentation/doctable_update/#apply-as-map-function","text":"This feature allows you to update columns based on the values of old columns. table = new_db() values = {table['name']:table['name']+'th', table['age']:table['age']+1, table['is_old']:True} table.update(values) table.select_df(limit=3, verbose=False) DocTable: UPDATE _documents_ SET name=(_documents_.name || ?), age=(_documents_.age + ?), is_old=? /DataDrive/code/doctable/examples/../doctable/doctable.py:324: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:350: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:440: UserWarning: Method .update() is depricated. Please use .q.update() instead. warnings.warn('Method .update() is depricated. Please use .q.update() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 1 user_0th 1.566417 True 1 2 user_1th 1.434875 True 2 3 user_2th 1.422777 True","title":"Apply as Map Function"},{"location":"legacy_documentation/doctable_update/#apply-as-set-of-ordered-map-functions","text":"This is useful for when the updating of one column might change the value of another, depending on the order in which it was applied. table = new_db() values = [(table['name'],table['age']-1), (table['age'],table['age']+1),] table.update(values) table.select_df(limit=3, verbose=False) DocTable: UPDATE _documents_ SET name=(_documents_.age - ?), age=(_documents_.age + ?) /DataDrive/code/doctable/examples/../doctable/doctable.py:324: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:350: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:440: UserWarning: Method .update() is depricated. Please use .q.update() instead. warnings.warn('Method .update() is depricated. Please use .q.update() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 1 -0.823513491706054 1.176487 False 1 2 -0.567734080088791 1.432266 False 2 3 -0.838314843815808 1.161685 False","title":"Apply as Set of Ordered Map Functions"},{"location":"legacy_documentation/doctable_update/#update-using-sql-where-string","text":"table = new_db() table.update({'age':1.00}, wherestr='is_old==true') table.select_df(limit=5, verbose=False) DocTable: UPDATE _documents_ SET age=? WHERE is_old==true /DataDrive/code/doctable/examples/../doctable/doctable.py:324: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:350: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:440: UserWarning: Method .update() is depricated. Please use .q.update() instead. warnings.warn('Method .update() is depricated. Please use .q.update() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:379: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id name age is_old 0 1 user_0 0.488699 False 1 2 user_1 0.391556 False 2 3 user_2 1.000000 True 3 4 user_3 0.472176 False 4 5 user_4 0.154501 False","title":"Update Using SQL WHERE String"},{"location":"legacy_documentation/example_nss_1_intro/","text":"Vignette 1: Storing Document Metadata In this example, I'll show how to create and manipulate two linked tables for storing document metadata using US National Security Strategy document metadata as an example. These are the vignettes I have created: 1: Storing Document Metadata 2: Storing Document Text 3: Storing Parsed Documents import sys sys.path.append('..') import doctable import spacy from tqdm import tqdm import pandas as pd import os from pprint import pprint import urllib.request # used for downloading nss docs # automatically clean up temp folder after python ends #tmpfolder = doctable.TempFolder('tmp') import tempfile tempdir = tempfile.TemporaryDirectory() tmpfolder = tempdir.name tmpfolder '/tmp/tmplxxguo16' Introduction to NSS Corpus This dataset is the plain text version of the US National Security Strategy documents. I compiled the metadata you see below from a page hosted by the historical dept of the secretary's office . In short, each US President must release at least one NSS per term, up to one per-year. This is the metadata we will be inserting into the table: # information about each NSS document document_metadata = [ {'year': 2000, 'party': 'D', 'president': 'Clinton'}, {'year': 2002, 'party': 'R', 'president': 'W. Bush'}, {'year': 2006, 'party': 'R', 'president': 'W. Bush'}, {'year': 2010, 'party': 'D', 'president': 'Obama'}, {'year': 2015, 'party': 'D', 'president': 'Obama'}, {'year': 2017, 'party': 'R', 'president': 'Trump'}, ] Create database schemas The first step will be to define a database schema that is appropriate for the data in document_metadata . We define an NSSDoc class to represent a single document. The doctable.schema decorator will convert the row objects into dataclasses with slots enabled, and inherit from doctable.DocTableRow to add some additional functionality. The type hints associated with each variable will be used in the schema definition for the new tables, and arguments to doctable.Col will mostly be passed to dataclasses.field (see docs for more detail), so all dataclass functionality is maintained. Also note that a method called .is_old() was defined. This method will not be included in a database schema, but I'll show later how it can be useful. # to be used as a database row representing a single NSS document @doctable.schema class NSSDoc: __slots__ = [] # include so that doctable.schema can create a slot class id: int = doctable.Col(primary_key=True, autoincrement=True) # can also use doctable.IDCol() as a shortcut year: int = None party: str = None president: str = None def is_old(self): '''Return whether the document is old or not.''' return self.year < 2010 We can see that these are regular dataclass methods because their constructors are defined. Note that the dataclass defaults the values to None, so take note of this when inserting or retrieving from a database. NSSDoc(year=1999) NSSDoc(year=1999, party=None, president=None) And we will also likely want to create a class that inherits from DocTable to statically define the table name, schema object, and any indices or constraints that should be associated with our table. We set the table name and the schema definition class using the reserved member variables _tabname_ and _schema_ , respectively. Note that the NSSDoc class is provided as the schema. We also can use this definition to create indices and constraints using the _indices_ and _constraints_ member variables. The indices are provided as name->columns pairs, and the constraints are tuples of the form (constraint_type, constraint_details) . In this case, we limit the values for check to R or D. class NSSDocTable(doctable.DocTable): _tabname_ = 'nss_documents' _schema_ = NSSDoc _indices_ = ( doctable.Index('party_index', 'party'), ) _constraints_ = ( doctable.Constraint('check', 'party in (\"R\", \"D\")'), # party can only take on values R or D. ) And then we create an instance of the NSSDocTable table using DocTable \\'s default constructor. We set target=f'{tmp}/nss_1.db' to indicate we want to access an sqlite database at that path. We also use the new_db=True to indicate that the database does not exist, so we should create a new one. fname = f'{tmpfolder}/nss_1.db' # clean up any old databases try: os.remove(fname) except: pass docs_table = NSSDocTable(target=fname, new_db=True) docs_table <__main__.NSSDocTable at 0x7f2a55b9b400> We can use .schema_table() to see information about the database schema. Note that doctable inferred column types based on the type hints. docs_table.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 year INTEGER True None auto 0 2 party VARCHAR True None auto 0 3 president VARCHAR True None auto 0 We are now ready to insert data into the new table. We simply add each document as a dictionary, and show the first n rows using .head() . docs_table.delete() # remove old entries if needed for doc in document_metadata: print(doc) docs_table.insert(doc) docs_table.head() {'year': 2000, 'party': 'D', 'president': 'Clinton'} {'year': 2002, 'party': 'R', 'president': 'W. Bush'} {'year': 2006, 'party': 'R', 'president': 'W. Bush'} {'year': 2010, 'party': 'D', 'president': 'Obama'} {'year': 2015, 'party': 'D', 'president': 'Obama'} {'year': 2017, 'party': 'R', 'president': 'Trump'} /DataDrive/code/doctable/examples/../doctable/doctable.py:494: UserWarning: Method .delete() is depricated. Please use .q.delete() instead. warnings.warn('Method .delete() is depricated. Please use .q.delete() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:364: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:390: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:407: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id year party president 0 1 2000 D Clinton 1 2 2002 R W. Bush 2 3 2006 R W. Bush 3 4 2010 D Obama 4 5 2015 D Obama We can verify that the constraint was defined by attempting to insert a row with an unknown party code. import sqlalchemy try: docs_table.insert({'party':'whateva'}) except sqlalchemy.exc.IntegrityError as e: print(e) /DataDrive/code/doctable/examples/../doctable/doctable.py:364: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:390: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' And we can use all the expected select (see select examples ) methods. democrats = docs_table.select(where=docs_table['party']=='D') democrats /DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') [NSSDoc(id=1, year=2000, party='D', president='Clinton'), NSSDoc(id=4, year=2010, party='D', president='Obama'), NSSDoc(id=5, year=2015, party='D', president='Obama')] clinton_doc = docs_table.select_first(where=docs_table['president']=='Clinton') clinton_doc /DataDrive/code/doctable/examples/../doctable/doctable.py:426: UserWarning: Method .select_first() is depricated. Please use .q.select_first() instead. warnings.warn('Method .select_first() is depricated. Please use .q.select_first() instead.') NSSDoc(id=1, year=2000, party='D', president='Clinton') Along with the methods we defined on the schema objects. clinton_doc.is_old() True Adding political party data Of course, relational database schemas often involve the use of more than one linked table. Now we'll attempt to integrate the data in party_metadata into our schema. # full name of party (we will use later) party_metadata = [ {'code': 'R', 'name': 'Republican'}, {'code': 'D', 'name': 'Democrat'}, ] First, we create the Party dataclass just as before. # to be used as a database row representing a single political party @doctable.schema class Party: __slots__ = [] id: int = doctable.Col(primary_key=True, autoincrement=True) # can also use doctable.IDCol() as a shortcut code: str = None name: str = None And then define a DocTable with a 'foreignkey' constraint that indicates it\\'s relationship to the document table. We can use the reference to the \"party\" column using nss_documents.party . class PartyTable(doctable.DocTable): _tabname_ = 'political_parties' _schema_ = Party _indices_ = { doctable.Index('code_index', 'code') } _constraints_ = ( doctable.Constraint('foreignkey', ('code',), ('nss_documents.party',)), ) party_table = PartyTable(target=fname) party_table <__main__.PartyTable at 0x7f2a55afa310> party_table.delete() # remove old entries if needed for party in party_metadata: print(party) party_table.insert(party) party_table.head() {'code': 'R', 'name': 'Republican'} {'code': 'D', 'name': 'Democrat'} /DataDrive/code/doctable/examples/../doctable/doctable.py:494: UserWarning: Method .delete() is depricated. Please use .q.delete() instead. warnings.warn('Method .delete() is depricated. Please use .q.delete() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:407: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id code name 0 1 R Republican 1 2 D Democrat Performing \"join\" select queries In contrast to sql, the type of join is inferred from the way the select query is used. Using a select method with columns for both tables will issue an outer join in lieu of other parameters. Also note that we must use as_dataclass to indicate the data should not use a dataclass for the results, since joined results includes fields from both party_table.select(['name', docs_table['president']], as_dataclass=False) /DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:445: UserWarning: The \"as_dataclass\" parameter has been depricated: please set get_raw=True or select_raw to specify that you would like to retrieve a raw RowProxy pobject. warnings.warn(f'The \"as_dataclass\" parameter has been depricated: please set get_raw=True or ' /DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: SELECT statement has a cartesian product between FROM element(s) \"nss_documents\" and FROM element \"political_parties\". Apply join condition(s) between each element to resolve. return self._engine.execute(query, *args, **kwargs) /DataDrive/code/doctable/examples/../doctable/doctable.py:453: UserWarning: Conversion from row to object failed according to the following error. Please use .q.select_raw() next time in the future to avoid this issue. e=RowDataConversionFailed(\"Conversion from <class 'sqlalchemy.engine.row.LegacyRow'> to <class '__main__.Party'> failed.\") warnings.warn(f'Conversion from row to object failed according to the following ' [('Republican', 'Clinton'), ('Republican', 'W. Bush'), ('Republican', 'W. Bush'), ('Republican', 'Obama'), ('Republican', 'Obama'), ('Republican', 'Trump'), ('Republican', None), ('Democrat', 'Clinton'), ('Democrat', 'W. Bush'), ('Democrat', 'W. Bush'), ('Democrat', 'Obama'), ('Democrat', 'Obama'), ('Democrat', 'Trump'), ('Democrat', None)] To perform an inner join, use a where conditional indicating the columns to be matched. docs_table.select(['year', 'president', party_table['name']], as_dataclass=False, where=docs_table['party']==party_table['code']) /DataDrive/code/doctable/examples/../doctable/doctable.py:453: UserWarning: Conversion from row to object failed according to the following error. Please use .q.select_raw() next time in the future to avoid this issue. e=RowDataConversionFailed(\"Conversion from <class 'sqlalchemy.engine.row.LegacyRow'> to <class '__main__.NSSDoc'> failed.\") warnings.warn(f'Conversion from row to object failed according to the following ' [(2000, 'Clinton', 'Democrat'), (2002, 'W. Bush', 'Republican'), (2006, 'W. Bush', 'Republican'), (2010, 'Obama', 'Democrat'), (2015, 'Obama', 'Democrat'), (2017, 'Trump', 'Republican')] And this works approximately the same when we switch the tables being selected. party_table.select(['code', 'name', docs_table['president']], as_dataclass=False, where=docs_table['party']==party_table['code']) [('R', 'Republican', 'Trump'), ('R', 'Republican', 'W. Bush'), ('R', 'Republican', 'W. Bush'), ('D', 'Democrat', 'Clinton'), ('D', 'Democrat', 'Obama'), ('D', 'Democrat', 'Obama')] And that is all for this vignette! See the list of vignettes at the top of this page for more examples.","title":"Vignette 1: Storing Document Metadata"},{"location":"legacy_documentation/example_nss_1_intro/#vignette-1-storing-document-metadata","text":"In this example, I'll show how to create and manipulate two linked tables for storing document metadata using US National Security Strategy document metadata as an example. These are the vignettes I have created: 1: Storing Document Metadata 2: Storing Document Text 3: Storing Parsed Documents import sys sys.path.append('..') import doctable import spacy from tqdm import tqdm import pandas as pd import os from pprint import pprint import urllib.request # used for downloading nss docs # automatically clean up temp folder after python ends #tmpfolder = doctable.TempFolder('tmp') import tempfile tempdir = tempfile.TemporaryDirectory() tmpfolder = tempdir.name tmpfolder '/tmp/tmplxxguo16'","title":"Vignette 1: Storing Document Metadata"},{"location":"legacy_documentation/example_nss_1_intro/#introduction-to-nss-corpus","text":"This dataset is the plain text version of the US National Security Strategy documents. I compiled the metadata you see below from a page hosted by the historical dept of the secretary's office . In short, each US President must release at least one NSS per term, up to one per-year. This is the metadata we will be inserting into the table: # information about each NSS document document_metadata = [ {'year': 2000, 'party': 'D', 'president': 'Clinton'}, {'year': 2002, 'party': 'R', 'president': 'W. Bush'}, {'year': 2006, 'party': 'R', 'president': 'W. Bush'}, {'year': 2010, 'party': 'D', 'president': 'Obama'}, {'year': 2015, 'party': 'D', 'president': 'Obama'}, {'year': 2017, 'party': 'R', 'president': 'Trump'}, ]","title":"Introduction to NSS Corpus"},{"location":"legacy_documentation/example_nss_1_intro/#create-database-schemas","text":"The first step will be to define a database schema that is appropriate for the data in document_metadata . We define an NSSDoc class to represent a single document. The doctable.schema decorator will convert the row objects into dataclasses with slots enabled, and inherit from doctable.DocTableRow to add some additional functionality. The type hints associated with each variable will be used in the schema definition for the new tables, and arguments to doctable.Col will mostly be passed to dataclasses.field (see docs for more detail), so all dataclass functionality is maintained. Also note that a method called .is_old() was defined. This method will not be included in a database schema, but I'll show later how it can be useful. # to be used as a database row representing a single NSS document @doctable.schema class NSSDoc: __slots__ = [] # include so that doctable.schema can create a slot class id: int = doctable.Col(primary_key=True, autoincrement=True) # can also use doctable.IDCol() as a shortcut year: int = None party: str = None president: str = None def is_old(self): '''Return whether the document is old or not.''' return self.year < 2010 We can see that these are regular dataclass methods because their constructors are defined. Note that the dataclass defaults the values to None, so take note of this when inserting or retrieving from a database. NSSDoc(year=1999) NSSDoc(year=1999, party=None, president=None) And we will also likely want to create a class that inherits from DocTable to statically define the table name, schema object, and any indices or constraints that should be associated with our table. We set the table name and the schema definition class using the reserved member variables _tabname_ and _schema_ , respectively. Note that the NSSDoc class is provided as the schema. We also can use this definition to create indices and constraints using the _indices_ and _constraints_ member variables. The indices are provided as name->columns pairs, and the constraints are tuples of the form (constraint_type, constraint_details) . In this case, we limit the values for check to R or D. class NSSDocTable(doctable.DocTable): _tabname_ = 'nss_documents' _schema_ = NSSDoc _indices_ = ( doctable.Index('party_index', 'party'), ) _constraints_ = ( doctable.Constraint('check', 'party in (\"R\", \"D\")'), # party can only take on values R or D. ) And then we create an instance of the NSSDocTable table using DocTable \\'s default constructor. We set target=f'{tmp}/nss_1.db' to indicate we want to access an sqlite database at that path. We also use the new_db=True to indicate that the database does not exist, so we should create a new one. fname = f'{tmpfolder}/nss_1.db' # clean up any old databases try: os.remove(fname) except: pass docs_table = NSSDocTable(target=fname, new_db=True) docs_table <__main__.NSSDocTable at 0x7f2a55b9b400> We can use .schema_table() to see information about the database schema. Note that doctable inferred column types based on the type hints. docs_table.schema_table() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 year INTEGER True None auto 0 2 party VARCHAR True None auto 0 3 president VARCHAR True None auto 0 We are now ready to insert data into the new table. We simply add each document as a dictionary, and show the first n rows using .head() . docs_table.delete() # remove old entries if needed for doc in document_metadata: print(doc) docs_table.insert(doc) docs_table.head() {'year': 2000, 'party': 'D', 'president': 'Clinton'} {'year': 2002, 'party': 'R', 'president': 'W. Bush'} {'year': 2006, 'party': 'R', 'president': 'W. Bush'} {'year': 2010, 'party': 'D', 'president': 'Obama'} {'year': 2015, 'party': 'D', 'president': 'Obama'} {'year': 2017, 'party': 'R', 'president': 'Trump'} /DataDrive/code/doctable/examples/../doctable/doctable.py:494: UserWarning: Method .delete() is depricated. Please use .q.delete() instead. warnings.warn('Method .delete() is depricated. Please use .q.delete() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:364: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:390: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:407: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id year party president 0 1 2000 D Clinton 1 2 2002 R W. Bush 2 3 2006 R W. Bush 3 4 2010 D Obama 4 5 2015 D Obama We can verify that the constraint was defined by attempting to insert a row with an unknown party code. import sqlalchemy try: docs_table.insert({'party':'whateva'}) except sqlalchemy.exc.IntegrityError as e: print(e) /DataDrive/code/doctable/examples/../doctable/doctable.py:364: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:390: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' And we can use all the expected select (see select examples ) methods. democrats = docs_table.select(where=docs_table['party']=='D') democrats /DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') [NSSDoc(id=1, year=2000, party='D', president='Clinton'), NSSDoc(id=4, year=2010, party='D', president='Obama'), NSSDoc(id=5, year=2015, party='D', president='Obama')] clinton_doc = docs_table.select_first(where=docs_table['president']=='Clinton') clinton_doc /DataDrive/code/doctable/examples/../doctable/doctable.py:426: UserWarning: Method .select_first() is depricated. Please use .q.select_first() instead. warnings.warn('Method .select_first() is depricated. Please use .q.select_first() instead.') NSSDoc(id=1, year=2000, party='D', president='Clinton') Along with the methods we defined on the schema objects. clinton_doc.is_old() True","title":"Create database schemas"},{"location":"legacy_documentation/example_nss_1_intro/#adding-political-party-data","text":"Of course, relational database schemas often involve the use of more than one linked table. Now we'll attempt to integrate the data in party_metadata into our schema. # full name of party (we will use later) party_metadata = [ {'code': 'R', 'name': 'Republican'}, {'code': 'D', 'name': 'Democrat'}, ] First, we create the Party dataclass just as before. # to be used as a database row representing a single political party @doctable.schema class Party: __slots__ = [] id: int = doctable.Col(primary_key=True, autoincrement=True) # can also use doctable.IDCol() as a shortcut code: str = None name: str = None And then define a DocTable with a 'foreignkey' constraint that indicates it\\'s relationship to the document table. We can use the reference to the \"party\" column using nss_documents.party . class PartyTable(doctable.DocTable): _tabname_ = 'political_parties' _schema_ = Party _indices_ = { doctable.Index('code_index', 'code') } _constraints_ = ( doctable.Constraint('foreignkey', ('code',), ('nss_documents.party',)), ) party_table = PartyTable(target=fname) party_table <__main__.PartyTable at 0x7f2a55afa310> party_table.delete() # remove old entries if needed for party in party_metadata: print(party) party_table.insert(party) party_table.head() {'code': 'R', 'name': 'Republican'} {'code': 'D', 'name': 'Democrat'} /DataDrive/code/doctable/examples/../doctable/doctable.py:494: UserWarning: Method .delete() is depricated. Please use .q.delete() instead. warnings.warn('Method .delete() is depricated. Please use .q.delete() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:407: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id code name 0 1 R Republican 1 2 D Democrat","title":"Adding political party data"},{"location":"legacy_documentation/example_nss_1_intro/#performing-join-select-queries","text":"In contrast to sql, the type of join is inferred from the way the select query is used. Using a select method with columns for both tables will issue an outer join in lieu of other parameters. Also note that we must use as_dataclass to indicate the data should not use a dataclass for the results, since joined results includes fields from both party_table.select(['name', docs_table['president']], as_dataclass=False) /DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') /DataDrive/code/doctable/examples/../doctable/doctable.py:445: UserWarning: The \"as_dataclass\" parameter has been depricated: please set get_raw=True or select_raw to specify that you would like to retrieve a raw RowProxy pobject. warnings.warn(f'The \"as_dataclass\" parameter has been depricated: please set get_raw=True or ' /DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: SELECT statement has a cartesian product between FROM element(s) \"nss_documents\" and FROM element \"political_parties\". Apply join condition(s) between each element to resolve. return self._engine.execute(query, *args, **kwargs) /DataDrive/code/doctable/examples/../doctable/doctable.py:453: UserWarning: Conversion from row to object failed according to the following error. Please use .q.select_raw() next time in the future to avoid this issue. e=RowDataConversionFailed(\"Conversion from <class 'sqlalchemy.engine.row.LegacyRow'> to <class '__main__.Party'> failed.\") warnings.warn(f'Conversion from row to object failed according to the following ' [('Republican', 'Clinton'), ('Republican', 'W. Bush'), ('Republican', 'W. Bush'), ('Republican', 'Obama'), ('Republican', 'Obama'), ('Republican', 'Trump'), ('Republican', None), ('Democrat', 'Clinton'), ('Democrat', 'W. Bush'), ('Democrat', 'W. Bush'), ('Democrat', 'Obama'), ('Democrat', 'Obama'), ('Democrat', 'Trump'), ('Democrat', None)] To perform an inner join, use a where conditional indicating the columns to be matched. docs_table.select(['year', 'president', party_table['name']], as_dataclass=False, where=docs_table['party']==party_table['code']) /DataDrive/code/doctable/examples/../doctable/doctable.py:453: UserWarning: Conversion from row to object failed according to the following error. Please use .q.select_raw() next time in the future to avoid this issue. e=RowDataConversionFailed(\"Conversion from <class 'sqlalchemy.engine.row.LegacyRow'> to <class '__main__.NSSDoc'> failed.\") warnings.warn(f'Conversion from row to object failed according to the following ' [(2000, 'Clinton', 'Democrat'), (2002, 'W. Bush', 'Republican'), (2006, 'W. Bush', 'Republican'), (2010, 'Obama', 'Democrat'), (2015, 'Obama', 'Democrat'), (2017, 'Trump', 'Republican')] And this works approximately the same when we switch the tables being selected. party_table.select(['code', 'name', docs_table['president']], as_dataclass=False, where=docs_table['party']==party_table['code']) [('R', 'Republican', 'Trump'), ('R', 'Republican', 'W. Bush'), ('R', 'Republican', 'W. Bush'), ('D', 'Democrat', 'Clinton'), ('D', 'Democrat', 'Obama'), ('D', 'Democrat', 'Obama')] And that is all for this vignette! See the list of vignettes at the top of this page for more examples.","title":"Performing \"join\" select queries"},{"location":"legacy_documentation/example_nss_2_parsing/","text":"Vignette 2: Storing Document Text In this example, I'll show how to create a database for document text + metadata storage using the DocTable class, and a parser class using a ParsePipeline . We will store the metadata you see below with the raw text and parsed tokens in the same DocTable. These are the vignettes I have created: 1: Storing Document Metadata 2: Storing Document Text 3: Storing Parsed Documents import sys sys.path.append('..') import doctable import spacy from tqdm import tqdm import pandas as pd import os from pprint import pprint import urllib.request # used for downloading nss docs # automatically clean up temp folder after python ends import tempfile tempdir = tempfile.TemporaryDirectory() tmpfolder = tempdir.name tmpfolder '/tmp/tmpm_ciuemc' Introduction to NSS Corpus This dataset is the plain text version of the US National Security Strategy documents. During the parsing process, all plain text files will be downloaded from my github project hosting the nss docs . I compiled the metadata you see below from a page hosted by the historical dept of the secretary's office . In short, each US President must release at least one NSS per term, with some (namely Clinton) producing more. I've defined the document metadata as nss_metadata , which contains the year (which I used to make the url), the president name, and the political party they belong to. We will later use download_nss() to actually download the text and store it into the database. def download_nss(year): ''' Simple helper function for downloading texts from my nssdocs repo.''' baseurl = 'https://raw.githubusercontent.com/devincornell/nssdocs/master/docs/{}.txt' url = baseurl.format(year) text = urllib.request.urlopen(url).read().decode('utf-8') return text document_metadata = [ {'year': 2000, 'party': 'D', 'president': 'Clinton'}, {'year': 2002, 'party': 'R', 'president': 'W. Bush'}, {'year': 2006, 'party': 'R', 'president': 'W. Bush'}, {'year': 2010, 'party': 'D', 'president': 'Obama'}, {'year': 2015, 'party': 'D', 'president': 'Obama'}, {'year': 2017, 'party': 'R', 'president': 'Trump'}, ] # downloader example: first 100 characters of 1993 NSS document text = download_nss(1993) text[:100] 'Preface \\n\\nAmerican Leadership for Peaceful Change \\n\\nOur great Nation stands at a crossroads in histo' 1. Create a Table Schema The first step will be to define a database schema that is appropriate for the data in document_metadata . We define an NSSDoc class to represent a single document. The doctable.schema decorator will convert the row objects into dataclasses with slots enabled, and inherit from doctable.DocTableRow to add some additional functionality. The type hints associated with each variable will be used in the schema definition for the new tables, and arguments to doctable.Col will mostly be passed to dataclasses.field (see docs for more detail), so all dataclass functionality is maintained. See the schema guide for examples of the full range of column types. from typing import Any # import generic type hint # to be used as a database row representing a single NSS document @doctable.schema class NSSDoc: __slots__ = [] # include so that doctable.schema can create a slot class id: int = doctable.IDCol() # this is an alias for doctable.Col(primary_key=True, autoincrement=True) year: int = doctable.Col() party: str = doctable.Col() president: str = doctable.Col() text: str = doctable.Col() tokens: Any = doctable.Col() # this will be used as a binary type that stores pickled data @property def num_tokens(self): return len(self.tokens) def paragraphs(self): return self.text.split('\\n\\n') 2. Define a Custom DocTable Now we define a class called NSSDocTable to represent the database table. This table must inherit from DocTable and will store connection and schema information. The DocTable class is often used by subclassing. Our NSSDocs class inherits from DocTable and will store connection and schema information. Because the default constructor checks for statically define member variables tabname and schema (as well as others), we can simply add them to the class definition. We also can use this definition to create indices and constraints using the _indices_ and _constraints_ member variables. The indices are provided as name->columns pairs, and the constraints are tuples of the form (constraint_type, constraint_details) . In this case, we limit the values for check to R or D. class NSSDocTable(doctable.DocTable): _tabname_ = 'nss_documents' _schema_ = NSSDoc _indices_ = ( doctable.Index('party_index', 'party'), ) _constraints_ = ( doctable.Constraint('check', 'party in (\"R\", \"D\")'), ) We can then create a connection to a database by instantiating the NSSDocTable class. We used target=':memory:' to indicate that the sqlite table should be created in-memory. # printing the DocTable object itself shows how many entries there are nss_table = NSSDocTable(target=':memory:') print(nss_table.count()) print(nss_table) nss_table.schema_table() 0 <NSSDocTable (6 cols)::sqlite:///:memory::nss_documents> /DataDrive/code/doctable/examples/../doctable/doctable.py:402: UserWarning: Method .count() is depricated. Please use .q.count() instead. warnings.warn('Method .count() is depricated. Please use .q.count() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 year INTEGER True None auto 0 2 party VARCHAR True None auto 0 3 president VARCHAR True None auto 0 4 text VARCHAR True None auto 0 5 tokens BLOB True None auto 0 2. Insert Data Into the Table Now let's download and store the text into the database. Each loop downloads a text document and inserts it into the doctable, and we use the .insert() method to insert a single row at a time. The row to be inserted is represented as a dictionary, and any missing column information is left as NULL. The ifnotunique argument is set to false because if we were to re-run this code, it needs to replace the existing document of the same year. Recall that in the schema we placed a unique constraint on the year column. for docmeta in tqdm(document_metadata): text = download_nss(docmeta['year']) nss_table.insert({**docmeta, **{'text': text}}, ifnotunique='replace') nss_table.head() 0%| | 0/6 [00:00<?, ?it/s]/DataDrive/code/doctable/examples/../doctable/doctable.py:364: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:390: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:00<00:00, 12.32it/s] /DataDrive/code/doctable/examples/../doctable/doctable.py:407: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id year party president text tokens 0 1 2000 D Clinton As we enter the new millennium, we are blessed... None 1 2 2002 R W. Bush The great struggles of the twentieth century b... None 2 3 2006 R W. Bush My fellow Americans, \\n\\nAmerica is at war. Th... None 3 4 2010 D Obama Time and again in our Nation's history, Americ... None 4 5 2015 D Obama Today, the United States is stronger and bette... None 3. Query Table Data Now that we have inserted the NSS documents into the table, there are a few ways we can query the data. To select the first entry of the table use .select_first() . This method returns a simple sqlalchemy.RowProxy object which can be accessed like a dictionary or like a tuple. row = nss_table.select_first(['id', 'year', 'party', 'president']) print(row) print(row.president) /DataDrive/code/doctable/examples/../doctable/doctable.py:426: UserWarning: Method .select_first() is depricated. Please use .q.select_first() instead. warnings.warn('Method .select_first() is depricated. Please use .q.select_first() instead.') NSSDoc(id=1, year=2000, party='D', president='Clinton') Clinton To select more than one row, use the .select() method. If you'd only like to return the first few rows, you can use the limit argument. rows = nss_table.select(limit=2) print(rows[0].year) print(rows[1].year) 2000 2002 /DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') We can also select only a few columns. nss_table.select(['year', 'president'], limit=3) [NSSDoc(year=2000, president='Clinton'), NSSDoc(year=2002, president='W. Bush'), NSSDoc(year=2006, president='W. Bush')] For convenience, we can also use the .select_df() method to return directly as a pandas dataframe. # use select_df to show a couple rows of our database nss_table.select_df(limit=2) /DataDrive/code/doctable/examples/../doctable/doctable.py:419: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id year party president text tokens 0 1 2000 D Clinton As we enter the new millennium, we are blessed... None 1 2 2002 R W. Bush The great struggles of the twentieth century b... None And access the .paragraphs() method we defined in NSSDoc . for row in nss_table.select(limit=3): print(f\"{row.president} ({row.year}): num_paragraphs={len(row.paragraphs())}\") Clinton (2000): num_paragraphs=569 W. Bush (2002): num_paragraphs=199 W. Bush (2006): num_paragraphs=474 /DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') 4. Create a Parser for Tokenization Now that the text is in the doctable, we can extract it using .select() , parse it, and store the parsed text back into the table using .update() . Now we create a parser using ParsePipeline and a list of functions to apply to the text sequentially. The Comp function returns a doctable parse function with additional keyword arguments. For instance, the following two expressions would be the same. doctable.Comp('keep_tok', keep_punct=True) # is equivalent to lambda x: doctable.parse.parse_tok_func(x, keep_punct=True) Note in this example that the 'tokenize' function takes two function arguments: keep_tok_func and parse_tok_func , which are also specified using the .Comp() function. The available pipeline components are listed in the parse function documentation . # add pipeline components parser = doctable.ParsePipeline([ spacy.load('en_core_web_sm'), # load a spacy parser as first step in pipeline doctable.Comp('tokenize', **{ 'split_sents': False, 'keep_tok_func': doctable.Comp('keep_tok'), 'parse_tok_func': doctable.Comp('parse_tok'), }) ]) parser.components [<spacy.lang.en.English at 0x7f37ce907f70>, functools.partial(<function tokenize at 0x7f37d3d74e50>, split_sents=False, keep_tok_func=functools.partial(<function keep_tok at 0x7f37d3d74f70>), parse_tok_func=functools.partial(<function parse_tok at 0x7f37d3d74ee0>))] Now we loop through rows in the doctable and for each iteration parse the text and insert it back into the table using .update() . We use the ParsePipeline method .parsemany() to parse paragraphs from each document in parallel. for doc in tqdm(nss_table.select(['id','year','text'])): paragraphs = parser.parsemany(doc.text.split('\\n\\n'), workers=30) # parse paragraphs in parallel nss_table.update({'tokens': [t for p in paragraphs for t in p]}, where=nss_table['id']==doc.id) 0%| | 0/6 [00:00<?, ?it/s]/DataDrive/code/doctable/examples/../doctable/doctable.py:489: UserWarning: Method .update() is depricated. Please use .q.update() instead. warnings.warn('Method .update() is depricated. Please use .q.update() instead.') 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:07<00:00, 1.18s/it] nss_table.select_df(limit=3) /DataDrive/code/doctable/examples/../doctable/doctable.py:419: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id year party president text tokens 0 1 2000 D Clinton As we enter the new millennium, we are blessed... [as, we, enter, the, new, millennium, ,, we, a... 1 2 2002 R W. Bush The great struggles of the twentieth century b... [the, great, struggles, of, the, twentieth, ce... 2 3 2006 R W. Bush My fellow Americans, \\n\\nAmerica is at war. Th... [my, fellow, Americans, ,, America, is, at, wa... for doc in nss_table.select(): print(f\"{doc.president} ({doc.year}): {len(doc.tokens)} tokens.\") Clinton (2000): 50156 tokens. W. Bush (2002): 14493 tokens. W. Bush (2006): 21590 tokens. Obama (2010): 31997 tokens. Obama (2015): 16611 tokens. Trump (2017): 24420 tokens. /DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') And that is all for this vignette! See the list of vignettes at the top of this page for more examples.","title":"Vignette 2: Storing Document Text"},{"location":"legacy_documentation/example_nss_2_parsing/#vignette-2-storing-document-text","text":"In this example, I'll show how to create a database for document text + metadata storage using the DocTable class, and a parser class using a ParsePipeline . We will store the metadata you see below with the raw text and parsed tokens in the same DocTable. These are the vignettes I have created: 1: Storing Document Metadata 2: Storing Document Text 3: Storing Parsed Documents import sys sys.path.append('..') import doctable import spacy from tqdm import tqdm import pandas as pd import os from pprint import pprint import urllib.request # used for downloading nss docs # automatically clean up temp folder after python ends import tempfile tempdir = tempfile.TemporaryDirectory() tmpfolder = tempdir.name tmpfolder '/tmp/tmpm_ciuemc'","title":"Vignette 2: Storing Document Text"},{"location":"legacy_documentation/example_nss_2_parsing/#introduction-to-nss-corpus","text":"This dataset is the plain text version of the US National Security Strategy documents. During the parsing process, all plain text files will be downloaded from my github project hosting the nss docs . I compiled the metadata you see below from a page hosted by the historical dept of the secretary's office . In short, each US President must release at least one NSS per term, with some (namely Clinton) producing more. I've defined the document metadata as nss_metadata , which contains the year (which I used to make the url), the president name, and the political party they belong to. We will later use download_nss() to actually download the text and store it into the database. def download_nss(year): ''' Simple helper function for downloading texts from my nssdocs repo.''' baseurl = 'https://raw.githubusercontent.com/devincornell/nssdocs/master/docs/{}.txt' url = baseurl.format(year) text = urllib.request.urlopen(url).read().decode('utf-8') return text document_metadata = [ {'year': 2000, 'party': 'D', 'president': 'Clinton'}, {'year': 2002, 'party': 'R', 'president': 'W. Bush'}, {'year': 2006, 'party': 'R', 'president': 'W. Bush'}, {'year': 2010, 'party': 'D', 'president': 'Obama'}, {'year': 2015, 'party': 'D', 'president': 'Obama'}, {'year': 2017, 'party': 'R', 'president': 'Trump'}, ] # downloader example: first 100 characters of 1993 NSS document text = download_nss(1993) text[:100] 'Preface \\n\\nAmerican Leadership for Peaceful Change \\n\\nOur great Nation stands at a crossroads in histo'","title":"Introduction to NSS Corpus"},{"location":"legacy_documentation/example_nss_2_parsing/#1-create-a-table-schema","text":"The first step will be to define a database schema that is appropriate for the data in document_metadata . We define an NSSDoc class to represent a single document. The doctable.schema decorator will convert the row objects into dataclasses with slots enabled, and inherit from doctable.DocTableRow to add some additional functionality. The type hints associated with each variable will be used in the schema definition for the new tables, and arguments to doctable.Col will mostly be passed to dataclasses.field (see docs for more detail), so all dataclass functionality is maintained. See the schema guide for examples of the full range of column types. from typing import Any # import generic type hint # to be used as a database row representing a single NSS document @doctable.schema class NSSDoc: __slots__ = [] # include so that doctable.schema can create a slot class id: int = doctable.IDCol() # this is an alias for doctable.Col(primary_key=True, autoincrement=True) year: int = doctable.Col() party: str = doctable.Col() president: str = doctable.Col() text: str = doctable.Col() tokens: Any = doctable.Col() # this will be used as a binary type that stores pickled data @property def num_tokens(self): return len(self.tokens) def paragraphs(self): return self.text.split('\\n\\n')","title":"1. Create a Table Schema"},{"location":"legacy_documentation/example_nss_2_parsing/#2-define-a-custom-doctable","text":"Now we define a class called NSSDocTable to represent the database table. This table must inherit from DocTable and will store connection and schema information. The DocTable class is often used by subclassing. Our NSSDocs class inherits from DocTable and will store connection and schema information. Because the default constructor checks for statically define member variables tabname and schema (as well as others), we can simply add them to the class definition. We also can use this definition to create indices and constraints using the _indices_ and _constraints_ member variables. The indices are provided as name->columns pairs, and the constraints are tuples of the form (constraint_type, constraint_details) . In this case, we limit the values for check to R or D. class NSSDocTable(doctable.DocTable): _tabname_ = 'nss_documents' _schema_ = NSSDoc _indices_ = ( doctable.Index('party_index', 'party'), ) _constraints_ = ( doctable.Constraint('check', 'party in (\"R\", \"D\")'), ) We can then create a connection to a database by instantiating the NSSDocTable class. We used target=':memory:' to indicate that the sqlite table should be created in-memory. # printing the DocTable object itself shows how many entries there are nss_table = NSSDocTable(target=':memory:') print(nss_table.count()) print(nss_table) nss_table.schema_table() 0 <NSSDocTable (6 cols)::sqlite:///:memory::nss_documents> /DataDrive/code/doctable/examples/../doctable/doctable.py:402: UserWarning: Method .count() is depricated. Please use .q.count() instead. warnings.warn('Method .count() is depricated. Please use .q.count() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 year INTEGER True None auto 0 2 party VARCHAR True None auto 0 3 president VARCHAR True None auto 0 4 text VARCHAR True None auto 0 5 tokens BLOB True None auto 0","title":"2. Define a Custom DocTable"},{"location":"legacy_documentation/example_nss_2_parsing/#2-insert-data-into-the-table","text":"Now let's download and store the text into the database. Each loop downloads a text document and inserts it into the doctable, and we use the .insert() method to insert a single row at a time. The row to be inserted is represented as a dictionary, and any missing column information is left as NULL. The ifnotunique argument is set to false because if we were to re-run this code, it needs to replace the existing document of the same year. Recall that in the schema we placed a unique constraint on the year column. for docmeta in tqdm(document_metadata): text = download_nss(docmeta['year']) nss_table.insert({**docmeta, **{'text': text}}, ifnotunique='replace') nss_table.head() 0%| | 0/6 [00:00<?, ?it/s]/DataDrive/code/doctable/examples/../doctable/doctable.py:364: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:390: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:00<00:00, 12.32it/s] /DataDrive/code/doctable/examples/../doctable/doctable.py:407: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id year party president text tokens 0 1 2000 D Clinton As we enter the new millennium, we are blessed... None 1 2 2002 R W. Bush The great struggles of the twentieth century b... None 2 3 2006 R W. Bush My fellow Americans, \\n\\nAmerica is at war. Th... None 3 4 2010 D Obama Time and again in our Nation's history, Americ... None 4 5 2015 D Obama Today, the United States is stronger and bette... None","title":"2. Insert Data Into the Table"},{"location":"legacy_documentation/example_nss_2_parsing/#3-query-table-data","text":"Now that we have inserted the NSS documents into the table, there are a few ways we can query the data. To select the first entry of the table use .select_first() . This method returns a simple sqlalchemy.RowProxy object which can be accessed like a dictionary or like a tuple. row = nss_table.select_first(['id', 'year', 'party', 'president']) print(row) print(row.president) /DataDrive/code/doctable/examples/../doctable/doctable.py:426: UserWarning: Method .select_first() is depricated. Please use .q.select_first() instead. warnings.warn('Method .select_first() is depricated. Please use .q.select_first() instead.') NSSDoc(id=1, year=2000, party='D', president='Clinton') Clinton To select more than one row, use the .select() method. If you'd only like to return the first few rows, you can use the limit argument. rows = nss_table.select(limit=2) print(rows[0].year) print(rows[1].year) 2000 2002 /DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') We can also select only a few columns. nss_table.select(['year', 'president'], limit=3) [NSSDoc(year=2000, president='Clinton'), NSSDoc(year=2002, president='W. Bush'), NSSDoc(year=2006, president='W. Bush')] For convenience, we can also use the .select_df() method to return directly as a pandas dataframe. # use select_df to show a couple rows of our database nss_table.select_df(limit=2) /DataDrive/code/doctable/examples/../doctable/doctable.py:419: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id year party president text tokens 0 1 2000 D Clinton As we enter the new millennium, we are blessed... None 1 2 2002 R W. Bush The great struggles of the twentieth century b... None And access the .paragraphs() method we defined in NSSDoc . for row in nss_table.select(limit=3): print(f\"{row.president} ({row.year}): num_paragraphs={len(row.paragraphs())}\") Clinton (2000): num_paragraphs=569 W. Bush (2002): num_paragraphs=199 W. Bush (2006): num_paragraphs=474 /DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.')","title":"3. Query Table Data"},{"location":"legacy_documentation/example_nss_2_parsing/#4-create-a-parser-for-tokenization","text":"Now that the text is in the doctable, we can extract it using .select() , parse it, and store the parsed text back into the table using .update() . Now we create a parser using ParsePipeline and a list of functions to apply to the text sequentially. The Comp function returns a doctable parse function with additional keyword arguments. For instance, the following two expressions would be the same. doctable.Comp('keep_tok', keep_punct=True) # is equivalent to lambda x: doctable.parse.parse_tok_func(x, keep_punct=True) Note in this example that the 'tokenize' function takes two function arguments: keep_tok_func and parse_tok_func , which are also specified using the .Comp() function. The available pipeline components are listed in the parse function documentation . # add pipeline components parser = doctable.ParsePipeline([ spacy.load('en_core_web_sm'), # load a spacy parser as first step in pipeline doctable.Comp('tokenize', **{ 'split_sents': False, 'keep_tok_func': doctable.Comp('keep_tok'), 'parse_tok_func': doctable.Comp('parse_tok'), }) ]) parser.components [<spacy.lang.en.English at 0x7f37ce907f70>, functools.partial(<function tokenize at 0x7f37d3d74e50>, split_sents=False, keep_tok_func=functools.partial(<function keep_tok at 0x7f37d3d74f70>), parse_tok_func=functools.partial(<function parse_tok at 0x7f37d3d74ee0>))] Now we loop through rows in the doctable and for each iteration parse the text and insert it back into the table using .update() . We use the ParsePipeline method .parsemany() to parse paragraphs from each document in parallel. for doc in tqdm(nss_table.select(['id','year','text'])): paragraphs = parser.parsemany(doc.text.split('\\n\\n'), workers=30) # parse paragraphs in parallel nss_table.update({'tokens': [t for p in paragraphs for t in p]}, where=nss_table['id']==doc.id) 0%| | 0/6 [00:00<?, ?it/s]/DataDrive/code/doctable/examples/../doctable/doctable.py:489: UserWarning: Method .update() is depricated. Please use .q.update() instead. warnings.warn('Method .update() is depricated. Please use .q.update() instead.') 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:07<00:00, 1.18s/it] nss_table.select_df(limit=3) /DataDrive/code/doctable/examples/../doctable/doctable.py:419: UserWarning: Method .select_df() is depricated. Please use .q.select_df() instead. warnings.warn('Method .select_df() is depricated. Please use .q.select_df() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id year party president text tokens 0 1 2000 D Clinton As we enter the new millennium, we are blessed... [as, we, enter, the, new, millennium, ,, we, a... 1 2 2002 R W. Bush The great struggles of the twentieth century b... [the, great, struggles, of, the, twentieth, ce... 2 3 2006 R W. Bush My fellow Americans, \\n\\nAmerica is at war. Th... [my, fellow, Americans, ,, America, is, at, wa... for doc in nss_table.select(): print(f\"{doc.president} ({doc.year}): {len(doc.tokens)} tokens.\") Clinton (2000): 50156 tokens. W. Bush (2002): 14493 tokens. W. Bush (2006): 21590 tokens. Obama (2010): 31997 tokens. Obama (2015): 16611 tokens. Trump (2017): 24420 tokens. /DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') And that is all for this vignette! See the list of vignettes at the top of this page for more examples.","title":"4. Create a Parser for Tokenization"},{"location":"legacy_documentation/example_nss_3_parsetrees/","text":"Vignette 3: Storing Parsed Documents Here I'll show how to make a DocTable for storing NSS documents at the paragraph level, and parse the documents in parallel. For context, check out Example 1 - here we'll just use some shortcuts for code used there. These come from the util.py code in the repo examples folder. These are the vignettes I have created: 1: Storing Document Metadata 2: Storing Document Text 3: Storing Parsed Documents import sys sys.path.append('..') #import util import doctable import spacy from tqdm import tqdm # automatically clean up temp folder after python ends import tempfile tempdir = tempfile.TemporaryDirectory() tmpfolder = tempdir.name tmpfolder '/tmp/tmp1isfmada' First we define the metadata and download the text data. import urllib def download_nss(year): ''' Simple helper function for downloading texts from my nssdocs repo.''' baseurl = 'https://raw.githubusercontent.com/devincornell/nssdocs/master/docs/{}.txt' url = baseurl.format(year) text = urllib.request.urlopen(url).read().decode('utf-8') return text document_metadata = [ {'year': 2000, 'party': 'D', 'president': 'Clinton'}, {'year': 2006, 'party': 'R', 'president': 'W. Bush'}, {'year': 2015, 'party': 'D', 'president': 'Obama'}, {'year': 2017, 'party': 'R', 'president': 'Trump'}, ] sep = '\\n\\n' first_n = 10 for md in document_metadata: text = download_nss(md['year']) md['text'] = sep.join(text.split(sep)[:first_n]) print(f\"{len(document_metadata[0]['text'])=}\") len(document_metadata[0]['text'])=6695 1. Define the DocTable Schema Now we define a doctable schema using the doctable.schema class decorator and the pickle file column type to prepare to store parsetrees as binary data. # to be used as a database row representing a single NSS document @doctable.schema class NSSDoc: __slots__ = [] # include so that doctable.schema can create a slot class id: int = doctable.IDCol() # this is an alias for doctable.Col(primary_key=True, autoincrement=True) year: int = doctable.Col() party: str = doctable.Col() president: str = doctable.Col() text: str = doctable.Col() doc: doctable.ParseTreeDoc = doctable.ParseTreeFileCol(f'{tmpfolder}/parsetree_pickle_files') And a class to represent an NSS DocTable. class NSSDocTable(doctable.DocTable): _tabname_ = 'nss_documents' _schema_ = NSSDoc nss_table = NSSDocTable(target=f'{tmpfolder}/nss_3.db', new_db=True) print(nss_table.count()) nss_table.schema_table() 0 /DataDrive/code/doctable/examples/../doctable/doctable.py:402: UserWarning: Method .count() is depricated. Please use .q.count() instead. warnings.warn('Method .count() is depricated. Please use .q.count() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 year INTEGER True None auto 0 2 party VARCHAR True None auto 0 3 president VARCHAR True None auto 0 4 text VARCHAR True None auto 0 5 doc VARCHAR True None auto 0 for md in document_metadata: nss_table.insert(md) nss_table.head() /DataDrive/code/doctable/examples/../doctable/doctable.py:364: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:390: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:407: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') /DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: TypeDecorator ParseTreeDocFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True. This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions. Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf) return self._engine.execute(query, *args, **kwargs) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id year party president text doc 0 1 2000 D Clinton As we enter the new millennium, we are blessed... None 1 2 2006 R W. Bush My fellow Americans, \\n\\nAmerica is at war. Th... None 2 3 2015 D Obama Today, the United States is stronger and bette... None 3 4 2017 R Trump An America that is safe, prosperous, and free ... None 2. Create a Parser Class Using a Pipeline Now we create a small NSSParser class that keeps a doctable.ParsePipeline object for doing the actual text processing. As you can see from our init method, instantiating the package will load a spacy module into memory and construct the pipeline from the selected components. We also create a wrapper over the pipeline .parse and .parsemany methods. Here we define, instantiate, and view the components of NSSParser . class NSSParser: ''' Handles text parsing for NSS documents.''' def __init__(self): nlp = spacy.load('en_core_web_sm') # this determines all settings for tokenizing self.pipeline = doctable.ParsePipeline([ nlp, # first run spacy parser doctable.Comp('merge_tok_spans', merge_ents=True), doctable.Comp('get_parsetrees', **{ 'text_parse_func': doctable.Comp('parse_tok', **{ 'format_ents': True, 'num_replacement': 'NUM', }) }) ]) def parse(self, text): return self.pipeline.parse(text) parser = NSSParser() # creates a parser instance parser.pipeline.components [<spacy.lang.en.English at 0x7fedee1c2cd0>, functools.partial(<function merge_tok_spans at 0x7fedf2d8f040>, merge_ents=True), functools.partial(<function get_parsetrees at 0x7fedf2d8f1f0>, text_parse_func=functools.partial(<function parse_tok at 0x7fedf2d82ee0>, format_ents=True, num_replacement='NUM'))] Now we parse the paragraphs of each document in parallel. for doc in tqdm(nss_table.select(['id','year','text'])): parsed = parser.parse(doc.text) #print(parsed.as_dict()) #break print(nss_table['doc']) nss_table.update({'doc': parsed}, where=nss_table['id']==doc.id, verbose=True) #nss_table.select_df(limit=2) /DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') 0%| | 0/4 [00:00<?, ?it/s]/DataDrive/code/doctable/examples/../doctable/doctable.py:489: UserWarning: Method .update() is depricated. Please use .q.update() instead. warnings.warn('Method .update() is depricated. Please use .q.update() instead.') 25%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 1/4 [00:00<00:00, 3.83it/s] nss_documents.doc DocTable: UPDATE nss_documents SET doc=? WHERE nss_documents.id = ? nss_documents.doc DocTable: UPDATE nss_documents SET doc=? WHERE nss_documents.id = ? 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 4.89it/s] nss_documents.doc DocTable: UPDATE nss_documents SET doc=? WHERE nss_documents.id = ? nss_documents.doc DocTable: UPDATE nss_documents SET doc=? WHERE nss_documents.id = ? 3. Work With Parsetrees Now that we have stored our parsed text as files in the database, we can manipulate the parsetrees. This example shows the 5 most common nouns from each national security strategy document. This is possible because the doctable.ParseTree data structures contain pos information originally provided by the spacy parser. Using ParseTreeFileType allows us to more efficiently store pickled binary data so that we can perform these kinds of analyses at scale. from collections import Counter # used to count tokens for nss in nss_table.select(): noun_counts = Counter([tok.text for pt in nss.doc for tok in pt if tok.pos == 'NOUN']) print(f\"{nss.president} ({nss.year}): {noun_counts.most_common(5)}\") Clinton (2000): [('world', 9), ('security', 9), ('prosperity', 7), ('threats', 5), ('efforts', 5)] W. Bush (2006): [('people', 4), ('world', 3), ('war', 2), ('security', 2), ('strategy', 2)] Obama (2015): [('security', 15), ('world', 9), ('opportunities', 7), ('strength', 7), ('challenges', 7)] Trump (2017): [('government', 5), ('principles', 4), ('peace', 3), ('people', 3), ('world', 3)] /DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: TypeDecorator ParseTreeDocFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True. This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions. Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf) return self._engine.execute(query, *args, **kwargs) Definitely check out this example on parsetreedocs if you're interested in more applications. And that is all for this vignette! See the list of vignettes at the top of this page for more examples.","title":"Vignette 3: Storing Parsed Documents"},{"location":"legacy_documentation/example_nss_3_parsetrees/#vignette-3-storing-parsed-documents","text":"Here I'll show how to make a DocTable for storing NSS documents at the paragraph level, and parse the documents in parallel. For context, check out Example 1 - here we'll just use some shortcuts for code used there. These come from the util.py code in the repo examples folder. These are the vignettes I have created: 1: Storing Document Metadata 2: Storing Document Text 3: Storing Parsed Documents import sys sys.path.append('..') #import util import doctable import spacy from tqdm import tqdm # automatically clean up temp folder after python ends import tempfile tempdir = tempfile.TemporaryDirectory() tmpfolder = tempdir.name tmpfolder '/tmp/tmp1isfmada' First we define the metadata and download the text data. import urllib def download_nss(year): ''' Simple helper function for downloading texts from my nssdocs repo.''' baseurl = 'https://raw.githubusercontent.com/devincornell/nssdocs/master/docs/{}.txt' url = baseurl.format(year) text = urllib.request.urlopen(url).read().decode('utf-8') return text document_metadata = [ {'year': 2000, 'party': 'D', 'president': 'Clinton'}, {'year': 2006, 'party': 'R', 'president': 'W. Bush'}, {'year': 2015, 'party': 'D', 'president': 'Obama'}, {'year': 2017, 'party': 'R', 'president': 'Trump'}, ] sep = '\\n\\n' first_n = 10 for md in document_metadata: text = download_nss(md['year']) md['text'] = sep.join(text.split(sep)[:first_n]) print(f\"{len(document_metadata[0]['text'])=}\") len(document_metadata[0]['text'])=6695","title":"Vignette 3: Storing Parsed Documents"},{"location":"legacy_documentation/example_nss_3_parsetrees/#1-define-the-doctable-schema","text":"Now we define a doctable schema using the doctable.schema class decorator and the pickle file column type to prepare to store parsetrees as binary data. # to be used as a database row representing a single NSS document @doctable.schema class NSSDoc: __slots__ = [] # include so that doctable.schema can create a slot class id: int = doctable.IDCol() # this is an alias for doctable.Col(primary_key=True, autoincrement=True) year: int = doctable.Col() party: str = doctable.Col() president: str = doctable.Col() text: str = doctable.Col() doc: doctable.ParseTreeDoc = doctable.ParseTreeFileCol(f'{tmpfolder}/parsetree_pickle_files') And a class to represent an NSS DocTable. class NSSDocTable(doctable.DocTable): _tabname_ = 'nss_documents' _schema_ = NSSDoc nss_table = NSSDocTable(target=f'{tmpfolder}/nss_3.db', new_db=True) print(nss_table.count()) nss_table.schema_table() 0 /DataDrive/code/doctable/examples/../doctable/doctable.py:402: UserWarning: Method .count() is depricated. Please use .q.count() instead. warnings.warn('Method .count() is depricated. Please use .q.count() instead.') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 year INTEGER True None auto 0 2 party VARCHAR True None auto 0 3 president VARCHAR True None auto 0 4 text VARCHAR True None auto 0 5 doc VARCHAR True None auto 0 for md in document_metadata: nss_table.insert(md) nss_table.head() /DataDrive/code/doctable/examples/../doctable/doctable.py:364: UserWarning: Method .insert() is depricated. Please use .q.insert_single(), .q.insert_single_raw(), .q.insert_multi(), or .q.insert_multi() instead. warnings.warn('Method .insert() is depricated. Please use .q.insert_single(), ' /DataDrive/code/doctable/examples/../doctable/doctable.py:390: UserWarning: .insert_single() is depricated: please use .q.insert_single() or .q.insert_single_raw() warnings.warn(f'.insert_single() is depricated: please use .q.insert_single() or ' /DataDrive/code/doctable/examples/../doctable/doctable.py:407: UserWarning: Method .head() is depricated. Please use .q.select_head() instead. warnings.warn('Method .head() is depricated. Please use .q.select_head() instead.') /DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: TypeDecorator ParseTreeDocFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True. This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions. Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf) return self._engine.execute(query, *args, **kwargs) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id year party president text doc 0 1 2000 D Clinton As we enter the new millennium, we are blessed... None 1 2 2006 R W. Bush My fellow Americans, \\n\\nAmerica is at war. Th... None 2 3 2015 D Obama Today, the United States is stronger and bette... None 3 4 2017 R Trump An America that is safe, prosperous, and free ... None","title":"1. Define the DocTable Schema"},{"location":"legacy_documentation/example_nss_3_parsetrees/#2-create-a-parser-class-using-a-pipeline","text":"Now we create a small NSSParser class that keeps a doctable.ParsePipeline object for doing the actual text processing. As you can see from our init method, instantiating the package will load a spacy module into memory and construct the pipeline from the selected components. We also create a wrapper over the pipeline .parse and .parsemany methods. Here we define, instantiate, and view the components of NSSParser . class NSSParser: ''' Handles text parsing for NSS documents.''' def __init__(self): nlp = spacy.load('en_core_web_sm') # this determines all settings for tokenizing self.pipeline = doctable.ParsePipeline([ nlp, # first run spacy parser doctable.Comp('merge_tok_spans', merge_ents=True), doctable.Comp('get_parsetrees', **{ 'text_parse_func': doctable.Comp('parse_tok', **{ 'format_ents': True, 'num_replacement': 'NUM', }) }) ]) def parse(self, text): return self.pipeline.parse(text) parser = NSSParser() # creates a parser instance parser.pipeline.components [<spacy.lang.en.English at 0x7fedee1c2cd0>, functools.partial(<function merge_tok_spans at 0x7fedf2d8f040>, merge_ents=True), functools.partial(<function get_parsetrees at 0x7fedf2d8f1f0>, text_parse_func=functools.partial(<function parse_tok at 0x7fedf2d82ee0>, format_ents=True, num_replacement='NUM'))] Now we parse the paragraphs of each document in parallel. for doc in tqdm(nss_table.select(['id','year','text'])): parsed = parser.parse(doc.text) #print(parsed.as_dict()) #break print(nss_table['doc']) nss_table.update({'doc': parsed}, where=nss_table['id']==doc.id, verbose=True) #nss_table.select_df(limit=2) /DataDrive/code/doctable/examples/../doctable/doctable.py:443: UserWarning: Method .select() is depricated. Please use .q.select() instead. warnings.warn('Method .select() is depricated. Please use .q.select() instead.') 0%| | 0/4 [00:00<?, ?it/s]/DataDrive/code/doctable/examples/../doctable/doctable.py:489: UserWarning: Method .update() is depricated. Please use .q.update() instead. warnings.warn('Method .update() is depricated. Please use .q.update() instead.') 25%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 1/4 [00:00<00:00, 3.83it/s] nss_documents.doc DocTable: UPDATE nss_documents SET doc=? WHERE nss_documents.id = ? nss_documents.doc DocTable: UPDATE nss_documents SET doc=? WHERE nss_documents.id = ? 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 4.89it/s] nss_documents.doc DocTable: UPDATE nss_documents SET doc=? WHERE nss_documents.id = ? nss_documents.doc DocTable: UPDATE nss_documents SET doc=? WHERE nss_documents.id = ?","title":"2. Create a Parser Class Using a Pipeline"},{"location":"legacy_documentation/example_nss_3_parsetrees/#3-work-with-parsetrees","text":"Now that we have stored our parsed text as files in the database, we can manipulate the parsetrees. This example shows the 5 most common nouns from each national security strategy document. This is possible because the doctable.ParseTree data structures contain pos information originally provided by the spacy parser. Using ParseTreeFileType allows us to more efficiently store pickled binary data so that we can perform these kinds of analyses at scale. from collections import Counter # used to count tokens for nss in nss_table.select(): noun_counts = Counter([tok.text for pt in nss.doc for tok in pt if tok.pos == 'NOUN']) print(f\"{nss.president} ({nss.year}): {noun_counts.most_common(5)}\") Clinton (2000): [('world', 9), ('security', 9), ('prosperity', 7), ('threats', 5), ('efforts', 5)] W. Bush (2006): [('people', 4), ('world', 3), ('war', 2), ('security', 2), ('strategy', 2)] Obama (2015): [('security', 15), ('world', 9), ('opportunities', 7), ('strength', 7), ('challenges', 7)] Trump (2017): [('government', 5), ('principles', 4), ('peace', 3), ('people', 3), ('world', 3)] /DataDrive/code/doctable/examples/../doctable/connectengine.py:69: SAWarning: TypeDecorator ParseTreeDocFileType() will not produce a cache key because the ``cache_ok`` attribute is not set to True. This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions. Set this attribute to True if this type object's state is safe to use in a cache key, or False to disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf) return self._engine.execute(query, *args, **kwargs) Definitely check out this example on parsetreedocs if you're interested in more applications. And that is all for this vignette! See the list of vignettes at the top of this page for more examples.","title":"3. Work With Parsetrees"},{"location":"legacy_documentation/example_nss_intro_dataclass/","text":"Example 1: US National Security Strategy Document Corpus In this example, I'll show how to create a database for document + metadata storage using the DocTable class, and a parser class using a ParsePipeline . We will store the metadata you see below with the raw text and parsed tokens in the same DocTable. import sys sys.path.append('..') import doctable import spacy from tqdm import tqdm import pandas as pd import os from pprint import pprint import urllib.request # used for downloading nss docs Introduction to NSS Corpus This dataset is the plain text version of the US National Security Strategy documents. During the parsing process, all plain text files will be downloaded from my github project hosting the nss docs . I compiled the metadata you see below from a page hosted by the historical dept of the secretary's office . In short, each US President must release at least one NSS per term, with some (namely Clinton) producing more. Here I've created the function download_nss to download the text data from my nssdocs github repository, and the python dictionary nss_metadata to store information about each document to be stored in the database. def download_nss(year): ''' Simple helper function for downloading texts from my nssdocs repo.''' baseurl = 'https://raw.githubusercontent.com/devincornell/nssdocs/master/docs/{}.txt' url = baseurl.format(year) text = urllib.request.urlopen(url).read().decode('utf-8') return text nss_metadata = { 1987: {'party': 'R', 'president': 'Reagan'}, 1993: {'party': 'R', 'president': 'H.W. Bush'}, 2002: {'party': 'R', 'president': 'W. Bush'}, 2015: {'party': 'D', 'president': 'Obama'}, 1994: {'party': 'D', 'president': 'Clinton'}, 1990: {'party': 'R', 'president': 'H.W. Bush'}, 1991: {'party': 'R', 'president': 'H.W. Bush'}, 2006: {'party': 'R', 'president': 'W. Bush'}, 1997: {'party': 'D', 'president': 'Clinton'}, 1995: {'party': 'D', 'president': 'Clinton'}, 1988: {'party': 'R', 'president': 'Reagan'}, 2017: {'party': 'R', 'president': 'Trump'}, 1996: {'party': 'D', 'president': 'Clinton'}, 2010: {'party': 'D', 'president': 'Obama'}, 1999: {'party': 'D', 'president': 'Clinton'}, 1998: {'party': 'D', 'president': 'Clinton'}, 2000: {'party': 'D', 'president': 'Clinton'} } # downloader example: first 100 characters of 1993 NSS document text = download_nss(1993) text[:100] 'Preface \\n\\nAmerican Leadership for Peaceful Change \\n\\nOur great Nation stands at a crossroads in histo' 1. Create a DocTable Schema The DocTable class is often used by subclassing. Our NSSDocs class inherits from DocTable and will store connection and schema information. Because the default constructor checks for statically define member variables tabname and schema (as well as others), we can simply add them to the class definition. In this example, we create the 'id' column as a unique index, the 'year', 'president', and 'party' columns for storing the metadata we defined above in nss_metadata , and columns for raw and parse text. See the schema guide for examples of the full range of column types. from dataclasses import dataclass from typing import Any @doctable.schema(require_slots=False) class NSSDoc: id: int = doctable.IDCol() year: int = doctable.Col(nullable=False) president: str = doctable.Col() party: str = doctable.Col() text: str = doctable.Col() parsed: Any = doctable.Col() We can then create a connection to a database by instantiating the NSSDocs class. Since the fname parameter was not provided, this doctable exists only in memory using sqlite (uses special sqlite name \":memory:\"). We will use this for these examples. We can check the sqlite table schema using .schema_table() . You can see that the 'pickle' datatype we chose above is represented as a BLOB column. This is because DocTable, using SQLAlchemy core, creates an interface on top of sqlite to handle the data conversion. You can view the number of documents using .count() or by viewing the db instance as a string (in this case with print function). # printing the DocTable object itself shows how many entries there are db = doctable.DocTable(schema=NSSDoc, target=':memory:', verbose=True) print(db.count()) print(db) db.schema_table() DocTable: SELECT count() AS count_1 FROM _documents_ LIMIT ? OFFSET ? 0 <DocTable (6 cols)::sqlite:///:memory::_documents_> .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 year INTEGER False None auto 0 2 president VARCHAR True None auto 0 3 party VARCHAR True None auto 0 4 text VARCHAR True None auto 0 5 parsed BLOB True None auto 0 2. Insert Data Into the Table Now let's download and store the text into the database. Each loop downloads a text document and inserts it into the doctable, and we use the .insert() method to insert a single row at a time. The row to be inserted is represented as a dictionary, and any missing column information is left as NULL. The ifnotunique argument is set to false because if we were to re-run this code, it needs to replace the existing document of the same year. Recall that in the schema we placed a unique constraint on the year column. for year, docmeta in tqdm(nss_metadata.items()): text = download_nss(year) new_doc = NSSDoc( year=year, party=docmeta['party'], president=docmeta['president'], text=text ) db.insert(new_doc, ifnotunique='replace', verbose=False) db.head() 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17/17 [00:01<00:00, 12.31it/s] DocTable: SELECT _documents_.id, _documents_.year, _documents_.president, _documents_.party, _documents_.text, _documents_.parsed FROM _documents_ LIMIT ? OFFSET ? .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id year president party text parsed 0 1 1987 Reagan R I. An American Perspective \\n\\nIn the early da... [[I., An, American, Perspective], [in, the, ea... 1 2 1993 H.W. Bush R Preface \\n\\nAmerican Leadership for Peaceful C... [[preface], [American, leadership, for, peacef... 2 3 2002 W. Bush R The great struggles of the twentieth century b... [[the, great, struggles, of, the, twentieth, c... 3 4 2015 Obama D Today, the United States is stronger and bette... [[Today, ,, the, United, States, is, stronger,... 4 5 1994 Clinton D Preface \\n\\nProtecting our nation's security -... [[preface], [protecting, our, nation, 's, secu... 3. Query Table Data Now that we have inserted the NSS documents into the table, there are a few ways we can query the data. To select the first entry of the table use .select_first() . This method returns a simple sqlalchemy.RowProxy object which can be accessed like a dictionary or like a tuple. row = db.select_first() #print(row) print(row['president']) DocTable: SELECT _documents_.id, _documents_.year, _documents_.president, _documents_.party, _documents_.text, _documents_.parsed FROM _documents_ LIMIT ? OFFSET ? Reagan To select more than one row, use the .select() method. If you'd only like to return the first few rows, you can use the limit argument. rows = db.select(limit=2) print(rows[0]['year']) print(rows[1]['year']) DocTable: SELECT _documents_.id, _documents_.year, _documents_.president, _documents_.party, _documents_.text, _documents_.parsed FROM _documents_ LIMIT ? OFFSET ? 1987 1993 We can also select only a few columns. db.select(['year', 'president'], limit=3) DocTable: SELECT _documents_.year, _documents_.president FROM _documents_ LIMIT ? OFFSET ? [NSSDoc(year=1987, president='Reagan'), NSSDoc(year=1993, president='H.W. Bush'), NSSDoc(year=2002, president='W. Bush')] For convenience, we can also use the .select_df() method to return directly as a pandas dataframe. # use select_df to show a couple rows of our database db.select_df(limit=2) DocTable: SELECT _documents_.id, _documents_.year, _documents_.president, _documents_.party, _documents_.text, _documents_.parsed FROM _documents_ LIMIT ? OFFSET ? .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id year president party text parsed 0 1 1987 Reagan R I. An American Perspective \\n\\nIn the early da... [[I., An, American, Perspective], [in, the, ea... 1 2 1993 H.W. Bush R Preface \\n\\nAmerican Leadership for Peaceful C... [[preface], [American, leadership, for, peacef... 4. Create a Parser for Tokenization Now that the text is in the doctable, we can extract it using .select() , parse it, and store the parsed text back into the table using .update() . Now we create a parser using ParsePipeline and a list of functions to apply to the text sequentially. The Comp function returns a doctable parse function with additional keyword arguments. For instance, the following two expressions would be the same. doctable.component('keep_tok', keep_punct=True) # is equivalent to lambda x: doctable.parse.parse_tok_func(x, keep_punct=True) Note in this example that the 'tokenize' function takes two function arguments keep_tok_func and parse_tok_func which are also specified using the .Comp() function. The available pipeline components are listed in the parse function documentation . # first load a spacy model nlp = spacy.load('en_core_web_sm') # add pipeline components parser = doctable.ParsePipeline([ nlp, # first run spacy parser doctable.Comp('tokenize', **{ 'split_sents': False, 'keep_tok_func': doctable.Comp('keep_tok'), 'parse_tok_func': doctable.Comp('parse_tok'), }) ]) parser.components [<spacy.lang.en.English at 0x7f2cd9334400>, functools.partial(<function tokenize at 0x7f2d86167160>, split_sents=False, keep_tok_func=functools.partial(<function keep_tok at 0x7f2d86167280>), parse_tok_func=functools.partial(<function parse_tok at 0x7f2d861671f0>))] Now we loop through rows in the doctable and for each iteration parse the text and insert it back into the table using .update() . We use the ParsePipeline method .parsemany() to parse paragraphs from each document in parallel. This is much faster. docs = db.select() for doc in tqdm(docs): doc.parsed = parser.parsemany(doc.text[:1000].split('\\n\\n'), workers=8) # parse paragraphs in parallel db.update_dataclass(doc, verbose=False) 0%| | 0/51 [00:00<?, ?it/s] DocTable: SELECT _documents_.id, _documents_.year, _documents_.president, _documents_.party, _documents_.text, _documents_.parsed FROM _documents_ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:04<00:00, 11.42it/s] See the 'parsed' column in the dataframe below to view the paragraphs. db.select_df(limit=3) DocTable: SELECT _documents_.id, _documents_.year, _documents_.president, _documents_.party, _documents_.text, _documents_.parsed FROM _documents_ LIMIT ? OFFSET ? .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id year president party text parsed 0 1 1987 Reagan R I. An American Perspective \\n\\nIn the early da... [[I., An, American, Perspective], [in, the, ea... 1 2 1993 H.W. Bush R Preface \\n\\nAmerican Leadership for Peaceful C... [[preface], [American, leadership, for, peacef... 2 3 2002 W. Bush R The great struggles of the twentieth century b... [[the, great, struggles, of, the, twentieth, c... And here we show a few tokenized paragraphs. paragraphs = db.select_first('parsed') for par in paragraphs[:3]: print(par, '\\n') DocTable: SELECT _documents_.parsed FROM _documents_ LIMIT ? OFFSET ? ['I.', 'An', 'American', 'Perspective'] ['in', 'the', 'early', 'days', 'of', 'this', 'administration', 'we', 'laid', 'the', 'foundation', 'for', 'a', 'more', 'constructive', 'and', 'positive', 'American', 'role', 'in', 'world', 'affairs', 'by', 'clarifying', 'the', 'essential', 'elements', 'of', 'U.S.', 'foreign', 'and', 'defense', 'policy', '.'] ['over', 'the', 'intervening', 'years', ',', 'we', 'have', 'looked', 'objectively', 'at', 'our', 'policies', 'and', 'performance', 'on', 'the', 'world', 'scene', 'to', 'ensure', 'they', 'reflect', 'the', 'dynamics', 'of', 'a', 'complex', 'and', 'ever', '-', 'changing', 'world', '.', 'where', 'course', 'adjustments', 'have', 'been', 'required', ',', 'i', 'have', 'directed', 'changes', '.', 'but', 'we', 'have', 'not', 'veered', 'and', 'will', 'not', 'veer', 'from', 'the', 'broad', 'aims', 'that', 'guide', 'America', \"'s\", 'leadership', 'role', 'in', 'today', \"'s\", 'world', ':']","title":"Example 1: US National Security Strategy Document Corpus"},{"location":"legacy_documentation/example_nss_intro_dataclass/#example-1-us-national-security-strategy-document-corpus","text":"In this example, I'll show how to create a database for document + metadata storage using the DocTable class, and a parser class using a ParsePipeline . We will store the metadata you see below with the raw text and parsed tokens in the same DocTable. import sys sys.path.append('..') import doctable import spacy from tqdm import tqdm import pandas as pd import os from pprint import pprint import urllib.request # used for downloading nss docs","title":"Example 1: US National Security Strategy Document Corpus"},{"location":"legacy_documentation/example_nss_intro_dataclass/#introduction-to-nss-corpus","text":"This dataset is the plain text version of the US National Security Strategy documents. During the parsing process, all plain text files will be downloaded from my github project hosting the nss docs . I compiled the metadata you see below from a page hosted by the historical dept of the secretary's office . In short, each US President must release at least one NSS per term, with some (namely Clinton) producing more. Here I've created the function download_nss to download the text data from my nssdocs github repository, and the python dictionary nss_metadata to store information about each document to be stored in the database. def download_nss(year): ''' Simple helper function for downloading texts from my nssdocs repo.''' baseurl = 'https://raw.githubusercontent.com/devincornell/nssdocs/master/docs/{}.txt' url = baseurl.format(year) text = urllib.request.urlopen(url).read().decode('utf-8') return text nss_metadata = { 1987: {'party': 'R', 'president': 'Reagan'}, 1993: {'party': 'R', 'president': 'H.W. Bush'}, 2002: {'party': 'R', 'president': 'W. Bush'}, 2015: {'party': 'D', 'president': 'Obama'}, 1994: {'party': 'D', 'president': 'Clinton'}, 1990: {'party': 'R', 'president': 'H.W. Bush'}, 1991: {'party': 'R', 'president': 'H.W. Bush'}, 2006: {'party': 'R', 'president': 'W. Bush'}, 1997: {'party': 'D', 'president': 'Clinton'}, 1995: {'party': 'D', 'president': 'Clinton'}, 1988: {'party': 'R', 'president': 'Reagan'}, 2017: {'party': 'R', 'president': 'Trump'}, 1996: {'party': 'D', 'president': 'Clinton'}, 2010: {'party': 'D', 'president': 'Obama'}, 1999: {'party': 'D', 'president': 'Clinton'}, 1998: {'party': 'D', 'president': 'Clinton'}, 2000: {'party': 'D', 'president': 'Clinton'} } # downloader example: first 100 characters of 1993 NSS document text = download_nss(1993) text[:100] 'Preface \\n\\nAmerican Leadership for Peaceful Change \\n\\nOur great Nation stands at a crossroads in histo'","title":"Introduction to NSS Corpus"},{"location":"legacy_documentation/example_nss_intro_dataclass/#1-create-a-doctable-schema","text":"The DocTable class is often used by subclassing. Our NSSDocs class inherits from DocTable and will store connection and schema information. Because the default constructor checks for statically define member variables tabname and schema (as well as others), we can simply add them to the class definition. In this example, we create the 'id' column as a unique index, the 'year', 'president', and 'party' columns for storing the metadata we defined above in nss_metadata , and columns for raw and parse text. See the schema guide for examples of the full range of column types. from dataclasses import dataclass from typing import Any @doctable.schema(require_slots=False) class NSSDoc: id: int = doctable.IDCol() year: int = doctable.Col(nullable=False) president: str = doctable.Col() party: str = doctable.Col() text: str = doctable.Col() parsed: Any = doctable.Col() We can then create a connection to a database by instantiating the NSSDocs class. Since the fname parameter was not provided, this doctable exists only in memory using sqlite (uses special sqlite name \":memory:\"). We will use this for these examples. We can check the sqlite table schema using .schema_table() . You can see that the 'pickle' datatype we chose above is represented as a BLOB column. This is because DocTable, using SQLAlchemy core, creates an interface on top of sqlite to handle the data conversion. You can view the number of documents using .count() or by viewing the db instance as a string (in this case with print function). # printing the DocTable object itself shows how many entries there are db = doctable.DocTable(schema=NSSDoc, target=':memory:', verbose=True) print(db.count()) print(db) db.schema_table() DocTable: SELECT count() AS count_1 FROM _documents_ LIMIT ? OFFSET ? 0 <DocTable (6 cols)::sqlite:///:memory::_documents_> .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type nullable default autoincrement primary_key 0 id INTEGER False None auto 1 1 year INTEGER False None auto 0 2 president VARCHAR True None auto 0 3 party VARCHAR True None auto 0 4 text VARCHAR True None auto 0 5 parsed BLOB True None auto 0","title":"1. Create a DocTable Schema"},{"location":"legacy_documentation/example_nss_intro_dataclass/#2-insert-data-into-the-table","text":"Now let's download and store the text into the database. Each loop downloads a text document and inserts it into the doctable, and we use the .insert() method to insert a single row at a time. The row to be inserted is represented as a dictionary, and any missing column information is left as NULL. The ifnotunique argument is set to false because if we were to re-run this code, it needs to replace the existing document of the same year. Recall that in the schema we placed a unique constraint on the year column. for year, docmeta in tqdm(nss_metadata.items()): text = download_nss(year) new_doc = NSSDoc( year=year, party=docmeta['party'], president=docmeta['president'], text=text ) db.insert(new_doc, ifnotunique='replace', verbose=False) db.head() 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17/17 [00:01<00:00, 12.31it/s] DocTable: SELECT _documents_.id, _documents_.year, _documents_.president, _documents_.party, _documents_.text, _documents_.parsed FROM _documents_ LIMIT ? OFFSET ? .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id year president party text parsed 0 1 1987 Reagan R I. An American Perspective \\n\\nIn the early da... [[I., An, American, Perspective], [in, the, ea... 1 2 1993 H.W. Bush R Preface \\n\\nAmerican Leadership for Peaceful C... [[preface], [American, leadership, for, peacef... 2 3 2002 W. Bush R The great struggles of the twentieth century b... [[the, great, struggles, of, the, twentieth, c... 3 4 2015 Obama D Today, the United States is stronger and bette... [[Today, ,, the, United, States, is, stronger,... 4 5 1994 Clinton D Preface \\n\\nProtecting our nation's security -... [[preface], [protecting, our, nation, 's, secu...","title":"2. Insert Data Into the Table"},{"location":"legacy_documentation/example_nss_intro_dataclass/#3-query-table-data","text":"Now that we have inserted the NSS documents into the table, there are a few ways we can query the data. To select the first entry of the table use .select_first() . This method returns a simple sqlalchemy.RowProxy object which can be accessed like a dictionary or like a tuple. row = db.select_first() #print(row) print(row['president']) DocTable: SELECT _documents_.id, _documents_.year, _documents_.president, _documents_.party, _documents_.text, _documents_.parsed FROM _documents_ LIMIT ? OFFSET ? Reagan To select more than one row, use the .select() method. If you'd only like to return the first few rows, you can use the limit argument. rows = db.select(limit=2) print(rows[0]['year']) print(rows[1]['year']) DocTable: SELECT _documents_.id, _documents_.year, _documents_.president, _documents_.party, _documents_.text, _documents_.parsed FROM _documents_ LIMIT ? OFFSET ? 1987 1993 We can also select only a few columns. db.select(['year', 'president'], limit=3) DocTable: SELECT _documents_.year, _documents_.president FROM _documents_ LIMIT ? OFFSET ? [NSSDoc(year=1987, president='Reagan'), NSSDoc(year=1993, president='H.W. Bush'), NSSDoc(year=2002, president='W. Bush')] For convenience, we can also use the .select_df() method to return directly as a pandas dataframe. # use select_df to show a couple rows of our database db.select_df(limit=2) DocTable: SELECT _documents_.id, _documents_.year, _documents_.president, _documents_.party, _documents_.text, _documents_.parsed FROM _documents_ LIMIT ? OFFSET ? .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id year president party text parsed 0 1 1987 Reagan R I. An American Perspective \\n\\nIn the early da... [[I., An, American, Perspective], [in, the, ea... 1 2 1993 H.W. Bush R Preface \\n\\nAmerican Leadership for Peaceful C... [[preface], [American, leadership, for, peacef...","title":"3. Query Table Data"},{"location":"legacy_documentation/example_nss_intro_dataclass/#4-create-a-parser-for-tokenization","text":"Now that the text is in the doctable, we can extract it using .select() , parse it, and store the parsed text back into the table using .update() . Now we create a parser using ParsePipeline and a list of functions to apply to the text sequentially. The Comp function returns a doctable parse function with additional keyword arguments. For instance, the following two expressions would be the same. doctable.component('keep_tok', keep_punct=True) # is equivalent to lambda x: doctable.parse.parse_tok_func(x, keep_punct=True) Note in this example that the 'tokenize' function takes two function arguments keep_tok_func and parse_tok_func which are also specified using the .Comp() function. The available pipeline components are listed in the parse function documentation . # first load a spacy model nlp = spacy.load('en_core_web_sm') # add pipeline components parser = doctable.ParsePipeline([ nlp, # first run spacy parser doctable.Comp('tokenize', **{ 'split_sents': False, 'keep_tok_func': doctable.Comp('keep_tok'), 'parse_tok_func': doctable.Comp('parse_tok'), }) ]) parser.components [<spacy.lang.en.English at 0x7f2cd9334400>, functools.partial(<function tokenize at 0x7f2d86167160>, split_sents=False, keep_tok_func=functools.partial(<function keep_tok at 0x7f2d86167280>), parse_tok_func=functools.partial(<function parse_tok at 0x7f2d861671f0>))] Now we loop through rows in the doctable and for each iteration parse the text and insert it back into the table using .update() . We use the ParsePipeline method .parsemany() to parse paragraphs from each document in parallel. This is much faster. docs = db.select() for doc in tqdm(docs): doc.parsed = parser.parsemany(doc.text[:1000].split('\\n\\n'), workers=8) # parse paragraphs in parallel db.update_dataclass(doc, verbose=False) 0%| | 0/51 [00:00<?, ?it/s] DocTable: SELECT _documents_.id, _documents_.year, _documents_.president, _documents_.party, _documents_.text, _documents_.parsed FROM _documents_ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51/51 [00:04<00:00, 11.42it/s] See the 'parsed' column in the dataframe below to view the paragraphs. db.select_df(limit=3) DocTable: SELECT _documents_.id, _documents_.year, _documents_.president, _documents_.party, _documents_.text, _documents_.parsed FROM _documents_ LIMIT ? OFFSET ? .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id year president party text parsed 0 1 1987 Reagan R I. An American Perspective \\n\\nIn the early da... [[I., An, American, Perspective], [in, the, ea... 1 2 1993 H.W. Bush R Preface \\n\\nAmerican Leadership for Peaceful C... [[preface], [American, leadership, for, peacef... 2 3 2002 W. Bush R The great struggles of the twentieth century b... [[the, great, struggles, of, the, twentieth, c... And here we show a few tokenized paragraphs. paragraphs = db.select_first('parsed') for par in paragraphs[:3]: print(par, '\\n') DocTable: SELECT _documents_.parsed FROM _documents_ LIMIT ? OFFSET ? ['I.', 'An', 'American', 'Perspective'] ['in', 'the', 'early', 'days', 'of', 'this', 'administration', 'we', 'laid', 'the', 'foundation', 'for', 'a', 'more', 'constructive', 'and', 'positive', 'American', 'role', 'in', 'world', 'affairs', 'by', 'clarifying', 'the', 'essential', 'elements', 'of', 'U.S.', 'foreign', 'and', 'defense', 'policy', '.'] ['over', 'the', 'intervening', 'years', ',', 'we', 'have', 'looked', 'objectively', 'at', 'our', 'policies', 'and', 'performance', 'on', 'the', 'world', 'scene', 'to', 'ensure', 'they', 'reflect', 'the', 'dynamics', 'of', 'a', 'complex', 'and', 'ever', '-', 'changing', 'world', '.', 'where', 'course', 'adjustments', 'have', 'been', 'required', ',', 'i', 'have', 'directed', 'changes', '.', 'but', 'we', 'have', 'not', 'veered', 'and', 'will', 'not', 'veer', 'from', 'the', 'broad', 'aims', 'that', 'guide', 'America', \"'s\", 'leadership', 'role', 'in', 'today', \"'s\", 'world', ':']","title":"4. Create a Parser for Tokenization"},{"location":"legacy_documentation/legacy_adv/","text":"DocTable (slightly more) Advanced Example In this notebook, I show how to define a DocTable with blob data types, add new rows, and then iterate through rows to populate previously empty fields. import email from .legacy_helper import get_sklearn_newsgroups # for this example import sys sys.path.append('..') import doctable as dt # this will be the table object we use to interact with our database. tempfolder = dt.TempFolder('tmp') Get News Data From sklearn.datasets Then parses into a dataframe. ddf = get_sklearn_newsgroups() print(ddf.shape) ddf.head(3) (11314, 3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } filename target text 0 21379 soc.religion.christian From: kbanner@philae.sas.upenn.edu (Ken Banner... 1 20874 soc.religion.christian From: simon@monu6.cc.monash.edu.au\\nSubject: S... 2 58936 sci.med From: jeffp@vetmed.wsu.edu (Jeff Parke)\\nSubje... Define NewsGroups DocTable This definition includes fields file_id, category, raw_text, subject, author, and tokenized_text. The extra columns compared to example_simple.ipynb are for storing extracted metadata. class NewsGroups(dt.DocTableLegacy): def __init__(self, fname): ''' DocTable class. Inputs: fname: fname is the name of the new sqlite database that will be used for instances of class. ''' tabname = 'newsgroups' super().__init__( fname=fname, tabname=tabname, colschema=( 'id integer primary key autoincrement', 'file_id int', 'category string', 'raw_text string', 'subject string', 'author string', 'tokenized_text blob', ), constraints=('UNIQUE(file_id)',) ) # create indices on file_id and category self.query(\"create index if not exists idx1 on \"+tabname+\"(file_id)\") self.query(\"create index if not exists idx2 on \"+tabname+\"(category)\") sng = NewsGroups(f'{tmp}/news_groupssss.db') print(sng) <Documents ct: 0> # add in raw data col_order = ('file_id','category','raw_text') data = [(dat['filename'],dat['target'],dat['text']) for ind,dat in ddf.iterrows()] sng.addmany(data,keys=col_order, ifnotunique='ignore') sng.getdf(limit=2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id file_id category raw_text subject author tokenized_text 0 1 21379 soc.religion.christian From: kbanner@philae.sas.upenn.edu (Ken Banner... None None None 1 2 20874 soc.religion.christian From: simon@monu6.cc.monash.edu.au\\nSubject: S... None None None Update \"tokenized_text\" Column Use .get() to loop through rows in the database, and .update() to add in the newly extracted data. In this case, we simply tokenize the text using the python builtin split() function. query = sng.get(sel=('file_id','raw_text',)) for row in query: dat = {'tokenized_text':row['raw_text'].split(),} sng.update(dat, 'file_id == {}'.format(row['file_id'])) sng.getdf(limit=2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id file_id category raw_text subject author tokenized_text 0 1 21379 soc.religion.christian From: kbanner@philae.sas.upenn.edu (Ken Banner... None None [From:, kbanner@philae.sas.upenn.edu, (Ken, Ba... 1 2 20874 soc.religion.christian From: simon@monu6.cc.monash.edu.au\\nSubject: S... None None [From:, simon@monu6.cc.monash.edu.au, Subject:... Extract Email Metadata This example takes it even further by using the \"email\" package to parse apart the blog files. It then uses the extracted information to populate the corresponding fields in the DocTable. query = sng.get(sel=('file_id','raw_text',), asdict=False) for fid,text in query: e = email.message_from_string(text) auth = e['From'] if 'From' in e.keys() else '' subj = e['Subject'] if 'Subject' in e.keys() else '' tok = e.get_payload().split() dat = { 'tokenized_text':tok, 'author':auth, 'subject':subj, } sng.update(dat, 'file_id == {}'.format(fid)) sng.getdf(limit=2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id file_id category raw_text subject author tokenized_text 0 1 21379 soc.religion.christian From: kbanner@philae.sas.upenn.edu (Ken Banner... Re: SATANIC TOUNGES kbanner@philae.sas.upenn.edu (Ken Banner) [In, article, <May.5.02.53.10.1993.28880@athos... 1 2 20874 soc.religion.christian From: simon@monu6.cc.monash.edu.au\\nSubject: S... Saint Story St. Aloysius Gonzaga simon@monu6.cc.monash.edu.au [Heres, a, story, of, a, Saint, that, people, ... sng.getdf().to_csv('newsgroup20.csv')","title":"DocTable (slightly more) Advanced Example"},{"location":"legacy_documentation/legacy_adv/#doctable-slightly-more-advanced-example","text":"In this notebook, I show how to define a DocTable with blob data types, add new rows, and then iterate through rows to populate previously empty fields. import email from .legacy_helper import get_sklearn_newsgroups # for this example import sys sys.path.append('..') import doctable as dt # this will be the table object we use to interact with our database. tempfolder = dt.TempFolder('tmp')","title":"DocTable (slightly more) Advanced Example"},{"location":"legacy_documentation/legacy_adv/#get-news-data-from-sklearndatasets","text":"Then parses into a dataframe. ddf = get_sklearn_newsgroups() print(ddf.shape) ddf.head(3) (11314, 3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } filename target text 0 21379 soc.religion.christian From: kbanner@philae.sas.upenn.edu (Ken Banner... 1 20874 soc.religion.christian From: simon@monu6.cc.monash.edu.au\\nSubject: S... 2 58936 sci.med From: jeffp@vetmed.wsu.edu (Jeff Parke)\\nSubje...","title":"Get News Data From sklearn.datasets"},{"location":"legacy_documentation/legacy_adv/#define-newsgroups-doctable","text":"This definition includes fields file_id, category, raw_text, subject, author, and tokenized_text. The extra columns compared to example_simple.ipynb are for storing extracted metadata. class NewsGroups(dt.DocTableLegacy): def __init__(self, fname): ''' DocTable class. Inputs: fname: fname is the name of the new sqlite database that will be used for instances of class. ''' tabname = 'newsgroups' super().__init__( fname=fname, tabname=tabname, colschema=( 'id integer primary key autoincrement', 'file_id int', 'category string', 'raw_text string', 'subject string', 'author string', 'tokenized_text blob', ), constraints=('UNIQUE(file_id)',) ) # create indices on file_id and category self.query(\"create index if not exists idx1 on \"+tabname+\"(file_id)\") self.query(\"create index if not exists idx2 on \"+tabname+\"(category)\") sng = NewsGroups(f'{tmp}/news_groupssss.db') print(sng) <Documents ct: 0> # add in raw data col_order = ('file_id','category','raw_text') data = [(dat['filename'],dat['target'],dat['text']) for ind,dat in ddf.iterrows()] sng.addmany(data,keys=col_order, ifnotunique='ignore') sng.getdf(limit=2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id file_id category raw_text subject author tokenized_text 0 1 21379 soc.religion.christian From: kbanner@philae.sas.upenn.edu (Ken Banner... None None None 1 2 20874 soc.religion.christian From: simon@monu6.cc.monash.edu.au\\nSubject: S... None None None","title":"Define NewsGroups DocTable"},{"location":"legacy_documentation/legacy_adv/#update-tokenized_text-column","text":"Use .get() to loop through rows in the database, and .update() to add in the newly extracted data. In this case, we simply tokenize the text using the python builtin split() function. query = sng.get(sel=('file_id','raw_text',)) for row in query: dat = {'tokenized_text':row['raw_text'].split(),} sng.update(dat, 'file_id == {}'.format(row['file_id'])) sng.getdf(limit=2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id file_id category raw_text subject author tokenized_text 0 1 21379 soc.religion.christian From: kbanner@philae.sas.upenn.edu (Ken Banner... None None [From:, kbanner@philae.sas.upenn.edu, (Ken, Ba... 1 2 20874 soc.religion.christian From: simon@monu6.cc.monash.edu.au\\nSubject: S... None None [From:, simon@monu6.cc.monash.edu.au, Subject:...","title":"Update \"tokenized_text\" Column"},{"location":"legacy_documentation/legacy_adv/#extract-email-metadata","text":"This example takes it even further by using the \"email\" package to parse apart the blog files. It then uses the extracted information to populate the corresponding fields in the DocTable. query = sng.get(sel=('file_id','raw_text',), asdict=False) for fid,text in query: e = email.message_from_string(text) auth = e['From'] if 'From' in e.keys() else '' subj = e['Subject'] if 'Subject' in e.keys() else '' tok = e.get_payload().split() dat = { 'tokenized_text':tok, 'author':auth, 'subject':subj, } sng.update(dat, 'file_id == {}'.format(fid)) sng.getdf(limit=2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id file_id category raw_text subject author tokenized_text 0 1 21379 soc.religion.christian From: kbanner@philae.sas.upenn.edu (Ken Banner... Re: SATANIC TOUNGES kbanner@philae.sas.upenn.edu (Ken Banner) [In, article, <May.5.02.53.10.1993.28880@athos... 1 2 20874 soc.religion.christian From: simon@monu6.cc.monash.edu.au\\nSubject: S... Saint Story St. Aloysius Gonzaga simon@monu6.cc.monash.edu.au [Heres, a, story, of, a, Saint, that, people, ... sng.getdf().to_csv('newsgroup20.csv')","title":"Extract Email Metadata"},{"location":"legacy_documentation/legacy_simple/","text":"DocTable Simple Example In this notebook, I show how to define a DocTable as a python class, populate the DocTable using the .add() and .addmany() commands, query data through generators and pandas dataframes, and finally update DocTable entries. from pprint import pprint from timeit import default_timer as timer from .legacy_helper import get_sklearn_newsgroups # for this example import sys sys.path.append('..') import doctable as dt # this will be the table object we use to interact with our database. Get News Data From sklearn.datasets Then parses into a dataframe. ddf = get_sklearn_newsgroups() print(ddf.info()) ddf.head(3) <class 'pandas.core.frame.DataFrame'> RangeIndex: 11314 entries, 0 to 11313 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 filename 11314 non-null object 1 target 11314 non-null object 2 text 11314 non-null object dtypes: object(3) memory usage: 265.3+ KB None .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } filename target text 0 21379 soc.religion.christian From: kbanner@philae.sas.upenn.edu (Ken Banner... 1 20874 soc.religion.christian From: simon@monu6.cc.monash.edu.au\\nSubject: S... 2 58936 sci.med From: jeffp@vetmed.wsu.edu (Jeff Parke)\\nSubje... Define DocTable Class This class definition will contain the columns, datatypes, unique constraints, and index commands needed for your DocTable. In moving your data from DataFrame to DocTable, you should consider column data types and custom indices carefully. # this class will represent the doctable. It inherits from DocTable a number of add/query/remove functions. # of course, you can add any additional methods to this class definition as you find useful. class SimpleNewsGroups(dt.DocTableLegacy): def __init__(self, fname): ''' This includes examples of init variables. See DocTable class for complete list of options. Inputs: fname: fname is the name of the new sqlite database that will be used for this class. ''' tabname = 'simplenewsgroups' super().__init__( fname=fname, tabname=tabname, colschema=( 'id integer primary key autoincrement', 'file_id int', 'category string', 'raw_text string', ) ) # this section defines any other commands that should be executed upon init # NOTICE: references tabname defined in the above __init__ function # extra commands to create index tables for fast lookup self.query(\"create index if not exists idx1 on \"+tabname+\"(file_id)\") self.query(\"create index if not exists idx2 on \"+tabname+\"(category)\") Create a connection to the database by constructing an instance of the class. If this is the first time you've run this code, it will create a new sqlite database file with no entries. sng = SimpleNewsGroups('simple_news_group.db') print(sng) <Documents ct: 0> Adding Data There are two common ways to add data to your DocTable. (1) Add in rows individually (2) Add in bulk with or without specifying column names # adds data one row at a time. Takes longer than bulk version start = timer() for ind,dat in ddf.iterrows(): row = {'file_id':int(dat['filename']), 'category':dat['target'], 'raw_text':dat['text']} sng.add(row, ifnotunique='replace') print((timer() - start)*1000, 'mil sec.') print(sng) 1629.2546929325908 mil sec. <Documents ct: 11314> # adds tuple data in bulk by specifying columns we are adding start = timer() col_order = ('file_id','category','raw_text') data = [(dat['filename'],dat['target'],dat['text']) for ind,dat in ddf.iterrows()] sng.addmany(data,keys=col_order, ifnotunique='replace') print((timer() - start)*1000, 'mil sec.') print(sng) 1269.7400120086968 mil sec. <Documents ct: 22628> Querying Data There are two primary ways of querying data from a DocTable: (1) retrieve one-by-one from generator using \".get()\" function. (2) retrieve all data in Pandas DataFrame suing \".getdf()\" function. result = sng.get( sel=('file_id','raw_text'), where='category == \"rec.motorcycles\"', orderby='file_id ASC', limit=3, ) for row in result: print(str(row['file_id'])+':', row['raw_text'][:50]) 72052: From: ivan@erich.triumf.ca (Ivan D. Reid) Subject: 72052: From: ivan@erich.triumf.ca (Ivan D. Reid) Subject: 101725: Subject: Re: Lexan Polish? From: jeff@mri.com (Jon result_df = sng.getdf( sel=('file_id','raw_text'), where='category == \"rec.motorcycles\"', orderby='file_id ASC', limit=5, ) result_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } file_id raw_text 0 72052 From: ivan@erich.triumf.ca (Ivan D. Reid)\\nSub... 1 72052 From: ivan@erich.triumf.ca (Ivan D. Reid)\\nSub... 2 101725 Subject: Re: Lexan Polish?\\nFrom: jeff@mri.com... 3 101725 Subject: Re: Lexan Polish?\\nFrom: jeff@mri.com... 4 102616 From: blgardne@javelin.sim.es.com (Dances With... Updating Data in DocTable The \".update()\" function will change entries in the DocTable. sng.update({'category':'nevermind',},where='file_id == \"103121\"') sng.getdf(where='file_id == \"103121\"') # to see update, look at \"category\" column entry .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id file_id category raw_text 0 395 103121 nevermind From: MJMUISE@1302.watstar.uwaterloo.ca (Mike ... 1 11709 103121 nevermind From: MJMUISE@1302.watstar.uwaterloo.ca (Mike ...","title":"DocTable Simple Example"},{"location":"legacy_documentation/legacy_simple/#doctable-simple-example","text":"In this notebook, I show how to define a DocTable as a python class, populate the DocTable using the .add() and .addmany() commands, query data through generators and pandas dataframes, and finally update DocTable entries. from pprint import pprint from timeit import default_timer as timer from .legacy_helper import get_sklearn_newsgroups # for this example import sys sys.path.append('..') import doctable as dt # this will be the table object we use to interact with our database.","title":"DocTable Simple Example"},{"location":"legacy_documentation/legacy_simple/#get-news-data-from-sklearndatasets","text":"Then parses into a dataframe. ddf = get_sklearn_newsgroups() print(ddf.info()) ddf.head(3) <class 'pandas.core.frame.DataFrame'> RangeIndex: 11314 entries, 0 to 11313 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 filename 11314 non-null object 1 target 11314 non-null object 2 text 11314 non-null object dtypes: object(3) memory usage: 265.3+ KB None .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } filename target text 0 21379 soc.religion.christian From: kbanner@philae.sas.upenn.edu (Ken Banner... 1 20874 soc.religion.christian From: simon@monu6.cc.monash.edu.au\\nSubject: S... 2 58936 sci.med From: jeffp@vetmed.wsu.edu (Jeff Parke)\\nSubje...","title":"Get News Data From sklearn.datasets"},{"location":"legacy_documentation/legacy_simple/#define-doctable-class","text":"This class definition will contain the columns, datatypes, unique constraints, and index commands needed for your DocTable. In moving your data from DataFrame to DocTable, you should consider column data types and custom indices carefully. # this class will represent the doctable. It inherits from DocTable a number of add/query/remove functions. # of course, you can add any additional methods to this class definition as you find useful. class SimpleNewsGroups(dt.DocTableLegacy): def __init__(self, fname): ''' This includes examples of init variables. See DocTable class for complete list of options. Inputs: fname: fname is the name of the new sqlite database that will be used for this class. ''' tabname = 'simplenewsgroups' super().__init__( fname=fname, tabname=tabname, colschema=( 'id integer primary key autoincrement', 'file_id int', 'category string', 'raw_text string', ) ) # this section defines any other commands that should be executed upon init # NOTICE: references tabname defined in the above __init__ function # extra commands to create index tables for fast lookup self.query(\"create index if not exists idx1 on \"+tabname+\"(file_id)\") self.query(\"create index if not exists idx2 on \"+tabname+\"(category)\") Create a connection to the database by constructing an instance of the class. If this is the first time you've run this code, it will create a new sqlite database file with no entries. sng = SimpleNewsGroups('simple_news_group.db') print(sng) <Documents ct: 0>","title":"Define DocTable Class"},{"location":"legacy_documentation/legacy_simple/#adding-data","text":"There are two common ways to add data to your DocTable. (1) Add in rows individually (2) Add in bulk with or without specifying column names # adds data one row at a time. Takes longer than bulk version start = timer() for ind,dat in ddf.iterrows(): row = {'file_id':int(dat['filename']), 'category':dat['target'], 'raw_text':dat['text']} sng.add(row, ifnotunique='replace') print((timer() - start)*1000, 'mil sec.') print(sng) 1629.2546929325908 mil sec. <Documents ct: 11314> # adds tuple data in bulk by specifying columns we are adding start = timer() col_order = ('file_id','category','raw_text') data = [(dat['filename'],dat['target'],dat['text']) for ind,dat in ddf.iterrows()] sng.addmany(data,keys=col_order, ifnotunique='replace') print((timer() - start)*1000, 'mil sec.') print(sng) 1269.7400120086968 mil sec. <Documents ct: 22628>","title":"Adding Data"},{"location":"legacy_documentation/legacy_simple/#querying-data","text":"There are two primary ways of querying data from a DocTable: (1) retrieve one-by-one from generator using \".get()\" function. (2) retrieve all data in Pandas DataFrame suing \".getdf()\" function. result = sng.get( sel=('file_id','raw_text'), where='category == \"rec.motorcycles\"', orderby='file_id ASC', limit=3, ) for row in result: print(str(row['file_id'])+':', row['raw_text'][:50]) 72052: From: ivan@erich.triumf.ca (Ivan D. Reid) Subject: 72052: From: ivan@erich.triumf.ca (Ivan D. Reid) Subject: 101725: Subject: Re: Lexan Polish? From: jeff@mri.com (Jon result_df = sng.getdf( sel=('file_id','raw_text'), where='category == \"rec.motorcycles\"', orderby='file_id ASC', limit=5, ) result_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } file_id raw_text 0 72052 From: ivan@erich.triumf.ca (Ivan D. Reid)\\nSub... 1 72052 From: ivan@erich.triumf.ca (Ivan D. Reid)\\nSub... 2 101725 Subject: Re: Lexan Polish?\\nFrom: jeff@mri.com... 3 101725 Subject: Re: Lexan Polish?\\nFrom: jeff@mri.com... 4 102616 From: blgardne@javelin.sim.es.com (Dances With...","title":"Querying Data"},{"location":"legacy_documentation/legacy_simple/#updating-data-in-doctable","text":"The \".update()\" function will change entries in the DocTable. sng.update({'category':'nevermind',},where='file_id == \"103121\"') sng.getdf(where='file_id == \"103121\"') # to see update, look at \"category\" column entry .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id file_id category raw_text 0 395 103121 nevermind From: MJMUISE@1302.watstar.uwaterloo.ca (Mike ... 1 11709 103121 nevermind From: MJMUISE@1302.watstar.uwaterloo.ca (Mike ...","title":"Updating Data in DocTable"},{"location":"legacy_documentation/parse_parsepipeline/","text":"ParsePipeline Basics Here I demonstrate the basics of parsing text using Spacy + doctable to tokenize text. Spacy does most of the heavy-lifting here to actually parse the document, and doctable methods handle the conversion from the Spacy Document object to a sequence of string tokens (words). from IPython import get_ipython import sys sys.path.append('..') import doctable ex_texts = [ 'I am pretty bored today. I have been stuck in quarantine for more than two months!', 'We are all waiting on Dr. Fauci to let us know when to return from quarantine.', 'On the bright side, I have more time to talk to my distant friends over video chat.', 'But still, I wish I could travel, go to bars, and go out to eat mrore often!', 'Here we show an example URL: https://devincornell.github.io/doctable/', 'And also one with <b><i>xml tags</i></b>.', ] 1. Build a ParsePipeline for Tokenization ParsePipeline makes it easy to define a processing pipeline as a list of functions (called components) to apply sequentially to each document in your corpus. You can use the .parsemany() method to run the pipeline on documents in paralel, or simply use the .parse() method to parse a single document. Our most basic pipeline uses a lambda function to split each text document by whitespace. parser_split = doctable.ParsePipeline([ lambda text: text.split(), ]) We then use the .parse() method to apply the pipeline to a single document. parsed_text = parser_split.parse(ex_texts[0]) print(parsed_text[:7]) ['I', 'am', 'pretty', 'bored', 'today.', 'I', 'have'] We can also use the .parsemany() method to parse all of our texts at once. Use the workers parameter to specify the number of processes to use if you want to use parallelization. parsed_texts = parser_split.parsemany(ex_texts, workers=2) # processes in parallel for text in parsed_texts: print(text[:7]) ['I', 'am', 'pretty', 'bored', 'today.', 'I', 'have'] ['We', 'are', 'all', 'waiting', 'on', 'Dr.', 'Fauci'] ['On', 'the', 'bright', 'side,', 'I', 'have', 'more'] ['But', 'still,', 'I', 'wish', 'I', 'could', 'travel,'] ['Here', 'we', 'show', 'an', 'example', 'URL:', 'https://devincornell.github.io/doctable/'] ['And', 'also', 'one', 'with', '<b><i>xml', 'tags</i></b>.'] 2. Use doctable Parsing Components doctable has some built-in methods for pre- and post-processing Spacy documents. This list includes all functions in the doctable.parse namespace, and you can access them using the doctable.Comp function. print(doctable.components) {'preprocess': <function preprocess at 0x7fb23bc901f0>, 'tokenize': <function tokenize at 0x7fb23bc90310>, 'parse_tok': <function parse_tok at 0x7fb23bc903a0>, 'keep_tok': <function keep_tok at 0x7fb23bc90430>, 'merge_tok_spans': <function merge_tok_spans at 0x7fb23bc904c0>, 'merge_tok_ngrams': <function merge_tok_ngrams at 0x7fb23bc90550>, 'get_parsetrees': <function get_parsetrees at 0x7fb23bc90670>} preproc = doctable.Comp('preprocess', replace_url='_URL_', replace_xml='') print(ex_texts[4]) preproc(ex_texts[4]) Here we show an example URL: https://devincornell.github.io/doctable/ 'Here we show an example URL: _URL_' Now we show a pipeline that uses the doctable preprocess method to remove xml tags and urls, the Spacy nlp model to parse the document, and the built-in tokenize method to convert the spacy doc object to a list of tokens. from doctable import Comp import spacy nlp = spacy.load('en_core_web_sm') parser_tok = doctable.ParsePipeline([ Comp('preprocess', replace_xml='', replace_url='XXURLXX'), nlp, Comp('tokenize', split_sents=False), ]) docs = parser_tok.parsemany(ex_texts) for doc in docs: print(doc[:10]) [I, am, pretty, bored, today, ., I, have, been, stuck] [We, are, all, waiting, on, Dr., Fauci, to, let, us] [On, the, bright, side, ,, I, have, more, time, to] [But, still, ,, I, wish, I, could, travel, ,, go] [Here, we, show, an, example, URL, :, XXURLXX] [And, also, one, with, xml, tags, .] 3. More Complicated Pipelines Now we show a more complicated mode. The function tokenize also takes two additional methods: keep_tok_func determines whether a Spacy token should be included in the final document, and the parse_tok_func determines how the spacy token objects should be converted to strings. We access the doctable keep_tok and parse_tok methods using the same Comp function to create nested parameter lists. parser_full = doctable.ParsePipeline([ # preprocess to remove xml tags and replace URLs (doctable.parse.preprocess) Comp('preprocess', replace_xml='', replace_url='XXURLXX'), nlp, # spacy nlp parser object # merge spacy multi-word named entities (doctable.parse.merge_tok_spans) Comp('merge_tok_spans', merge_ents=True, merge_noun_chunks=False), # tokenize document Comp('tokenize', **{ 'split_sents': False, # choose tokens to keep (doctable.parse.keep_tok) 'keep_tok_func': Comp('keep_tok', **{ 'keep_whitespace': False, # don't keep whitespace 'keep_punct': True, # keep punctuation and stopwords 'keep_stop': True, }), # choose how to convert Spacy token t text (doctable.parse.parse_tok) 'parse_tok_func': Comp('parse_tok', **{ 'format_ents': True, 'lemmatize': False, 'num_replacement': 'NUM', 'ent_convert': lambda e: e.text.upper(), # function to capitalize named entities }) }) ]) len(parser_full.components) 4 parsed_docs = parser_full.parsemany(ex_texts) for tokens in parsed_docs: print(tokens[:10]) ['i', 'am', 'pretty', 'bored', 'TODAY', '.', 'i', 'have', 'been', 'stuck'] ['we', 'are', 'all', 'waiting', 'on', 'dr.', 'FAUCI', 'to', 'let', 'us'] ['on', 'the', 'bright', 'side', ',', 'i', 'have', 'more', 'time', 'to'] ['but', 'still', ',', 'i', 'wish', 'i', 'could', 'travel', ',', 'go'] ['here', 'we', 'show', 'an', 'example', 'url', ':', 'xxurlxx'] ['and', 'also', 'NUM', 'with', 'xml', 'tags', '.'] These are the fundamentals of building ParsePipeline s in doctable. While these tools are totally optional, I believe they make it easier to structure your code for text analysis applications.","title":"ParsePipeline Basics"},{"location":"legacy_documentation/parse_parsepipeline/#parsepipeline-basics","text":"Here I demonstrate the basics of parsing text using Spacy + doctable to tokenize text. Spacy does most of the heavy-lifting here to actually parse the document, and doctable methods handle the conversion from the Spacy Document object to a sequence of string tokens (words). from IPython import get_ipython import sys sys.path.append('..') import doctable ex_texts = [ 'I am pretty bored today. I have been stuck in quarantine for more than two months!', 'We are all waiting on Dr. Fauci to let us know when to return from quarantine.', 'On the bright side, I have more time to talk to my distant friends over video chat.', 'But still, I wish I could travel, go to bars, and go out to eat mrore often!', 'Here we show an example URL: https://devincornell.github.io/doctable/', 'And also one with <b><i>xml tags</i></b>.', ]","title":"ParsePipeline Basics"},{"location":"legacy_documentation/parse_parsepipeline/#1-build-a-parsepipeline-for-tokenization","text":"ParsePipeline makes it easy to define a processing pipeline as a list of functions (called components) to apply sequentially to each document in your corpus. You can use the .parsemany() method to run the pipeline on documents in paralel, or simply use the .parse() method to parse a single document. Our most basic pipeline uses a lambda function to split each text document by whitespace. parser_split = doctable.ParsePipeline([ lambda text: text.split(), ]) We then use the .parse() method to apply the pipeline to a single document. parsed_text = parser_split.parse(ex_texts[0]) print(parsed_text[:7]) ['I', 'am', 'pretty', 'bored', 'today.', 'I', 'have'] We can also use the .parsemany() method to parse all of our texts at once. Use the workers parameter to specify the number of processes to use if you want to use parallelization. parsed_texts = parser_split.parsemany(ex_texts, workers=2) # processes in parallel for text in parsed_texts: print(text[:7]) ['I', 'am', 'pretty', 'bored', 'today.', 'I', 'have'] ['We', 'are', 'all', 'waiting', 'on', 'Dr.', 'Fauci'] ['On', 'the', 'bright', 'side,', 'I', 'have', 'more'] ['But', 'still,', 'I', 'wish', 'I', 'could', 'travel,'] ['Here', 'we', 'show', 'an', 'example', 'URL:', 'https://devincornell.github.io/doctable/'] ['And', 'also', 'one', 'with', '<b><i>xml', 'tags</i></b>.']","title":"1. Build a ParsePipeline for Tokenization"},{"location":"legacy_documentation/parse_parsepipeline/#2-use-doctable-parsing-components","text":"doctable has some built-in methods for pre- and post-processing Spacy documents. This list includes all functions in the doctable.parse namespace, and you can access them using the doctable.Comp function. print(doctable.components) {'preprocess': <function preprocess at 0x7fb23bc901f0>, 'tokenize': <function tokenize at 0x7fb23bc90310>, 'parse_tok': <function parse_tok at 0x7fb23bc903a0>, 'keep_tok': <function keep_tok at 0x7fb23bc90430>, 'merge_tok_spans': <function merge_tok_spans at 0x7fb23bc904c0>, 'merge_tok_ngrams': <function merge_tok_ngrams at 0x7fb23bc90550>, 'get_parsetrees': <function get_parsetrees at 0x7fb23bc90670>} preproc = doctable.Comp('preprocess', replace_url='_URL_', replace_xml='') print(ex_texts[4]) preproc(ex_texts[4]) Here we show an example URL: https://devincornell.github.io/doctable/ 'Here we show an example URL: _URL_' Now we show a pipeline that uses the doctable preprocess method to remove xml tags and urls, the Spacy nlp model to parse the document, and the built-in tokenize method to convert the spacy doc object to a list of tokens. from doctable import Comp import spacy nlp = spacy.load('en_core_web_sm') parser_tok = doctable.ParsePipeline([ Comp('preprocess', replace_xml='', replace_url='XXURLXX'), nlp, Comp('tokenize', split_sents=False), ]) docs = parser_tok.parsemany(ex_texts) for doc in docs: print(doc[:10]) [I, am, pretty, bored, today, ., I, have, been, stuck] [We, are, all, waiting, on, Dr., Fauci, to, let, us] [On, the, bright, side, ,, I, have, more, time, to] [But, still, ,, I, wish, I, could, travel, ,, go] [Here, we, show, an, example, URL, :, XXURLXX] [And, also, one, with, xml, tags, .]","title":"2. Use doctable Parsing Components"},{"location":"legacy_documentation/parse_parsepipeline/#3-more-complicated-pipelines","text":"Now we show a more complicated mode. The function tokenize also takes two additional methods: keep_tok_func determines whether a Spacy token should be included in the final document, and the parse_tok_func determines how the spacy token objects should be converted to strings. We access the doctable keep_tok and parse_tok methods using the same Comp function to create nested parameter lists. parser_full = doctable.ParsePipeline([ # preprocess to remove xml tags and replace URLs (doctable.parse.preprocess) Comp('preprocess', replace_xml='', replace_url='XXURLXX'), nlp, # spacy nlp parser object # merge spacy multi-word named entities (doctable.parse.merge_tok_spans) Comp('merge_tok_spans', merge_ents=True, merge_noun_chunks=False), # tokenize document Comp('tokenize', **{ 'split_sents': False, # choose tokens to keep (doctable.parse.keep_tok) 'keep_tok_func': Comp('keep_tok', **{ 'keep_whitespace': False, # don't keep whitespace 'keep_punct': True, # keep punctuation and stopwords 'keep_stop': True, }), # choose how to convert Spacy token t text (doctable.parse.parse_tok) 'parse_tok_func': Comp('parse_tok', **{ 'format_ents': True, 'lemmatize': False, 'num_replacement': 'NUM', 'ent_convert': lambda e: e.text.upper(), # function to capitalize named entities }) }) ]) len(parser_full.components) 4 parsed_docs = parser_full.parsemany(ex_texts) for tokens in parsed_docs: print(tokens[:10]) ['i', 'am', 'pretty', 'bored', 'TODAY', '.', 'i', 'have', 'been', 'stuck'] ['we', 'are', 'all', 'waiting', 'on', 'dr.', 'FAUCI', 'to', 'let', 'us'] ['on', 'the', 'bright', 'side', ',', 'i', 'have', 'more', 'time', 'to'] ['but', 'still', ',', 'i', 'wish', 'i', 'could', 'travel', ',', 'go'] ['here', 'we', 'show', 'an', 'example', 'url', ':', 'xxurlxx'] ['and', 'also', 'NUM', 'with', 'xml', 'tags', '.'] These are the fundamentals of building ParsePipeline s in doctable. While these tools are totally optional, I believe they make it easier to structure your code for text analysis applications.","title":"3. More Complicated Pipelines"},{"location":"legacy_documentation/parse_parsetrees/","text":"Working with doctable Parsetrees Here I'll show you how to extract and use parsetrees in your doctable using Spacy + doctable. The motivation is that parsetree information in raw Spacy Document objects are very large and not suitable for storage when using large corpora. We solve this by simply converting the Spacy Document object to a tree data structure built from python lists and dictionaries, and use the ParseTree object to serialize, de-serialize, and interact with the tree structure. We use this feature using the get_parsetrees pipeline component after the spacy parser. Check the docs to learn more about this function. You can see more examples of creating parse pipelines in our overview examples . import spacy nlp = spacy.load('en_core_web_sm') import pandas as pd import sys sys.path.append('..') import doctable First we define some example text docuemnts, Star Wars themed. text = 'Help me Obi-Wan Kenobi. You\u2019re my only hope. ' \\ 'I find your lack of faith disturbing. ' \\ 'Do, or do not - there is no try. ' text 'Help me Obi-Wan Kenobi. You\u2019re my only hope. I find your lack of faith disturbing. Do, or do not - there is no try. ' Creating ParseTreeDoc Objects The most direct way of creating a parsetree is to parse the desired text using the spacy language model, then use ParseTreeDoc.from_spacy() to construct the ParseTreeDoc . The ParseTreeDoc object is a container for parsetree objects representing each of the sentences identified with the SpaCy parser. spacydoc = nlp(text) doc = doctable.ParseTreeDoc.from_spacy(spacydoc) print(f'{len(doc)} sentences of type {type(doc)}') 4 sentences of type <class 'doctable.parse.documents.parsetreedoc.ParseTreeDoc'> The most important arguments to parse_tok_func are text_parse_func and userdata_map . text_parse_func determines the mapping from a spacy doc object to the text representation of each token accessed through token.text . By default this parameter is set to lambda d: d.text . userdata_map is a dictionary mapping an attribute name to a function. You can, for instance, extract info from the original spacy doc object through this method. I'll explain later how these attributes can be accessed and used. doc = doctable.ParseTreeDoc.from_spacy(spacydoc, text_parse_func=lambda spacydoc: spacydoc.text, userdata_map = { 'lower': lambda spacydoc: spacydoc.text.lower().strip(), 'lemma': lambda spacydoc: spacydoc.lemma_, 'like_num': lambda spacydoc: spacydoc.like_num, } ) print(doc[0]) # show the first sentence ParseTree(Help me Obi - Wan Kenobi .) Working With ParseTree s ParseTreeDoc objects represent sequences of ParseTree objects identified by the spacy language parser. You can see we can access individual sentence parsetrees using numerical indexing or through iteration. print(doc[0]) for sent in doc: print(sent) ParseTree(Help me Obi - Wan Kenobi .) ParseTree(Help me Obi - Wan Kenobi .) ParseTree(You \u2019re my only hope .) ParseTree(I find your lack of faith disturbing .) ParseTree(Do , or do not - there is no try .) Now we will show how to work with ParseTree objects. These objects are collections of tokens that can be accessed either as a tree (based on the structure of the dependency tree produced by spacy), or as an ordered sequence. We can use numerical indexing or iteration to interact with individual tokens. for token in doc[0]: print(token) Help me Obi - Wan Kenobi . We can work with the tree structure of a ParseTree object using the root property. print(doc[0].root) Help And access the children of a given token using the childs property. The following tokens are children of the root token. for child in doc[0].root.childs: print(child) me Kenobi . These objects can be serialized using the .as_dict() method and de-serialized using the .from_dict() method. serialized = doc.as_dict() deserialized = doctable.ParseTreeDoc.from_dict(serialized) for sent in deserialized: print(sent) ParseTree(Help me Obi - Wan Kenobi .) ParseTree(You \u2019re my only hope .) ParseTree(I find your lack of faith disturbing .) ParseTree(Do , or do not - there is no try .) More About Token s Each token in a ParseTree is represented by a Token object. These objects maintain the tree structure of a parsetree, and each node contains some default information as well as optional and custom information. These are the most important member variables: Member Variables i : index of token in sentence text : text representation of token tag : the part-of-speech tag offered by the dependency parser (different from POS tagger) dep : the dependency relation to parent object. See the Spacy annotation docs for more detail. parent : reference to parent node childs : list of references to child nodes Optional Member Variables The following are provided if the associated spacy parser component was enabled. pos : part-of-speech tag created if user enablled POS 'tagger' in Spacy. See Spacy POS tag docs for more detail. Also check out docs for UPOS tags . ent : named entity type of token (if NER was enabled when creating parsetree). See Spacy NER docs for more detail. for tok in doc[0][:3]: print(f\"{tok.text}:\\n\\ti={tok.i}\\n\\ttag={tok.tag}\\n\\tdep={tok.dep}\\n\\tent={tok.ent}\\n\\tpos={tok.pos}\") Help: i=0 tag=VB dep=ROOT ent= pos=VERB me: i=1 tag=PRP dep=dobj ent= pos=PRON Obi: i=2 tag=NNP dep=compound ent= pos=PROPN We can also access the custom token properties provided to the ParseTreeDoc.from_spacy() method earlier. for token in doc[0]: print(f\"{token.text}: {token['lemma']}\") Help: help me: I Obi: Obi -: - Wan: Wan Kenobi: Kenobi .: . Recursive Functions on Parsetrees We can also navigate the tree structure of parsetrees using recursive functions. Here I simply print out the trajectory of this recursive function. def print_recursion(tok, level=0): if not tok.childs: print(' '*level + 'base node', tok) else: print(' '*level + 'entering', tok) for child in tok.childs: print_recursion(child, level+1) print(' '*level + 'leaving', tok) print_recursion(doc[0].root) entering Help base node me entering Kenobi entering Wan base node Obi base node - leaving Wan leaving Kenobi base node . leaving Help Create Using ParsePipeline s The most common use case, however, probably involves the creation of of a ParsePipeline in which the end result will be a ParseTreeDoc . We make this using the get_parsetrees pipeline component, and here we show several of the possible arguments. parser = doctable.ParsePipeline([ nlp, # the spacy parser doctable.Comp('get_parsetrees', **{ 'text_parse_func': lambda spacydoc: spacydoc.text, 'userdata_map': { 'lower': lambda spacydoc: spacydoc.text.lower().strip(), 'lemma': lambda spacydoc: spacydoc.lemma_, 'like_num': lambda spacydoc: spacydoc.like_num, } }) ]) parser.components [<spacy.lang.en.English at 0x7f3d584c7dc0>, functools.partial(<function get_parsetrees at 0x7f3d27b95dc0>, text_parse_func=<function <lambda> at 0x7f3d26a17a60>, userdata_map={'lower': <function <lambda> at 0x7f3d26a17160>, 'lemma': <function <lambda> at 0x7f3d26a170d0>, 'like_num': <function <lambda> at 0x7f3d26a17040>})] You can see that the parser provides the same output as we got before with ParseTreeDoc.from_spacy() . for sent in parser.parse(text): print(sent) ParseTree(Help me Obi - Wan Kenobi .) ParseTree(You \u2019re my only hope .) ParseTree(I find your lack of faith disturbing .) ParseTree(Do , or do not - there is no try .)","title":"Working with doctable Parsetrees"},{"location":"legacy_documentation/parse_parsetrees/#working-with-doctable-parsetrees","text":"Here I'll show you how to extract and use parsetrees in your doctable using Spacy + doctable. The motivation is that parsetree information in raw Spacy Document objects are very large and not suitable for storage when using large corpora. We solve this by simply converting the Spacy Document object to a tree data structure built from python lists and dictionaries, and use the ParseTree object to serialize, de-serialize, and interact with the tree structure. We use this feature using the get_parsetrees pipeline component after the spacy parser. Check the docs to learn more about this function. You can see more examples of creating parse pipelines in our overview examples . import spacy nlp = spacy.load('en_core_web_sm') import pandas as pd import sys sys.path.append('..') import doctable First we define some example text docuemnts, Star Wars themed. text = 'Help me Obi-Wan Kenobi. You\u2019re my only hope. ' \\ 'I find your lack of faith disturbing. ' \\ 'Do, or do not - there is no try. ' text 'Help me Obi-Wan Kenobi. You\u2019re my only hope. I find your lack of faith disturbing. Do, or do not - there is no try. '","title":"Working with doctable Parsetrees"},{"location":"legacy_documentation/parse_parsetrees/#creating-parsetreedoc-objects","text":"The most direct way of creating a parsetree is to parse the desired text using the spacy language model, then use ParseTreeDoc.from_spacy() to construct the ParseTreeDoc . The ParseTreeDoc object is a container for parsetree objects representing each of the sentences identified with the SpaCy parser. spacydoc = nlp(text) doc = doctable.ParseTreeDoc.from_spacy(spacydoc) print(f'{len(doc)} sentences of type {type(doc)}') 4 sentences of type <class 'doctable.parse.documents.parsetreedoc.ParseTreeDoc'> The most important arguments to parse_tok_func are text_parse_func and userdata_map . text_parse_func determines the mapping from a spacy doc object to the text representation of each token accessed through token.text . By default this parameter is set to lambda d: d.text . userdata_map is a dictionary mapping an attribute name to a function. You can, for instance, extract info from the original spacy doc object through this method. I'll explain later how these attributes can be accessed and used. doc = doctable.ParseTreeDoc.from_spacy(spacydoc, text_parse_func=lambda spacydoc: spacydoc.text, userdata_map = { 'lower': lambda spacydoc: spacydoc.text.lower().strip(), 'lemma': lambda spacydoc: spacydoc.lemma_, 'like_num': lambda spacydoc: spacydoc.like_num, } ) print(doc[0]) # show the first sentence ParseTree(Help me Obi - Wan Kenobi .)","title":"Creating ParseTreeDoc Objects"},{"location":"legacy_documentation/parse_parsetrees/#working-with-parsetrees","text":"ParseTreeDoc objects represent sequences of ParseTree objects identified by the spacy language parser. You can see we can access individual sentence parsetrees using numerical indexing or through iteration. print(doc[0]) for sent in doc: print(sent) ParseTree(Help me Obi - Wan Kenobi .) ParseTree(Help me Obi - Wan Kenobi .) ParseTree(You \u2019re my only hope .) ParseTree(I find your lack of faith disturbing .) ParseTree(Do , or do not - there is no try .) Now we will show how to work with ParseTree objects. These objects are collections of tokens that can be accessed either as a tree (based on the structure of the dependency tree produced by spacy), or as an ordered sequence. We can use numerical indexing or iteration to interact with individual tokens. for token in doc[0]: print(token) Help me Obi - Wan Kenobi . We can work with the tree structure of a ParseTree object using the root property. print(doc[0].root) Help And access the children of a given token using the childs property. The following tokens are children of the root token. for child in doc[0].root.childs: print(child) me Kenobi . These objects can be serialized using the .as_dict() method and de-serialized using the .from_dict() method. serialized = doc.as_dict() deserialized = doctable.ParseTreeDoc.from_dict(serialized) for sent in deserialized: print(sent) ParseTree(Help me Obi - Wan Kenobi .) ParseTree(You \u2019re my only hope .) ParseTree(I find your lack of faith disturbing .) ParseTree(Do , or do not - there is no try .)","title":"Working With ParseTrees"},{"location":"legacy_documentation/parse_parsetrees/#more-about-tokens","text":"Each token in a ParseTree is represented by a Token object. These objects maintain the tree structure of a parsetree, and each node contains some default information as well as optional and custom information. These are the most important member variables:","title":"More About Tokens"},{"location":"legacy_documentation/parse_parsetrees/#member-variables","text":"i : index of token in sentence text : text representation of token tag : the part-of-speech tag offered by the dependency parser (different from POS tagger) dep : the dependency relation to parent object. See the Spacy annotation docs for more detail. parent : reference to parent node childs : list of references to child nodes","title":"Member Variables"},{"location":"legacy_documentation/parse_parsetrees/#optional-member-variables","text":"The following are provided if the associated spacy parser component was enabled. pos : part-of-speech tag created if user enablled POS 'tagger' in Spacy. See Spacy POS tag docs for more detail. Also check out docs for UPOS tags . ent : named entity type of token (if NER was enabled when creating parsetree). See Spacy NER docs for more detail. for tok in doc[0][:3]: print(f\"{tok.text}:\\n\\ti={tok.i}\\n\\ttag={tok.tag}\\n\\tdep={tok.dep}\\n\\tent={tok.ent}\\n\\tpos={tok.pos}\") Help: i=0 tag=VB dep=ROOT ent= pos=VERB me: i=1 tag=PRP dep=dobj ent= pos=PRON Obi: i=2 tag=NNP dep=compound ent= pos=PROPN We can also access the custom token properties provided to the ParseTreeDoc.from_spacy() method earlier. for token in doc[0]: print(f\"{token.text}: {token['lemma']}\") Help: help me: I Obi: Obi -: - Wan: Wan Kenobi: Kenobi .: .","title":"Optional Member Variables"},{"location":"legacy_documentation/parse_parsetrees/#recursive-functions-on-parsetrees","text":"We can also navigate the tree structure of parsetrees using recursive functions. Here I simply print out the trajectory of this recursive function. def print_recursion(tok, level=0): if not tok.childs: print(' '*level + 'base node', tok) else: print(' '*level + 'entering', tok) for child in tok.childs: print_recursion(child, level+1) print(' '*level + 'leaving', tok) print_recursion(doc[0].root) entering Help base node me entering Kenobi entering Wan base node Obi base node - leaving Wan leaving Kenobi base node . leaving Help","title":"Recursive Functions on Parsetrees"},{"location":"legacy_documentation/parse_parsetrees/#create-using-parsepipelines","text":"The most common use case, however, probably involves the creation of of a ParsePipeline in which the end result will be a ParseTreeDoc . We make this using the get_parsetrees pipeline component, and here we show several of the possible arguments. parser = doctable.ParsePipeline([ nlp, # the spacy parser doctable.Comp('get_parsetrees', **{ 'text_parse_func': lambda spacydoc: spacydoc.text, 'userdata_map': { 'lower': lambda spacydoc: spacydoc.text.lower().strip(), 'lemma': lambda spacydoc: spacydoc.lemma_, 'like_num': lambda spacydoc: spacydoc.like_num, } }) ]) parser.components [<spacy.lang.en.English at 0x7f3d584c7dc0>, functools.partial(<function get_parsetrees at 0x7f3d27b95dc0>, text_parse_func=<function <lambda> at 0x7f3d26a17a60>, userdata_map={'lower': <function <lambda> at 0x7f3d26a17160>, 'lemma': <function <lambda> at 0x7f3d26a170d0>, 'like_num': <function <lambda> at 0x7f3d26a17040>})] You can see that the parser provides the same output as we got before with ParseTreeDoc.from_spacy() . for sent in parser.parse(text): print(sent) ParseTree(Help me Obi - Wan Kenobi .) ParseTree(You \u2019re my only hope .) ParseTree(I find your lack of faith disturbing .) ParseTree(Do , or do not - there is no try .)","title":"Create Using ParsePipelines"}]}