{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DocParser Class\n",
    "This example gives an overview of DocParser functionality. See [reference docs for more detail](https://devincornell.github.io/doctable/ref/doctable.DocParser.html). The class includes only classmethods and staticmethods, so it is meant to be inhereted rather than instantiated.\n",
    "\n",
    "The DocParser class currently facilitates conversion from spacy doc objects to one of two object types:\n",
    "1. **Token lists**: The `.tokenize_doc()` method produces sequences of token strings used for input into algorithms like word2vec, topic modeling, co-occurrence analyses, etc. These require no doctable-specific functionality to manipulate. To accomplish this, it also draws on `.parse_tok()` to specify rules for converting spacy token objects to strings and `.use_tok()` to decide whether or not to include a token.\n",
    "2. **Parsetrees**: The `get_parsetrees()` method produces objects used for grammatical structure analysis. Generally contains token information along with gramattical relationships observed in the original sentences. By default relies on the `.parse_tok()` method to convert token objects to string representations. DocParser can convert these to nested dictionaries or also provides a built-in `ParseTree` class for working with them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import doctable as dt\n",
    "from spacy import displacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "exstr = 'James will paint the house for $20 (twenty dollars). He is a rule-breaker'\n",
    "doc = nlp(exstr)\n",
    "doc"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "James will paint the house for $20 (twenty dollars). He is a rule-breaker"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "The first step before using spacy to parse a document is to preprocess, which usually means removing artifacts. The `.preprocess()` method has features for replacing urls, replacing xml tags, and removing digits."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "advstr = 'James said he will paint the house red for $20 (twenty dollars). He is such a <i>rule-breaker</i>: http://rulebreaking.com'\n",
    "dt.DocParser.preprocess(advstr, replace_url='URL', replace_xml='', replace_digits='DIG')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'James said he will paint the house red for $DIG (twenty dollars). He is such a rule-breaker: URL'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normally after preprocessing you will then feed into the spacy parser."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenization\n",
    "Many text analysis applications begin with converting raw text into sequences of tokens. The `.tokenize_doc()`, `.use_tok()`, and `.parse_tok()` methods are convenient tools to assist with this task."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# basic doc tokenizer works like this\n",
    "print(dt.DocParser.tokenize_doc(doc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '20', '(', 'twenty', 'dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# and there are a number of options when using this method\n",
    "print(dt.DocParser.tokenize_doc(doc, split_sents=True, merge_ents=True, merge_noun_chunks=True))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['James', 'will', 'paint', 'the house', 'for', '$', '20', '(', 'twenty dollars', ')', '.'], ['he', 'is', 'a rule-breaker']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# you can override functionality to decide to keep the token and to convert tok object to str\n",
    "use_tok = lambda tok: not tok.like_num\n",
    "parse_tok = lambda tok: tok.lower_\n",
    "print(dt.DocParser.tokenize_doc(doc, use_tok_func=use_tok, parse_tok_func=parse_tok))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['james', 'will', 'paint', 'the house', 'for', '$', '(', 'twenty dollars', ')', '.', 'he', 'is', 'a rule-breaker']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# or use DocParser .use_tok() and .parse_tok() methods for additional features.\n",
    "# this filters out stopwords and converts all number quantities to __NUM__\n",
    "use_tok = lambda tok: dt.DocParser.use_tok(tok, filter_stop=True)\n",
    "parse_tok = lambda tok: dt.DocParser.parse_tok(tok, format_ents=True, replace_num='__NUM__')\n",
    "print(dt.DocParser.tokenize_doc(doc, use_tok_func=use_tok, parse_tok_func=parse_tok))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['James', 'paint', 'the house', '$', '__NUM__', '(', 'Twenty Dollars', ')', '.', 'a rule-breaker']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parsetree Extraction\n",
    "In cases where you want to keep information about gramattical structure in your parsed document, use the `.get_parsetrees()` method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# extracts a parsetree for each sentence in the document\n",
    "print(dt.DocParser.get_parsetrees(doc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ParseTree(['James', 'will', 'paint', 'the house', 'for', '$', '20', '(', 'twenty dollars', ')', '.']), ParseTree(['he', 'is', 'a rule-breaker'])]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# by default this works like .tokenize_doc() except that it doesn't remove toks\n",
    "# it includes a lot of other information as well\n",
    "# it will include .pos and .ent if they were available in spacy parsing\n",
    "s1, s2 = dt.DocParser.get_parsetrees(doc)\n",
    "print([t.tok for t in s1])\n",
    "print([t.dep for t in s1])\n",
    "print([t.tag for t in s1])\n",
    "print([t.pos for t in s1])\n",
    "print([t.ent for t in s1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['James', 'will', 'paint', 'the house', 'for', '$', '20', '(', 'twenty dollars', ')', '.']\n",
      "['nsubj', 'aux', 'ROOT', 'dobj', 'prep', 'nmod', 'pobj', 'punct', 'appos', 'punct', 'punct']\n",
      "['NNP', 'MD', 'VB', 'NN', 'IN', '$', 'CD', '-LRB-', 'NNS', '-RRB-', '.']\n",
      "['PROPN', 'AUX', 'VERB', 'NOUN', 'ADP', 'SYM', 'NUM', 'PUNCT', 'NOUN', 'PUNCT', 'PUNCT']\n",
      "['ORG', '', '', '', '', '', 'MONEY', '', 'MONEY', '', '']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# much like .tokenize_doc(), you can specify the parse_token functionality \n",
    "#     that will be applied to the .tok property\n",
    "parse_tok = lambda tok: tok.text.upper()\n",
    "s1, s2 = dt.DocParser.get_parsetrees(doc, parse_tok_func=parse_tok)\n",
    "print(s1.toks)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['JAMES', 'WILL', 'PAINT', 'THE HOUSE', 'FOR', '$', '20', '(', 'TWENTY DOLLARS', ')', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# to attach additional info to parsetree tokens, use info_func_map\n",
    "infomap = {'is_stop':lambda tok: tok.is_stop, 'like_num': lambda tok: tok.like_num}\n",
    "s1, s2 = dt.DocParser.get_parsetrees(doc, info_func_map=infomap)\n",
    "print(s1.toks)\n",
    "print([t.info['like_num'] for t in s1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['James', 'will', 'paint', 'the house', 'for', '$', '20', '(', 'twenty dollars', ')', '.']\n",
      "[False, False, False, False, False, False, True, False, False, False, False]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# we can convert to dict to see tree structure\n",
    "s1, s2 = dt.DocParser.get_parsetrees(doc)\n",
    "s1.asdict()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'i': 2,\n",
       " 'tok': 'paint',\n",
       " 'tag': 'VB',\n",
       " 'dep': 'ROOT',\n",
       " 'info': {},\n",
       " 'childs': [{'i': 0,\n",
       "   'tok': 'James',\n",
       "   'tag': 'NNP',\n",
       "   'dep': 'nsubj',\n",
       "   'info': {},\n",
       "   'childs': [],\n",
       "   'pos': 'PROPN',\n",
       "   'ent': 'ORG'},\n",
       "  {'i': 1,\n",
       "   'tok': 'will',\n",
       "   'tag': 'MD',\n",
       "   'dep': 'aux',\n",
       "   'info': {},\n",
       "   'childs': [],\n",
       "   'pos': 'AUX',\n",
       "   'ent': ''},\n",
       "  {'i': 3,\n",
       "   'tok': 'the house',\n",
       "   'tag': 'NN',\n",
       "   'dep': 'dobj',\n",
       "   'info': {},\n",
       "   'childs': [],\n",
       "   'pos': 'NOUN',\n",
       "   'ent': ''},\n",
       "  {'i': 4,\n",
       "   'tok': 'for',\n",
       "   'tag': 'IN',\n",
       "   'dep': 'prep',\n",
       "   'info': {},\n",
       "   'childs': [{'i': 6,\n",
       "     'tok': '20',\n",
       "     'tag': 'CD',\n",
       "     'dep': 'pobj',\n",
       "     'info': {},\n",
       "     'childs': [{'i': 5,\n",
       "       'tok': '$',\n",
       "       'tag': '$',\n",
       "       'dep': 'nmod',\n",
       "       'info': {},\n",
       "       'childs': [],\n",
       "       'pos': 'SYM',\n",
       "       'ent': ''},\n",
       "      {'i': 7,\n",
       "       'tok': '(',\n",
       "       'tag': '-LRB-',\n",
       "       'dep': 'punct',\n",
       "       'info': {},\n",
       "       'childs': [],\n",
       "       'pos': 'PUNCT',\n",
       "       'ent': ''},\n",
       "      {'i': 8,\n",
       "       'tok': 'twenty dollars',\n",
       "       'tag': 'NNS',\n",
       "       'dep': 'appos',\n",
       "       'info': {},\n",
       "       'childs': [],\n",
       "       'pos': 'NOUN',\n",
       "       'ent': 'MONEY'},\n",
       "      {'i': 9,\n",
       "       'tok': ')',\n",
       "       'tag': '-RRB-',\n",
       "       'dep': 'punct',\n",
       "       'info': {},\n",
       "       'childs': [],\n",
       "       'pos': 'PUNCT',\n",
       "       'ent': ''}],\n",
       "     'pos': 'NUM',\n",
       "     'ent': 'MONEY'}],\n",
       "   'pos': 'ADP',\n",
       "   'ent': ''},\n",
       "  {'i': 10,\n",
       "   'tok': '.',\n",
       "   'tag': '.',\n",
       "   'dep': 'punct',\n",
       "   'info': {},\n",
       "   'childs': [],\n",
       "   'pos': 'PUNCT',\n",
       "   'ent': ''}],\n",
       " 'pos': 'VERB',\n",
       " 'ent': ''}"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Working with ParseTree objects\n",
    "ParseTree objects can be parsed either iteratively (as we showed earlier), or recursively. The `.bubble_accum()` and `.bubble_reduce()` methods are convenient ways of using recursive functions on parsetrees. The `.root` property is also a useful way to write your own recursive functions on the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# .bubble_accum() \n",
    "def get_ents(pnode):\n",
    "    if pnode.ent != '':\n",
    "        return [pnode]\n",
    "    else:\n",
    "        return []\n",
    "s1, s2 = dt.DocParser.get_parsetrees(doc)\n",
    "s1.bubble_accum(get_ents)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[ParseNode(James), ParseNode(20), ParseNode(twenty dollars)]"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# .bubble_reduce() will aggregate data as it performs a DFS\n",
    "def f(pn,ct):\n",
    "    return ct + 1\n",
    "s1, s2 = dt.DocParser.get_parsetrees(doc)\n",
    "s1.bubble_reduce(f, 0)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# or write your own recursive functions using s1.root to \n",
    "#     access the root node of the tree\n",
    "def printnodes(pnode):\n",
    "    print(pnode.tok, pnode.dep, pnode.pos)\n",
    "    for child in pnode:\n",
    "        printnodes(child)\n",
    "s1, s2 = dt.DocParser.get_parsetrees(doc)\n",
    "printnodes(s1.root)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "paint ROOT VERB\n",
      "James nsubj PROPN\n",
      "will aux AUX\n",
      "the house dobj NOUN\n",
      "for prep ADP\n",
      "20 pobj NUM\n",
      "$ nmod SYM\n",
      "( punct PUNCT\n",
      "twenty dollars appos NOUN\n",
      ") punct PUNCT\n",
      ". punct PUNCT\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}