{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DocParser Tokenization\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import doctable as dt\n",
    "from spacy import displacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.matcher import Matcher"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "DocParser is built specifically to convert spacy doc objects to token lists or simple parsetree objects which are convenient to store in a doctable. As such, we begin by creating a spacy doc object."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "exstr = 'James will paint the house for $20 (twenty dollars). He is a rule-breaker'\n",
    "doc = nlp(exstr)\n",
    "doc"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "James will paint the house for $20 (twenty dollars). He is a rule-breaker"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Document Tokenizing\n",
    "\n",
    "Typically you will want to parse at the Spacy doc aobject. The `.tokenize_doc()` method includes common functionality for tokenizing your documents. Arguments to this function present a series of decisions that need to be made for every tokenization process. There are two "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# the most basic version performs tokenizing with all default settings\n",
    "print(dt.DocParser.tokenize_doc(doc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '20', '(', 'twenty', 'dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# split into sentences (list of lists)\n",
    "print(dt.DocParser.tokenize_doc(doc, split_sents=True))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['James', 'will', 'paint', 'the', 'house', 'for', '$', '20', '(', 'twenty', 'dollars', ')', '.'], ['he', 'is', 'a', 'rule', '-', 'breaker']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(dt.DocParser.tokenize_doc(doc, merge_ents=True))\n",
    "doc = nlp(exstr) # reverts doc back to original because adding the match (called in .tokenize_doc()) modified it"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '20', '(', 'twenty dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(dt.DocParser.tokenize_doc(doc, merge_noun_chunks=True))\n",
    "doc = nlp(exstr) # reverts doc back to original because adding the match (called in .tokenize_doc()) modified it"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['James', 'will', 'paint', 'the house', 'for', '$', '20', '(', 'twenty dollars', ')', '.', 'he', 'is', 'a rule-breaker']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Choose to include tokens\n",
    "You may not want to include all tokens, depending on spacy token information. For this case, we use the `.use_tok()` method which includes some built-in arguments to do some boilerplate steps. Again see the [full documentaiton](https://devincornell.github.io/doctable/ref/doctable.DocParser.html) to see all arguments and defaults.\n",
    "The function simply returns a boolean True/False value given a spacy token, but can be passed to `.tokenize_doc()` for added flexibility.\n",
    "\n",
    "It is most easily used by overriding parameters through a lambda function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# first try a custom function keeps only non-numbers\n",
    "use_tok_nobreaker = lambda tok: not tok.like_num\n",
    "print(dt.DocParser.tokenize_doc(doc, use_tok_func=use_tok_nobreaker))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '(', 'dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# now, you can override the .use_tok() to take care of some simple stuff\n",
    "use_tok_nostop = lambda tok: dt.DocParser.use_tok(tok, filter_stop=True)\n",
    "print(dt.DocParser.tokenize_doc(doc, use_tok_func=use_tok_nostop))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['James', 'paint', 'house', '$', '20', '(', 'dollars', ')', '.', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# remove digits\n",
    "use_tok_nodigit = lambda tok: dt.DocParser.use_tok(tok, filter_digit=True)\n",
    "print(dt.DocParser.tokenize_doc(doc, use_tok_func=use_tok_nodigit))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '(', 'twenty', 'dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# remove numbers (see it removed both \"20\" and \"Twenty\")\n",
    "use_tok_nonum = lambda tok: dt.DocParser.use_tok(tok, filter_num=True)\n",
    "print(dt.DocParser.tokenize_doc(doc, use_tok_func=use_tok_nonum))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '(', 'dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# here it thought 'James' was an organization. Use the filter_ent_types arg to remove specific ent types\n",
    "use_tok_nonames = lambda tok: dt.DocParser.use_tok(tok, filter_ent_types=['ORG'])\n",
    "print(dt.DocParser.tokenize_doc(doc, use_tok_func=use_tok_nonames))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['will', 'paint', 'the', 'house', 'for', '$', '20', '(', 'twenty', 'dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# remove all entities using the filter_all_ents argument\n",
    "use_tok_nonents = lambda tok: dt.DocParser.use_tok(tok, filter_all_ents=True)\n",
    "print(dt.DocParser.tokenize_doc(doc, use_tok_func=use_tok_nonents))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['will', 'paint', 'the', 'house', 'for', '$', '(', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# you can also add to the use_tok method using a custom function\n",
    "def custom_use_tok(tok):\n",
    "    use = dt.DocParser.use_tok(tok, filter_num=True)\n",
    "    return use and tok.pos_ != 'VERB' # here removes all verbs (including \"paint\")\n",
    "print(dt.DocParser.tokenize_doc(doc, use_tok_func=custom_use_tok))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['James', 'will', 'the', 'house', 'for', '$', '(', 'dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Choose how to parse tokens\n",
    "Conversion from a spacy token to a string can happen a number of different ways. The `.parse_tok()` method provides a number of features for this task, or a custom function can be provided."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#parse_tok(tok, replace_num=None, replace_digit=None, lemmatize=False, normal_convert=None, format_ents=True, ent_convert=None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# a custom function will simply return the original text using the tok.text property\n",
    "parse_tok = lambda tok: tok.text\n",
    "print(dt.DocParser.tokenize_doc(doc, parse_tok_func=parse_tok))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '20', '(', 'twenty', 'dollars', ')', '.', 'He', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# using .parse_tok(), first try replacing numbers with \"__NUM__\"\n",
    "parse_tok = lambda tok: dt.DocParser.parse_tok(tok, replace_num='__NUM__')\n",
    "print(dt.DocParser.tokenize_doc(doc, parse_tok_func=parse_tok))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '__NUM__', '(', '__NUM__', 'dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# now lemmatize\n",
    "parse_tok = lambda tok: dt.DocParser.parse_tok(tok, lemmatize=True)\n",
    "print(dt.DocParser.tokenize_doc(doc, parse_tok_func=parse_tok))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '20', '(', 'twenty', 'dollars', ')', '.', '-pron-', 'be', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# format_ents is one of the most useful features. \n",
    "# It will standardize ents by converting all consecutive whitespace to \n",
    "# spaces and then capitalize the first letter. This is the default setting, but it can be turned off.\n",
    "parse_tok = lambda tok: dt.DocParser.parse_tok(tok, format_ents=True)\n",
    "print(dt.DocParser.tokenize_doc(doc, parse_tok_func=parse_tok))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$', '20', '(', 'Twenty', 'Dollars', ')', '.', 'he', 'is', 'a', 'rule', '-', 'breaker']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Merging N-Grams\n",
    "DocParser offers two convenient ways to work with n-grams: (1) using the spacy matcher and (2) using the post-processed multi-token matcher. The first method is applied after normal spacy processing is finished. It involves passing a tuple of ngrams as tuples to apply after all parsing has completed. The good thing about this approach is that it doesn't require much code. The unfortunate thing is that it can only access the tokens after normal parsing. If you would like to merge tokens with hyphens between them or currency symbols to their numbers, you should use the pre-processing method.\n",
    "\n",
    "The pre-processing spacy.Matcher functionality is used to create ngrams which access certain underlying spacy components like IS_DIGIT etc. See [Spacy Matcher documention for more details](https://spacy.io/usage/rule-based-matching). The basic workflow is to create a matcher object, add patterns, and then pass matcher to .tokenize_doc(). Note that since the doc object itself is modified, python must be restarted to revert back to other tokenization method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# post-parsing ngram merging\n",
    "ngrams = (\n",
    "    ('the', 'house'),\n",
    "    ('rule', '-', 'breaker'),\n",
    "    ('he', 'is', 'a'),\n",
    ")\n",
    "# by default \n",
    "print(dt.DocParser.tokenize_doc(doc, ngrams=ngrams))\n",
    "print()\n",
    "print(dt.DocParser.tokenize_doc(doc, ngrams=ngrams, ngram_sep='_')) # specify ngram_sep"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['James', 'will', 'paint', 'the house', 'for', '$', '20', '(', 'twenty', 'dollars', ')', '.', 'he is a', 'rule - breaker']\n",
      "\n",
      "['James', 'will', 'paint', 'the_house', 'for', '$', '20', '(', 'twenty', 'dollars', ')', '.', 'he_is_a', 'rule_-_breaker']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# spacy matcher object (will be passed to docparser)\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# matches currency numbers\n",
    "pattern = [{'TEXT':'$'},{'IS_DIGIT':True}]\n",
    "matcher.add('currency', None, pattern)\n",
    "\n",
    "# matches the phrase \"he will\" or \"He Will\" or \"HE WILL\"\n",
    "pattern2 = [{'LOWER':'he'},{'LOWER':'will'}]\n",
    "matcher.add('he_will', None, pattern2)\n",
    "\n",
    "# matches hyphens\n",
    "pattern3 = [{'IS_SPACE':False},{'TEXT':'-'},{'IS_SPACE':False}]\n",
    "matcher.add('he_will', None, pattern3)\n",
    "\n",
    "print([tok for tok in dt.DocParser.tokenize_doc(doc, spacy_ngram_matcher=matcher)])\n",
    "doc = nlp(exstr) # reverts doc back to original because adding the match (called in .tokenize_doc()) modified it"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['James', 'will', 'paint', 'the', 'house', 'for', '$20', '(', 'twenty', 'dollars', ')', '.', 'he', 'is', 'a', 'rule-breaker']\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}