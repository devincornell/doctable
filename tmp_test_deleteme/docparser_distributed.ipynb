{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Distributed Parsing\n",
    "These examples will show how to use the DocParser class to make a parallelized parsing pipeline. Most of these features are in two methods:\n",
    "\n",
    "\n",
    "* **`.distribute_parse()`**: A text processing-specific method that will distribute texts among processors, apply a supplied preprocssing function, process using spacy's nlp.pipe(), and then parse to a non-spacy object using a supplied function. If a dt_inst keyword argument is defined, your supplied function may additionally insert the parsed result into the DocTable directly from the parsing process.\n",
    "\n",
    "* **`.distribute_process()`**: A more general function that allows you to distribute parsing of any element list and then if supplied with dt_inst can insert into the DocTable. This is ideal for cases where you do not want to store all texts into memory at once (as `distribute_parse()` requires, and could instead supply a list of filenames that then could be read, preprocessed, postprocessed, and inserted into the database all within the distributed processes.\n",
    "\n",
    "* **`.distribute_chunks()`**: A function that creates processes and allows you to provide a function which operates on a chunk of the provided elements."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from spacy import displacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.matcher import Matcher\n",
    "from pprint import pprint\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import doctable as dt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "texts_small = ['The hat is red. And so are you.\\n\\nWhatever, they said. Whatever indeed.', \n",
    "               'But why is the hat blue?\\n\\nAre you colorblind? See the answer here: http://google.com']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `.distribute_parse()`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# this is the straightforward mode where it tokenizes each doc in parallel\n",
    "# using default document preprocessing and parsing\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp)\n",
    "print(parsed)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['the', 'hat', 'is', 'red', '.', 'and', 'so', 'are', 'you', '.', 'whatever', ',', 'they', 'said', '.', 'whatever', 'indeed', '.'], ['but', 'why', 'is', 'the', 'hat', 'blue', '?', 'are', 'you', 'colorblind', '?', 'see', 'the', 'answer', 'here', ':', 'http://google.com']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# use paragraph_sep to maintain paragraph information\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, paragraph_sep='\\n\\n')\n",
    "print(parsed)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[['the', 'hat', 'is', 'red', '.', 'and', 'so', 'are', 'you', '.'], ['whatever', ',', 'they', 'said', '.', 'whatever', 'indeed', '.']], [['but', 'why', 'is', 'the', 'hat', 'blue', '?'], ['are', 'you', 'colorblind', '?', 'see', 'the', 'answer', 'here', ':', 'http://google.com']]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# this shows an exmple where it will customize every element of the process\n",
    "def preprocess(text): return dt.DocParser.preprocess(text, replace_url='')\n",
    "def use_token_overload(tok): return dt.DocParser.use_tok(tok, filter_stop=False, filter_punct=False)\n",
    "def parse_token_overload(tok): return dt.DocParser.parse_tok(tok, lemmatize=False)\n",
    "def parsefunc(doc): return dt.DocParser.tokenize_doc(doc, use_tok_func=use_token_overload, parse_tok_func=parse_token_overload, split_sents=True)\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, parsefunc=parsefunc, preprocessfunc=preprocess)\n",
    "pprint(parsed)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[['the', 'hat', 'is', 'red', '.'],\n",
      "  ['and', 'so', 'are', 'you', '.'],\n",
      "  ['whatever', ',', 'they', 'said', '.'],\n",
      "  ['whatever', 'indeed', '.']],\n",
      " [['but', 'why', 'is', 'the', 'hat', 'blue', '?'],\n",
      "  ['are'],\n",
      "  ['you', 'colorblind', '?'],\n",
      "  ['see', 'the', 'answer', 'here', ':']]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parse and insert into database\n",
    "The best part about all of these methods is that you can place them into a doctable directly, rather than returning them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def parse_insert(doc, db):\n",
    "    toks = dt.DocParser.tokenize_doc(doc)\n",
    "    db.insert({'tokens':toks})\n",
    "db = dt.DocTable(schema=[('pickle','tokens')], fname='t12.db')\n",
    "db.delete() # empty if it had some rows\n",
    "dt.DocParser.distribute_parse(texts_small, nlp, dt_inst=db, parsefunc=parse_insert)\n",
    "print(db.select_df())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                              tokens\n",
      "0  [the, hat, is, red, ., and, so, are, you, ., w...\n",
      "1  [but, why, is, the, hat, blue, ?, are, you, co...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### With parsetrees or arbitrary objects\n",
    "The fact that you can pass custom parsers means you can also use parsetrees or any other custom document representation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def preprocess(text): return dt.DocParser.preprocess(text, replace_url='')\n",
    "def parse_token_overload(tok): return dt.DocParser.parse_tok(tok, lemmatize=False)\n",
    "def parsefunc(doc): return dt.DocParser.get_parsetrees(doc, parse_tok_func=parse_token_overload)\n",
    "parsed = dt.DocParser.distribute_parse(texts_small, nlp, parsefunc=parsefunc, preprocessfunc=preprocess)\n",
    "pprint(parsed)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ParseTree(['the', 'hat', 'is', 'red', '.']),\n",
      "  ParseTree(['and', 'so', 'are', 'you', '.', '']),\n",
      "  ParseTree(['whatever', ',', 'they', 'said', '.']),\n",
      "  ParseTree(['whatever', 'indeed', '.'])],\n",
      " [ParseTree(['but', 'why', 'is', 'the', 'hat', 'blue', '?', '']),\n",
      "  ParseTree(['are']),\n",
      "  ParseTree(['you', 'colorblind', '?']),\n",
      "  ParseTree(['see', 'the', 'answer', 'here', ':'])]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def pf(doc):\n",
    "    return len(doc)\n",
    "dt.DocParser.distribute_parse(texts_small, nlp, parsefunc=pf)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[19, 18]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def pf(doc):\n",
    "    return len(doc)\n",
    "dt.DocParser.distribute_parse(texts_small, nlp, parsefunc=pf, paragraph_sep='\\n\\n')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[10, 8], [7, 10]]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}